‹2ã§Infoá§typeßarticle£keyπjournals/pvldb/0001LSDT20•titleŸ6Deep Entity Matching with Pre-Trained Language Models.§year§2020£doiŸ(https://doi.org/10.14778/3421424.3421431©publisher±Proc. VLDB Endow.ßauthorsïØYuliang Li 0001™Jinfeng Li∞Yoshihiko Suhara™AnHai DoanÆWang-Chiew Tan®Abstract⁄We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straight-forward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn "harder" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5%.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3421424.3421431®Keywordsê¶Badges¿•Track∞research-article®Citation"®DownloadÃÓ©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/YangKLLDCS20•titleŸ4NeuroCard: One Cardinality Estimator for All Tables.§year§2020£doiŸ(https://doi.org/10.14778/3421424.3421432©publisher±Proc. VLDB Endow.ßauthorsó≠Zongheng Yang≠Amog KamsettyØSifei Luan 0001™Eric Liang®Yan Duan¨Xi Chen 0022™Ion Stoica®Abstract⁄“Query optimizers rely on accurate cardinality estimates to produce good execution plans. Despite decades of research, existing cardinality estimators are inaccurate for complex queries, due to making lossy modeling assumptions and not capturing inter-table correlations. In this work, we show that it is possible to learn the correlations across all tables in a database without any independence assumptions. We present NeuroCard, a join cardinality estimator that builds a single neural density estimator over an entire database. Leveraging join sampling and modern deep autoregressive models, NeuroCard makes no inter-table or inter-column independence assumptions in its probabilistic modeling. NeuroCard achieves orders of magnitude higher accuracy than the best prior methods (a new state-of-the-art result of 8.5x maximum error on JOB-light), scales to dozens of tables, while being compact in space (several MBs) and efficient to construct or update (seconds to minutes).©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3421424.3421432®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/AkenYBFZBP21•titleŸyAn Inquiry into Machine Learning-based Automatic Configuration Tuning Services on Real-World Database Management Systems.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450992©publisher±Proc. VLDB Endow.ßauthorsó≠Dana Van AkenÆDongsheng Yang≤Sebastien Brillard´Ari Fiorino´Bohan Zhang±Christian Billian¨Andrew Pavlo®Abstract⁄ÏModern database management systems (DBMS) expose dozens of configurable knobs that control their runtime behavior. Setting these knobs correctly for an application's workload can improve the performance and efficiency of the DBMS. But because of their complexity, tuning a DBMS often requires considerable effort from experienced database administrators (DBAs). Recent work on automated tuning methods using machine learning (ML) have shown to achieve better performance compared with expert DBAs. These ML-based methods, however, were evaluated on synthetic workloads with limited tuning opportunities, and thus it is unknown whether they provide the same benefit in a production environment.To better understand ML-based tuning, we conducted a thorough evaluation of ML-based DBMS knob tuning methods on an enterprise database application. We use the OtterTune tuning service to compare three state-of-the-art ML algorithms on an Oracle installation with a real workload trace. Our results with OtterTune show that these algorithms generate knob configurations that improve performance by 45% over enterprise-grade configurations. We also identify deployment and measurement issues that were overlooked by previous research in automated DBMS tuning services.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450992®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ€©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/MarcusKRSMK0K20•titleΩBenchmarking Learned Indexes.§year§2020£doiŸ(https://doi.org/10.14778/3421424.3421425©publisher±Proc. VLDB Endow.ßauthorsò´Ryan Marcus¨Andreas Kipf≥Alexander van Renen≠Mihail Stoian≠Sanchit Misra≠Alfons Kemper≥Thomas Neumann 0001™Tim Kraska®Abstract⁄_Recent advancements in learned index structures propose replacing existing index structures, like B-Trees, with approximate learned models. In this work, we present a unified benchmark that compares well-tuned implementations of three learned index structures against several state-of-the-art "traditional" baselines. Using four real-world datasets, we demonstrate that learned index structures can indeed outperform non-learned indexes in read-only in-memory workloads over a dense array. We investigate the impact of caching, pipelining, dataset size, and key size. We study the performance profile of learned index structures, and build an explanation for why learned models achieve such good performance. Finally, we investigate other important properties of learned index structures, such as their performance in multi-threaded systems and their build times.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3421424.3421425®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃë©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/WangQWWZ21•titleŸ0Are We Ready For Learned Cardinality Estimation?§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461552©publisher±Proc. VLDB Endow.ßauthorsï≠Xiaoying Wang™Changbo Qu™Weiyuan Wu¨Jiannan Wang≠Qingqing Zhou®Abstract⁄ÃCardinality estimation is a fundamental but long unresolved problem in query optimization. Recently, multiple papers from different research groups consistently report that learned models have the potential to replace existing cardinality estimators. In this paper, we ask a forward-thinking question: Are we ready to deploy these learned cardinality models in production? Our study consists of three main parts. Firstly, we focus on the static environment (i.e., no data updates) and compare five new learned methods with nine traditional methods on four real-world datasets under a unified workload setting. The results show that learned models are indeed more accurate than traditional methods, but they often suffer from high training and inference costs. Secondly, we explore whether these learned models are ready for dynamic environments (i.e., frequent data updates). We find that they cannot catch up with fast data updates and return large errors for different reasons. For less frequent updates, they can perform better but there is no clear winner among themselves. Thirdly, we take a deeper look into learned models and explore when they may go wrong. Our results show that the performance of learned methods can be greatly affected by the changes in correlation, skewness, or domain size. More importantly, their behaviors are much harder to interpret and often unpredictable. Based on these findings, we identify two promising research directions (control the cost of learned models and make learned models trustworthy) and suggest a number of research opportunities. We hope that our study can guide researchers and practitioners to work together to eventually push learned cardinality estimators into real database systems.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461552®Keywordsê¶Badges¿•Track∞research-article®Citation
®DownloadJ©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/DingNAK20•titleŸTTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed Workloads.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425880©publisher±Proc. VLDB Endow.ßauthorsî´Jialin Ding≠Vikram Nathan±Mohammad Alizadeh™Tim Kraska®Abstract⁄˙Filtering data based on predicates is one of the most fundamental operations for any modern data warehouse. Techniques to accelerate the execution of filter expressions include clustered indexes, specialized sort orders (e.g., Z-order), multi-dimensional indexes, and, for high selectivity queries, secondary indexes. However, these schemes are hard to tune and their performance is inconsistent. Recent work on learned multi-dimensional indexes has introduced the idea of automatically optimizing an index for a particular dataset and workload. However, the performance of that work suffers in the presence of correlated data and skewed query workloads, both of which are common in real applications. In this paper, we introduce Tsunami, which addresses these limitations to achieve up to 6X faster query performance and up to 8X smaller index size than existing learned multi-dimensional indexes, in addition to up to 11X faster query performance and 170X smaller index size than optimally-tuned traditional indexes.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425880®Keywordsê¶Badges¿•Track∞research-article®Citation	®DownloadÃ∆©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/0001ZSYHJLWL21•titleŸ)openGauss: An Autonomous Database System.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476380©publisher±Proc. VLDB Endow.ßauthorsô∞Guoliang Li 0001´Xuanhe Zhou¶Ji Sun®Xiang YußYue Han¨Lianyuan Jin®Wenbo Li≠Tianqing Wang®Shifu Li®Abstract⁄"Although learning-based database optimization techniques have been studied from academia in recent years, they have not been widely deployed in commercial database systems. In this work, we build an autonomous database framework and integrate our proposed learning-based database techniques into an open-source database system openGauss. We propose effective learning-based models to build learned optimizers (including learned query rewrite, learned cost/cardinality estimation, learned join order selection and physical operator selection) and learned database advisors (including self-monitoring, self-diagnosis, self-configuration, and self-optimization). We devise an effective validation model to validate the effectiveness of learned models. We build effective training data management and model management platforms to easily deploy learned models. We have evaluated our techniques on real-world datasets and the experimental results validated the effectiveness of our techniques. We also provide our learnings of deploying learning-based techniques.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476380®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÕ©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/YangFO0021•titleŸ<Efficient Bi-triangle Counting for Large Bipartite Networks.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447702©publisher±Proc. VLDB Endow.ßauthorsï´Yixing Yang¨Yixiang Fang±Maria E. Orlowska±Wenjie Zhang 0001ØXuemin Lin 0001®Abstract⁄ÑA bipartite network is a network with two disjoint vertex sets and its edges only exist between vertices from different sets. It has received much interest since it can be used to model the relationship between two different sets of objects in many applications (e.g., the relationship between users and items in E-commerce). In this paper, we study the problem of efficient bi-triangle counting for a large bipartite network, where a bi-triangle is a cycle with three vertices from one vertex set and three vertices from another vertex set. Counting bi-triangles has found many real applications such as computing the transitivity coefficient and clustering coefficient for bipartite networks. To enable efficient bi-triangle counting, we first develop a baseline algorithm relying on the observation that each bi-triangle can be considered as the join of three wedges. Then, we propose a more sophisticated algorithm which regards a bi-triangle as the join of two super-wedges, where a wedge is a path with two edges while a super-wedge is a path with three edges. We further optimize the algorithm by ranking vertices according to their degrees. We have performed extensive experiments on both real and synthetic bipartite networks, where the largest one contains more than one billion edges, and the results show that the proposed solutions are up to five orders of magnitude faster than the baseline method.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447702®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ¶©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/LiSZJLDZY00021•titleŸQVolcanoML: Speeding up End-to-End AutoML via Scalable Search Space Decomposition.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476270©publisher±Proc. VLDB Endow.ßauthorsõ¨Yang Li 0106ßYu Shen¨Wentao Zhang¨Jiawei Jiang™Yaliang Li™Bolin Ding¨Jingren Zhou≠Zhi Yang 0001ÆWentao Wu 0001≠Ce Zhang 0001¨Bin Cui 0001®Abstract⁄•End-to-end AutoML has attracted intensive interests from both academia and industry, which automatically searches for ML pipelines in a space induced by feature engineering, algorithm/model selection, and hyper-parameter tuning. Existing AutoML systems, however, suffer from scalability issues when applying to application domains with large, high-dimensional search spaces. We present VOLCANOML, a scalable and extensible framework that facilitates systematic exploration of large AutoML search spaces. VOLCANOML introduces and implements basic building blocks that decompose a large search space into smaller ones, and allows users to utilize these building blocks to compose an execution plan for the AutoML problem at hand. VOLCANOML further supports a Volcano-style execution model - akin to the one supported by modern database systems - to execute the plan constructed. Our evaluation demonstrates that, not only does VOLCANOML raise the level of expressiveness for search space decomposition in AutoML, it also leads to actual findings of decomposition strategies that are significantly more efficient than the ones employed by state-of-the-art AutoML systems such as auto-sklearn.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476270®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/BoniolPPF21•titleŸ.SAND: Streaming Subsequence Anomaly Detection.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467863©publisher±Proc. VLDB Endow.ßauthorsî´Paul BoniolØJohn PaparrizosØThemis Palpanas≥Michael J. Franklin®Abstract⁄‘With the increasing demand for real-time analytics and decision making, anomaly detection methods need to operate over streams of values and handle drifts in data distribution. Unfortunately, existing approaches have severe limitations: they either require prior domain knowledge or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. In addition, subsequence anomaly detection methods usually require access to the entire dataset and are not able to learn and detect anomalies in streaming settings. To address these problems, we propose SAND, a novel online method suitable for domain-agnostic anomaly detection. SAND aims to detect anomalies based on their distance to a model that represents normal behavior. SAND relies on a novel steaming methodology to incrementally update such model, which adapts to distribution drifts and omits obsolete data. The experimental results on several real-world datasets demonstrate that SAND correctly identifies single and recurrent anomalies without prior knowledge of the characteristics of these anomalies. SAND outperforms by a large margin the current state-of-the-art algorithms in terms of accuracy while achieving orders of magnitude speedups.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467863®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadk©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/0001Z021•titleøMachine Learning for Databases.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476405©publisher±Proc. VLDB Endow.ßauthorsì∞Guoliang Li 0001´Xuanhe Zhou¨Lei Cao 0004®Abstract⁄YMachine learning techniques have been proposed to optimize the databases. For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, machine learning based techniques can alleviate this problem by judiciously selecting optimization strategy. In this tutorial, we categorize database tasks into three typical problems that can be optimized by different machine learning models, including NP-hard problems (e.g., knob space exploration, index/view selection, partition-key recommendation for offline optimization; query rewrite, join order selection for online optimization), regression problems (e.g., cost/cardinality estimation, index/view benefit estimation, query latency prediction), and prediction problems (e.g., query workload prediction). We review existing machine learning based techniques to address these problems and provide research challenges.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476405®Keywordsê¶Badges¿•Track∞research-article®Citation®Download'©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/GugnaniKL20•titleŸ;Understanding the Idiosyncrasies of Real Persistent Memory.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436921©publisher±Proc. VLDB Endow.ßauthorsì∞Shashank Gugnani≠Arjun Kashyap©Xiaoyi Lu®Abstract⁄ÙHigh capacity persistent memory (PMEM) is finally commercially available in the form of Intel's Optane DC Persistent Memory Module (DCPMM). Researchers have raced to evaluate and understand the performance of DCPMM itself as well as systems and applications designed to leverage PMEM resulting from over a decade of research. Early evaluations of DCPMM show that its behavior is more nuanced and idiosyncratic than previously thought. Several assumptions made about its performance that guided the design of PMEM-enabled systems have been shown to be incorrect. Unfortunately, several peculiar performance characteristics of DCPMM are related to the memory technology (3D-XPoint) used and its internal architecture. It is expected that other technologies (such as STT-RAM, memristor, ReRAM, NVDIMM), with highly variable characteristics, will be commercially shipped as PMEM in the near future. Current evaluation studies fail to understand and categorize the idiosyncratic behavior of PMEM; i.e., how do the peculiarities of DCPMM related to other classes of PMEM. Clearly, there is a need for a study which can guide the design of systems and is agnostic to PMEM technology and internal architecture.In this paper, we first list and categorize the idiosyncratic behavior of PMEM by performing targeted experiments with our proposed PMIdioBench benchmark suite on a real DCPMM platform. Next, we conduct detailed studies to guide the design of storage systems, considering generic PMEM characteristics. The first study guides data placement on NUMA systems with PMEM while the second study guides the design of lock-free data structures, for both eADR- and ADR-enabled PMEM systems. Our results are often counter-intuitive and highlight the challenges of system design with PMEM.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436921®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÕc©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Lin000T21•titleŸ6Hierarchical Core Maintenance on Large Dynamic Graphs.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446099©publisher±Proc. VLDB Endow.ßauthorsïßZhe LinÆFan Zhang 0036ØXuemin Lin 0001±Wenjie Zhang 0001¨Zhihong Tian®Abstract⁄ôThe model of k-core and its decomposition have been applied in various areas, such as social networks, the world wide web, and biology. A graph can be decomposed into an elegant k-core hierarchy to facilitate cohesive subgraph discovery and network analysis. As many real-life graphs are fast evolving, existing works proposed efficient algorithms to maintain the coreness value of every vertex against structure changes. However, the maintenance of the k-core hierarchy in existing studies is not complete because the connections among different k-cores in the hierarchy are not considered. In this paper, we study hierarchical core maintenance which is to compute the k-core hierarchy incrementally against graph dynamics. The problem is challenging because the change of hierarchy may be large and complex even for a slight graph update. In order to precisely locate the area affected by graph dynamics, we conduct in-depth analyses on the structural properties of the hierarchy, and propose well-designed local update techniques. Our algorithms significantly outperform the baselines on runtime by up to 3 orders of magnitude, as demonstrated on 10 real-world large graphs.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446099®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ„©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/HentschelID20•titleŸ1Stacked Filters: Learning to Filter by Structure.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436919©publisher±Proc. VLDB Endow.ßauthorsì™Kyle DeedsØBrian HentschelÆStratos Idreos®Abstract⁄åWe present Stacked Filters, a new probabilistic filter which is fast and robust similar to query-agnostic filters (such as Bloom and Cuckoo filters), and at the same time brings low false positive rates and sizes similar to classifier-based filters (such as Learned Filters). The core idea is that Stacked Filters incorporate workload knowledge about frequently queried non-existing values. Instead of learning, they structurally incorporate that knowledge using hashing and several sequenced filter layers, indexing both data and frequent negatives. Stacked Filters can also gather workload knowledge on-the-fly and adaptively build the filter. We show experimentally that for a given memory budget, Stacked Filters achieve end-to-end query throughput up to 130x better than the best alternative for a workload, either query-agnostic or classifier-based filters, and depending on where data is (SSD or HDD).©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436919®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÉ©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/SarkarSZA21•titleŸ;Constructing and Analyzing the LSM Compaction Design Space.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476274©publisher±Proc. VLDB Endow.ßauthorsî∞Subhadeep Sarkar≤Dimitris Staratzis™Zichen Zhu≥Manos Athanassoulis®Abstract⁄XLog-structured merge (LSM) trees offer efficient ingestion by appending incoming data, and thus, are widely used as the storage layer of production NoSQL data stores. To enable competitive read performance, LSM-trees periodically re-organize data to form a tree with levels of exponentially increasing capacity, through iterative compactions. Compactions fundamentally influence the performance of an LSM-engine in terms of write amplification, write throughput, point and range lookup performance, space amplification, and delete performance. Hence, choosing the appropriate compaction strategy is crucial and, at the same time, hard as the LSM-compaction design space is vast, largely unexplored, and has not been formally defined in the literature. As a result, most LSM-based engines use a fixed compaction strategy, typically hand-picked by an engineer, which decides how and when to compact data.In this paper, we present the design space of LSM-compactions, and evaluate state-of-the-art compaction strategies with respect to key performance metrics. Toward this goal, our first contribution is to introduce a set of four design primitives that can formally define any compaction strategy: (i) the compaction trigger, (ii) the data layout, (iii) the compaction granularity, and (iv) the data movement policy. Together, these primitives can synthesize both existing and completely new compaction strategies. Our second contribution is to experimentally analyze 10 compaction strategies. We present 12 observations and 7 high-level takeaway messages, which show how LSM systems can navigate the compaction design space.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476274®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÅ©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/HuCW0021•titleŸ;Persistent Memory Hash Indexes: An Experimental Evaluation.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446101©publisher±Proc. VLDB Endow.ßauthorsï©Daokun Hu´Zhiwen Chen´Jianbing Wu∞Jianhua Sun 0002≠Hao Chen 0002®Abstract⁄?Persistent memory (PM) is increasingly being leveraged to build hash-based indexing structures featuring cheap persistence, high performance, and instant recovery, especially with the recent release of Intel Optane DC Persistent Memory Modules. However, most of them are evaluated on DRAM-based emulators with unreal assumptions, or focus on the evaluation of specific metrics with important properties sidestepped. Thus, it is essential to understand how well the proposed hash indexes perform on real PM and how they differentiate from each other if a wider range of performance metrics are considered.To this end, this paper provides a comprehensive evaluation of persistent hash tables. In particular, we focus on the evaluation of six state-of-the-art hash tables including Level hashing, CCEH, Dash, PCLHT, Clevel, and SOFT, with real PM hardware. Our evaluation was conducted using a unified benchmarking framework and representative workloads. Besides characterizing common performance properties, we also explore how hardware configurations (such as PM bandwidth, CPU instructions, and NUMA) affect the performance of PM-based hash tables. With our in-depth analysis, we identify design trade-offs and good paradigms in prior arts, and suggest desirable optimizations and directions for the future development of PM-based hash tables.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446101®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÕ`©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/RuiLT20•titleŸOEfficient Join Algorithms For Large Database Tables in a Multi-GPU Environment.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436927©publisher±Proc. VLDB Endow.ßauthorsìßRan Rui¶Hao Li´Yi-Cheng Tu®Abstract⁄∏Relational join processing is one of the core functionalities in database management systems. It has been demonstrated that GPUs as a general-purpose parallel computing platform is very promising in processing relational joins. However, join algorithms often need to handle very large input data, which is an issue that was not sufficiently addressed in existing work. Besides, as more and more desktop and workstation platforms support multi-GPU environment, the combined computing capability of multiple GPUs can easily achieve that of a computing cluster. It is worth exploring how join processing would benefit from the adaptation of multiple GPUs. We identify the low rate and complex patterns of data transfer among the CPU and GPUs as the main challenges in designing efficient algorithms for large table joins. To overcome such challenges, we propose three distinctive designs of multi-GPU join algorithms, namely, the nested loop, global sort-merge and hybrid joins for large table joins with different join conditions. Extensive experiments running on multiple databases and two different hardware configurations demonstrate high scalability of our algorithms over data size and significant performance boost brought by the use of multiple GPUs. Furthermore, our algorithms achieve much better performance as compared to existing join algorithms, with a speedup up to 25X and 2.8X over best known code developed for multi-core CPUs and GPUs respectively.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436927®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÎ©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/ZhangKRSS21•titleŸ6Adaptive Code Generation for Data-Intensive Analytics.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447697©publisher±Proc. VLDB Endow.ßauthorsï¨Wangda Zhang¨Junyoung KimØKenneth A. Ross´Eric Sedlar≠Lukas Stadler®Abstract⁄Modern database management systems employ sophisticated query optimization techniques that enable the generation of efficient plans for queries over very large data sets. A variety of other applications also process large data sets, but cannot leverage database-style query optimization for their code. We therefore identify an opportunity to enhance an open-source programming language compiler with database-style query optimization. Our system dynamically generates execution plans at query time, and runs those plans on chunks of data at a time. Based on feedback from earlier chunks, alternative plans might be used for later chunks. The compiler extension could be used for a variety of data-intensive applications, allowing all of them to benefit from this class of performance optimizations.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447697®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃó©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/NargesianAJ21•titleŸHTailoring Data Source Distributions for Fairness-aware Data Integration.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476299©publisher±Proc. VLDB Endow.ßauthorsì±Fatemeh NargesianØAbolfazl AsudehÆH. V. Jagadish®Abstract⁄◊Data scientists often develop data sets for analysis by drawing upon sources of data available to them. A major challenge is to ensure that the data set used for analysis has an appropriate representation of relevant (demographic) groups: it meets desired distribution requirements. Whether data is collected through some experiment or obtained from some data provider, the data from any single source may not meet the desired distribution requirements. Therefore, a union of data from multiple sources is often required. In this paper, we study how to acquire such data in the most cost effective manner, for typical cost functions observed in practice. We present an optimal solution for binary groups when the underlying distributions of data sources are known and all data sources have equal costs. For the generic case with unequal costs, we design an approximation algorithm that performs well in practice. When the underlying distributions are unknown, we develop an exploration-exploitation based strategy with a reward function that captures the cost and approximations of group distributions in each data source. Besides theoretical analysis, we conduct comprehensive experiments that confirm the effectiveness of our algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476299®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloade©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/MohanPRC21•titleŸ5Analyzing and Mitigating Data Stalls in DNN Training.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446100©publisher±Proc. VLDB Endow.ßauthorsîØJayashree Mohan∞Amar PhanishayeeØAshish Raniwala±Vijay Chidambaram®Abstract⁄!Training Deep Neural Networks (DNNs) is resource-intensive and time-consuming. While prior research has explored many different ways of reducing DNN training time, the impact of input data pipeline, i.e., fetching raw data items from storage and performing data pre-processing in memory, has been relatively unexplored. This paper makes the following contributions: (1) We present the first comprehensive analysis of how the input data pipeline affects the training time of widely-used computer vision and audio Deep Neural Networks (DNNs), that typically involve complex data pre-processing. We analyze nine different models across three tasks and four datasets while varying factors such as the amount of memory, number of CPU threads, storage device, GPU generation etc on servers that are a part of a large production cluster at Microsoft. We find that in many cases, DNN training time is dominated by data stall time: time spent waiting for data to be fetched and pre-processed. (2) We build a tool, DS-Analyzer to precisely measure data stalls using a differential technique, and perform predictive what-if analysis on data stalls. (3) Finally, based on the insights from our analysis, we design and implement three simple but effective techniques in a data-loading library, CoorDL, to mitigate data stalls. Our experiments on a range of DNN tasks, models, datasets, and hardware configs show that when PyTorch uses CoorDL instead of the state-of-the-art DALI data loading library, DNN training time is reduced significantly (by as much as 5X on a single server).©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446100®Keywordsê¶Badges¿•Track∞research-article®Citation®Download_©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ZhuWHZPQZC21•titleŸGFLAT: Fast, Lightweight and Accurate Method for Cardinality Estimation.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461539©publisher±Proc. VLDB Endow.ßauthorsò®Rong Zhu®Ziniu Wu™Yuxing Han®Kai ZengØAndreas PfadlerÆZhengping Qian¨Jingren Zhou¨Bin Cui 0001®Abstract⁄ÈQuery optimizers rely on accurate cardinality estimation (CardEst) to produce good execution plans. The core problem of CardEst is how to model the rich joint distribution of attributes in an accurate and compact manner. Despite decades of research, existing methods either over-simplify the models only using independent factorization which leads to inaccurate estimates, or over-complicate them by lossless conditional factorization without any independent assumption which results in slow probability computation. In this paper, we propose FLAT, a CardEst method that is simultaneously &lt;u&gt;f&lt;/u&gt;ast in probability computation, &lt;u&gt;l&lt;/u&gt;ightweight in model size and &lt;u&gt;a&lt;/u&gt;ccurate in es&lt;u&gt;t&lt;/u&gt;imation quality. The key idea of FLAT is a novel unsupervised graphical model, called FSPN. It utilizes both independent and conditional factorization to adaptively model different levels of attributes correlations, and thus combines their advantages. FLAT supports efficient online probability computation in near linear time on the underlying FSPN model, provides effective offline model construction and enables incremental model updates. It can estimate cardinality for both single table queries and multi-table join queries. Extensive experimental study demonstrates the superiority of FLAT over existing CardEst methods: FLAT achieves 1--5 orders of magnitude better accuracy, 1--3 orders of magnitude faster probability computation speed and 1--2 orders of magnitude lower storage cost. We also integrate FLAT into Postgres to perform an end-to-end test. It improves the query execution time by 12.9% on the well-known IMDB benchmark workload, which is very close to the optimal result 14.2% using the true cardinality.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461539®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/MackePGLXH21•titleŸ5Fine-Grained Lineage for Safer Notebook Interactions.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447712©publisher±Proc. VLDB Endow.ßauthorsñ≠Stephen Macke∂Aditya G. Parameswaran´Hongpu Gong≤Doris Jung Lin Lee©Doris Xin´Andrew Head®Abstract⁄JComputational notebooks have emerged as the platform of choice for data science and analytical workflows, enabling rapid iteration and exploration. By keeping intermediate program state in memory and segmenting units of execution into so-called "cells", notebooks allow users to enjoy particularly tight feedback. However, as cells are added, removed, reordered, and rerun, this hidden intermediate state accumulates, making execution behavior difficult to reason about, and leading to errors and lack of reproducibility. We present nbsafety, a custom Jupyter kernel that uses runtime tracing and static analysis to automatically manage lineage associated with cell execution and global notebook state. nbsafety detects and prevents errors that users make during unaided notebook interactions, all while preserving the flexibility of existing notebook semantics. We evaluate nbsafety's ability to prevent erroneous interactions by replaying and analyzing 666 real notebook sessions. Of these, nbsafety identified 117 sessions with potential safety errors, and in the remaining 549 sessions, the cells that nbsafety identified as resolving safety issues were more than 7X more likely to be selected by users for re-execution compared to a random baseline, even though the users were not using nbsafety and were therefore not influenced by its suggestions.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447712®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/YuanJZTBJ21•titleŸITensor Relational Algebra for Distributed Machine Learning System Design.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457399©publisher±Proc. VLDB Endow.ßauthorsñ¨Binhang Yuan∞Dimitrije Jankov¨Jia Zou 0001™Yuxin Tang∞Daniel BourgeoisÆChris Jermaine®Abstract⁄ÉWe consider the question: what is the abstraction that should be implemented by the computational engine of a machine learning system? Current machine learning systems typically push whole tensors through a series of compute kernels such as matrix multiplications or activation functions, where each kernel runs on an AI accelerator (ASIC) such as a GPU. This implementation abstraction provides little built-in support for ML systems to scale past a single machine, or for handling large models with matrices or tensors that do not easily fit into the RAM of an ASIC. In this paper, we present an alternative implementation abstraction called the tensor relational algebra (TRA). The TRA is a set-based algebra based on the relational algebra. Expressions in the TRA operate over binary tensor relations, where keys are multi-dimensional arrays and values are tensors. The TRA is easily executed with high efficiency in a parallel or distributed environment, and amenable to automatic optimization. Our empirical study shows that the optimized TRA-based back-end can significantly outperform alternatives for running ML workflows in distributed clusters.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457399®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/0010Y0M21•titleŸAEpoch-based Commit and Replication in Distributed OLTP Databases.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446098©publisher±Proc. VLDB Endow.ßauthorsî™Yi Lu 0010´Xiangyao Yu¨Lei Cao 0004≠Samuel Madden®Abstract⁄ﬁMany modern data-oriented applications are built on top of distributed OLTP databases for both scalability and high availability. Such distributed databases enforce atomicity, durability, and consistency through two-phase commit (2PC) and synchronous replication at the granularity of every single transaction. In this paper, we present COCO, a new distributed OLTP database that supports epoch-based commit and replication. The key idea behind COCO is that it separates transactions into epochs and treats a whole epoch of transactions as the commit unit. In this way, the overhead of 2PC and synchronous replication is significantly reduced. We support two variants of optimistic concurrency control (OCC) using physical time and logical time with various optimizations, which are enabled by the epoch-based execution. Our evaluation on two popular benchmarks (YCSB and TPC-C) show that COCO outperforms systems with fine-grained 2PC and synchronous replication by up to a factor of four.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446098®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ ©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/TranMS20•titleŸ;Real-Time Distance-Based Outlier Detection in Data Streams.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425885©publisher±Proc. VLDB Endow.ßauthorsì¨Luan V. Tran¨Minyoung Mun≠Cyrus Shahabi®Abstract⁄ıReal-time outlier detection in data streams has drawn much attention recently as many applications need to be able to detect abnormal behaviors as soon as they occur. The arrival and departure of streaming data on edge devices impose new challenges to process the data quickly in real-time due to memory and CPU limitations of these devices. Existing methods are slow and not memory efficient as they mostly focus on quick detection of inliers and pay less attention to expediting neighbor searches for outlier candidates. In this study, we propose a new algorithm, CPOD, to improve the efficiency of outlier detections while reducing its memory requirements. CPOD uses a unique data structure called "core point" with multi-distance indexing to both quickly identify inliers and reduce neighbor search spaces for outlier candidates. We show that with six real-world and one synthetic dataset, CPOD is, on average, 10, 19, and 73 times faster than M_MCOD, NETS, and MCOD, respectively, while consuming low memory.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425885®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ¨©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/0009CCSMC21•titleŸ%On Analyzing Graphs with Motif-Paths.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447714©publisher±Proc. VLDB Endow.ßauthorsñ∞Xiaodong Li 0009≠Reynold Cheng∂Kevin Chen-Chuan Chang´Caihua Shan™Chenhao Ma´Hongtai Cao®Abstract⁄Path-based solutions have been shown to be useful for various graph analysis tasks, such as link prediction and graph clustering. However, they are no longer adequate for handling complex and gigantic graphs. Recently, motif-based analysis has attracted a lot of attention. A motif, or a small graph with a few nodes, is often considered as a fundamental unit of a graph. Motif-based analysis captures high-order structure between nodes, and performs better than traditional "edge-based" solutions. In this paper, we study motif-path, which is conceptually a concatenation of one or more motif instances. We examine how motif-paths can be used in three path-based mining tasks, namely link prediction, local graph clustering and node ranking. We further address the situation when two graph nodes are not connected through a motif-path, and develop a novel defragmentation method to enhance it. Experimental results on real graph datasets demonstrate the use of motif-paths and defragmentation techniques improves graph analysis effectiveness.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447714®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ®©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/ZhangMJKKKVK21•titleŸPDistributed Deep Learning on Data Systems: A Comparative Analysis of Approaches.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467867©publisher±Proc. VLDB Endow.ßauthorsò´Yuhao ZhangØFrank McquillanØNandish Jayaram™Nikhil Kak´Ekta Khanna¨Orhan KislalÆDomino ValdanoØArun Kumar 0001®Abstract⁄ŸDeep learning (DL) is growing in popularity for many data analytics applications, including among enterprises. Large business-critical datasets in such settings typically reside in RDBMSs or other data systems. The DB community has long aimed to bring machine learning (ML) to DBMS-resident data. Given past lessons from in-DBMS ML and recent advances in scalable DL systems, DBMS and cloud vendors are increasingly interested in adding more DL support for DB-resident data. Recently, a new parallel DL model selection execution approach called Model Hopper Parallelism (MOP) was proposed. In this paper, we characterize the particular suitability of MOP for DL on data systems, but to bring MOP-based DL to DB-resident data, we show that there is no single "best" approach, and an interesting tradeoff space of approaches exists. We explain four canonical approaches and build prototypes upon Greenplum Database, compare them analytically on multiple criteria (e.g., runtime efficiency and ease of governance) and compare them empirically with large-scale DL workloads. Our experiments and analyses show that it is non-trivial to meet all practical desiderata well and there is a Pareto frontier; for instance, some approaches are 3x-6x faster but fare worse on governance and portability. Our results and insights can help DBMS and cloud vendors design better DL support for DB users. All of our source code, data, and other artifacts are available at https://github.com/makemebitter/cerebro-ds.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467867®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ§©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ShetiyaTK020•titleŸRAstrid: Accurate Selectivity Estimation for String Predicates using Deep Learning.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436907©publisher±Proc. VLDB Endow.ßauthorsî≠Suraj ShetiyaªSaravanan Thirumuruganathan´Nick KoudasØGautam Das 0001®Abstract⁄GAccurate selectivity estimation for string predicates is a long-standing research challenge in databases. Supporting pattern matching on strings (such as prefix, substring, and suffix) makes this problem much more challenging, thereby necessitating a dedicated study. Traditional approaches often build pruned summary data structures such as tries followed by selectivity estimation using statistical correlations. However, this produces insufficiently accurate cardinality estimates resulting in the selection of sub-optimal plans by the query optimizer. Recently proposed deep learning based approaches leverage techniques from natural language processing such as embeddings to encode the strings and use it to train a model. While this is an improvement over traditional approaches, there is a large scope for improvement.We propose Astrid, a framework for string selectivity estimation that synthesizes ideas from traditional and deep learning based approaches. We make two complementary contributions. First, we propose an embedding algorithm that is query-type (prefix, substring, and suffix) and selectivity aware. Consider three strings 'ab', 'abc' and 'abd' whose prefix frequencies are 1000, 800 and 100 respectively. Our approach would ensure that the embedding for 'ab' is closer to 'abc' than 'abd'. Second, we describe how neural language models could be used for selectivity estimation. While they work well for prefix queries, their performance for substring queries is sub-optimal. We modify the objective function of the neural language model so that it could be used for estimating selectivities of pattern matching queries. We also propose a novel and efficient algorithm for optimizing the new objective function. We conduct extensive experiments over benchmark datasets and show that our proposed approaches achieve state-of-the-art results.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436907®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÇ©PaperRefsêã§Infoá§typeßarticle£keyŸ"journals/pvldb/Thirumuruganathan21•titleŸJDeep Learning for Blocking in Entity Matching: A Design Space Exploration.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476294©publisher±Proc. VLDB Endow.ßauthorsòªSaravanan Thirumuruganathan¶Han Li≠Nan Tang 0001ÆMourad Ouzzani´Yash Govind≠Derek Paulsen™Glenn Fung™AnHai Doan®Abstract⁄UEntity matching (EM) finds data instances that refer to the same real-world entity. Most EM solutions perform blocking then matching. Many works have applied deep learning (DL) to matching, but far fewer works have applied DL to blocking. These blocking works are also limited in that they consider only a simple form of DL and some of them require labeled training data. In this paper, we develop the DeepBlocker framework that significantly advances the state of the art in applying DL to blocking for EM. We first define a large space of DL solutions for blocking, which contains solutions of varying complexity and subsumes most previous works. Next, we develop eight representative solutions in this space. These solutions do not require labeled training data and exploit recent advances in DL (e.g., sequence modeling, transformer, self supervision). We empirically determine which solutions perform best on what kind of datasets (structured, textual, or dirty). We show that the best solutions (among the above eight) outperform the best existing DL solution and the best existing non-DL solutions (including a state-of-the-art industrial non-DL solution), on dirty and textual data, and are comparable on structured data. Finally, we show that the combination of the best DL and non-DL solutions can perform even better, suggesting a new venue for research.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476294®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadv©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/SalazarNA21•titleŸ7Automated Feature Engineering for Algorithmic Fairness.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3463474©publisher±Proc. VLDB Endow.ßauthorsìØRicardo Salazar≠Felix Neutatz∞Ziawasch Abedjan®Abstract⁄AOne of the fundamental problems of machine ethics is to avoid the perpetuation and amplification of discrimination through machine learning applications. In particular, it is desired to exclude the influence of attributes with sensitive information, such as gender or race, and other causally related attributes on the machine learning task. The state-of-the-art bias reduction algorithm Capuchin breaks the causality chain of such attributes by adding and removing tuples. However, this horizontal approach can be considered invasive because it changes the data distribution. A vertical approach would be to prune sensitive features entirely. While this would ensure fairness without tampering with the data, it could also hurt the machine learning accuracy. Therefore, we propose a novel multi-objective feature selection strategy that leverages feature construction to generate more features that lead to both high accuracy and fairness. On three well-known datasets, our system achieves higher accuracy than other fairness-aware approaches while maintaining similar or higher fairness.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3463474®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadq©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/LeeZLHTLZ21•titleŸXThe Art of Balance: A RateupDB Experience of Building a CPU/GPU Hybrid Database Product.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476378©publisher±Proc. VLDB Endow.ßauthorsó©Rubao Lee≠Minghong Zhou¶Chi Li¨Shenggang Hu≠Jianping Teng´Dongyang Li≥Xiaodong Zhang 0001®Abstract⁄rGPU-accelerated database systems have been studied for more than 10 years, ranging from prototyping development to industry products serving in multiple domains of data applications. Existing GPU database research solutions are often focused on specific aspects in parallel algorithms and system implementations for specific features, while industry product development generally concentrates on delivering a whole system by considering its holistic performance and cost. Aiming to fill this gap between academic research and industry development, we present a comprehensive industry product study on a complete CPU/GPU HTAP system, called RateupDB. We firmly believe "the art of balance" addresses major issues in the development of RateupDB. Specifically, we consider balancing multiple factors in the software development cycle, such as the trade-off between OLAP and OLTP, the trade-off between system performance and development productivity, and balanced choices of algorithms in the product. We also present RateupDB's complete TPC-H test performance to demonstrate its significant advantages over other existing GPU DBMS products.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476378®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadk©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/TangFLTDLMO21•titleŸfRPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457391©publisher±Proc. VLDB Endow.ßauthorsò≠Nan Tang 0001¶Ju Fan©Fangyi Li´Jianhong Tu∞Xiaoyong Du 0001∞Guoliang Li 0001≠Samuel MaddenÆMourad Ouzzani®Abstract⁄ÊCan AI help automate human-easy but computer-hard data preparation tasks that burden data scientists, practitioners, and crowd workers? We answer this question by presenting RPT, a denoising autoencoder for tuple-to-X models ("X" could be tuple, token, label, JSON, and so on). RPT is pre-trained for a tuple-to-tuple model by corrupting the input tuple and then learning a model to reconstruct the original tuple. It adopts a Transformer-based neural translation architecture that consists of a bidirectional encoder (similar to BERT) and a left-to-right autoregressive decoder (similar to GPT), leading to a generalization of both BERT and GPT. The pre-trained RPT can already support several common data preparation tasks such as data cleaning, auto-completion and schema matching. Better still, RPT can be fine-tuned on a wide range of data preparation tasks, such as value normalization, data transformation, data annotation, etc. To complement RPT, we also discuss several appealing techniques such as collaborative training and few-shot learning for entity resolution, and few-shot learning and NLP question-answering for information extraction. In addition, we identify a series of research opportunities to advance the field of data preparation.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457391®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadf©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/WhittakerACDGHH21•titleŸ<Scaling Replicated State Machines with Compartmentalization.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476273©publisher±Proc. VLDB Endow.ßauthorsô¥Michael J. Whittaker≤Ailidani Ailijiang∞Aleksey CharapkoÆMurat DemirbasØNeil GiridharanµJoseph M. Hellerstein¨Heidi Howard™Ion Stoica∞Adriana Szekeres®Abstract⁄∏State machine replication protocols, like MultiPaxos and Raft, are a critical component of many distributed systems and databases. However, these protocols offer relatively low throughput due to several bottlenecked components. Numerous existing protocols fix different bottlenecks in isolation but fall short of a complete solution. When you fix one bottleneck, another arises. In this paper, we introduce compartmentalization, the first comprehensive technique to eliminate state machine replication bottlenecks. Compartmentalization involves decoupling individual bottlenecks into distinct components and scaling these components independently. Compartmentalization has two key strengths. First, compartmentalization leads to strong performance. In this paper, we demonstrate how to compartmentalize MultiPaxos to increase its throughput by 6√ó on a write-only workload and 16√ó on a mixed read-write workload. Unlike other approaches, we achieve this performance without the need for specialized hardware. Second, compartmentalization is a technique, not a protocol. Industry practitioners can apply compartmentalization to their protocols incrementally without having to adopt a completely new protocol.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476273®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadV©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/ZhouJSZYLWLL21•titleŸ-DBMind: A Self-Driving Platform in openGauss.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476334©publisher±Proc. VLDB Endow.ßauthorsô´Xuanhe Zhou¨Lianyuan Jin¶Ji Sun¨Xinyang Zhao®Xiang Yu®Shifu Li≠Tianqing Wang¶Kun Li™Luyang Liu®Abstract⁄~We demonstrate a self-driving system DBMind, which provides three autonomous capabilities in database, including self-monitoring, self-diagnosis and self-optimization. First, self-monitoring judiciously collects database metrics and detects anomalies (e.g., slow queries and IO contention), which can profile database status while only slightly affecting system performance (&lt;5%). Then, self-diagnosis utilizes an LSTM model to analyze the root causes of the anomalies and automatically detect root causes from a pre-defined failure hierarchy. Next, self-optimization automatically optimizes the database performance using learning-based techniques, including deep reinforcement learning based knob tuning, reinforcement learning based index selection, and encoder-decoder based view selection. We have implemented DBMind in an open source database openGauss and demonstrated real scenarios.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476334®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadN©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/WuZCCWX21•titleŸ/Updatable Learned Index with Precise Positions.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457393©publisher±Proc. VLDB Endow.ßauthorsñ´Jiacheng WuØYong Zhang 0002´Shimin Chen¨Yu Chen 0052≠Jin Wang 0007≠Chunxiao Xing®Abstract⁄iIndex plays an essential role in modern database engines to accelerate the query processing. The new paradigm of "learned index" has significantly changed the way of designing index structures in DBMS. The key insight is that indexes could be regarded as learned models that predict the position of a lookup key in the dataset. While such studies show promising results in both lookup time and index size, they cannot efficiently support update operations. Although recent studies have proposed some preliminary approaches to support update, they are at the cost of scarifying the lookup performance as they suffer from the overheads brought by imprecise predictions in the leaf nodes.In this paper, we propose LIPP, a brand new framework of learned index to address such issues. Similar with state-of-the-art learned index structures, LIPP is able to support all kinds of index operations, namely lookup query, range query, insert, delete, update and bulkload. Meanwhile, we overcome the limitations of previous studies by properly extending the tree structure when dealing with update operations so as to eliminate the deviation of location predicted by the models in the leaf nodes. Moreover, we further propose a dynamic adjustment strategy to ensure that the height of the tree index is tightly bounded and provide comprehensive theoretical analysis to illustrate it. We conduct an extensive set of experiments on several real-life and synthetic datasets. The results demonstrate that our method consistently outperforms state-of-the-art solutions, achieving by up to 4X for a broader class of workloads with different index operations.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457393®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadK©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ChanIUTMLC21•titleŸXKDV-Explorer: A Near Real-Time Kernel Density Visualization System for Spatial Analysis.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476312©publisher±Proc. VLDB Endow.ßauthorsó¨Tsz Nam Chan™Pak Lon Ip´Leong Hou U≠Weng Hou TongØShivansh Mittal•Ye Li≠Reynold Cheng®Abstract⁄Kernel density visualization (KDV) is a commonly used visualization tool for many spatial analysis tasks, including disease outbreak detection, crime hotspot detection, and traffic accident hotspot detection. Although the most popular geographical information systems, e.g., QGIS, and ArcGIS, can also support this operation, these solutions are not scalable to generate a single KDV for datasets with million-scale data points, let alone to support exploratory operations (e.g., zoom in, zoom out, and panning operations) with KDV in near real-time (&lt; 5 sec). In this demonstration, we develop a near real-time visualization system, called KDV-Explorer, that is built on top of our prior study on the efficient kernel density computation. Participants will be invited to conduct some kernel density analysis on three large-scale datasets (up to 1.3 million data points), including the traffic accident dataset, crime dataset and COVID-19 dataset. We will also compare the performance of our solution and the solutions in QGIS and ArcGIS.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476312®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadA©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/LiBNLMP20•titleŸfMainlining Databases: Supporting Fast Transactional Workloads on Universal Columnar Data File Formats.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436913©publisher±Proc. VLDB Endow.ßauthorsñÆTianyu Li 0001±Matthew Butrovich´Amadou Ngom¨Wan Shen Lim¨Wes McKinney¨Andrew Pavlo®Abstract⁄tThe proliferation of modern data processing tools has given rise to open-source columnar data formats. These formats help organizations avoid repeated conversion of data to a new format for each application. However, these formats are read-only, and organizations must use a heavy-weight transformation process to load data from on-line transactional processing (OLTP) systems. As a result, DBMSs often fail to take advantage of full network bandwidth when transferring data. We aim to reduce or even eliminate this overhead by developing a storage architecture for in-memory database management systems (DBMSs) that is aware of the eventual usage of its data and emits columnar storage blocks in a universal open-source format. We introduce relaxations to common analytical data formats to efficiently update records and rely on a lightweight transformation process to convert blocks to a read-optimized layout when they are cold. We also describe how to access data from third-party analytical tools with minimal serialization overhead. We implemented our storage engine based on the Apache Arrow format and integrated it into the NoisePage DBMS to evaluate our work. Our experiments show that our approach achieves comparable performance with dedicated OLTP DBMSs while enabling orders-of-magnitude faster data exports to external data science and machine learning tools than existing methods.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436913®Keywordsê¶Badges¿•Track∞research-article®Citation®Download>©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/YaoC21•titleŸ<Efficient Size-Bounded Community Search over Large Networks.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457407©publisher±Proc. VLDB Endow.ßauthorsíßKai Yao´Lijun Chang®Abstract⁄9The problem of community search, which aims to find a cohesive subgraph containing user-given query vertices, has been extensively studied recently. Most of the existing studies mainly focus on the cohesiveness of the returned community, while ignoring the size of the community, and may yield communities of very large sizes. However, many applications naturally require that the number of vertices/members in a community should fall within a certain range. In this paper, we design exact algorithms for the general size-bounded community search problem that aims to find a subgraph with the largest min-degree among all connected subgraphs that contain the query vertex q and have at least l and at most h vertices, where q, l, h are specified by the query. As the problem is NP-hard, we propose a branch-reduce-and-bound algorithm SC-BRB by developing nontrivial reducing techniques, upper bounding techniques, and branching techniques. Experiments on large real graphs show that SC-BRB on average increases the minimum degree of the community returned by the state-of-the-art heuristic algorithm GreedyF by a factor of 2.41 and increases the edge density by a factor of 2.2. In addition, SC-BRB is several orders of magnitude faster than a baseline approach, and all of our proposed techniques contribute to the efficiency of SC-BRB.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457407®Keywordsê¶Badges¿•Track∞research-article®Citation®Download'©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/MinWHHXECH21•titleŸ]Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476264©publisher±Proc. VLDB Endow.ßauthorsò¨Seungwon Min¶Kun Wu´Sitao Huang∞Mert Hidayetoglu¨Jinjun XiongÆEiman Ebrahimi´Deming ChenÆWen-mei W. Hwu®Abstract⁄≠Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale graph-based recommender systems. Training GCN requires the minibatch generator traversing graphs and sampling the sparsely located neighboring nodes to obtain their features. Since real-world graphs often exceed the capacity of GPU memory, current GCN training systems keep the feature table in host memory and rely on the CPU to collect sparse features before sending them to the GPUs. This approach, however, puts tremendous pressure on host memory bandwidth and the CPU. This is because the CPU needs to (1) read sparse features from memory, (2) write features into memory as a dense format, and (3) transfer the features from memory to the GPUs.In this work, we propose a novel GPU-oriented data communication approach for GCN training, where GPU threads directly access sparse features in host memory through zero-copy accesses without much CPU help. By removing the CPU gathering stage, our method significantly reduces the consumption of the host resources and data access latency. We further present two important techniques to achieve high host memory access efficiency by the GPU: (1) automatic data access address alignment to maximize PCIe packet efficiency, and (2) asynchronous zero-copy access and kernel execution to fully overlap data transfer with training. We incorporate our method into PyTorch and evaluate its effectiveness using several graphs with sizes up to 111 million nodes and 1.6 billion edges. In a multi-GPU training setup, our method is 65--92% faster than the conventional data transfer method, and can even match the performance of all-in-GPU-memory training for some graphs that fit in GPU memory.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476264®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/SunSC0H20•titleŸ=RapidMatch: A Holistic Approach to Subgraph Query Processing.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425888©publisher±Proc. VLDB Endow.ßauthorsï´Shixuan Sun®Xibo Sun©Yulin CheÆQiong Luo 0001¨Bingsheng He®Abstract⁄6A subgraph query searches for all embeddings in a data graph that are identical to a query graph. Two kinds of algorithms, either graph exploration based or join based, have been developed for processing subgraph queries. Due to algorithmic and implementational differences, join-based systems can handle query graphs of a few vertices efficiently whereas exploration-based approaches typically process up to several tens of vertices in the query graph. In this paper, we first compare these two kinds of methods and prove that the complexity of result enumeration in state-of-the-art exploration-based methods matches that of the worst-case optimal join. Furthermore, we propose RapidMatch, a holistic subgraph query processing framework integrating the two approaches. Specifically, RapidMatch not only runs relational operators such as selections and joins, but also utilizes graph structural information, as in graph exploration, for filtering and join plan generation. Consequently, it outperforms the state of the art in both approaches on a wide range of query workloads.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425888®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÕ/©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/PaulHLL20•titleŸZImproving Execution Efficiency of Just-in-time Compilation based Query Processing on GPUs.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425890©publisher±Proc. VLDB Endow.ßauthorsî™Johns Paul¨Bingsheng He≠Shengliang LuÆChiew Tong Lau®Abstract⁄™In recent years, we have witnessed significant efforts to improve the performance of Online Analytical Processing (OLAP) on graphics processing units (GPUs). Most existing studies have focused on improving memory efficiency since memory stalls can play an essential role in query processing performance on GPUs. Motivated by the recent rise of just-in-time (JIT) compilation in query processing, we investigate whether and how we can further improve query processing performance on GPU. Specifically, we study the execution of state-of-the-art JIT compile-based query processing systems. We find that thanks to advanced techniques such as database compression and JIT compilation, memory stalls are no longer the most significant bottleneck. Instead, current JIT compile-based query processing encounters severe under-utilization of GPU hardware due to divergent execution and degraded parallelism arising from resource contention. To address these issues, we propose a JIT compile-based query engine named Pyper to improve GPU utilization during query execution. Specifically, Pyper has two new operators, Shuffle and Segment, for query plan transformation, which can be plugged into a physical query plan in order to reduce divergent execution and resolve resource contention, respectively. To determine the insertion points for these two operators, we present an analytical model that helps insert Shuffle and Segment operators into a query plan in a cost-based manner. Our experiments show that 1) the analytical analysis of divergent execution and resource contention helps to improve the accuracy of the cost model, 2) Pyper significantly outperforms other GPU query engines on TPC-H and SSB queries.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425890®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÚ©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/BensonMR21•titleŸ5Viper: An Efficient Hybrid PMem-DRAM Key-Value Store.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461543©publisher±Proc. VLDB Endow.ßauthorsìØLawrence BensonÆHendrik Makait¨Tilmann Rabl®Abstract⁄ﬂKey-value stores (KVSs) have found wide application in modern software systems. For persistence, their data resides in slow secondary storage, which requires KVSs to employ various techniques to increase their read and write performance from and to the underlying medium. Emerging persistent memory (PMem) technologies offer data persistence at close-to-DRAM speed, making them a promising alternative to classical disk-based storage. However, simply drop-in replacing existing storage with PMem does not yield good results, as block-based access behaves differently in PMem than on disk and ignores PMem's byte addressability, layout, and unique performance characteristics. In this paper, we propose three PMem-specific access patterns and implement them in a hybrid PMem-DRAM KVS called Viper. We employ a DRAM-based hash index and a PMem-aware storage layout to utilize the random-write speed of DRAM and efficient sequential-write performance PMem. Our evaluation shows that Viper significantly outperforms existing KVSs for core KVS operations while providing full data persistence. Moreover, Viper outperforms existing PMem-only, hybrid, and disk-based KVSs by 4--18X for write workloads, while matching or surpassing their get performance.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461543®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃµ©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/LiuLL0PS21•titleŸBDealer: An End-to-End Model Marketplace with Differential Privacy.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447700©publisher±Proc. VLDB Endow.ßauthorsñ™Jinfei Liu≠Jian Lou 0001©Junxu Liu≠Li Xiong 0001®Jian PeiØJimeng Sun 0001®Abstract⁄™Data-driven machine learning has become ubiquitous. A marketplace for machine learning models connects data owners and model buyers, and can dramatically facilitate data-driven machine learning applications. In this paper, we take a formal data marketplace perspective and propose the first en&lt;u&gt;D&lt;/u&gt;-to-end mod&lt;u&gt;e&lt;/u&gt;l m&lt;u&gt;a&lt;/u&gt;rketp&lt;u&gt;l&lt;/u&gt;ace with diff&lt;u&gt;e&lt;/u&gt;rential p&lt;u&gt;r&lt;/u&gt;ivacy (Dealer) towards answering the following questions: How to formulate data owners' compensation functions and model buyers' price functions? How can the broker determine prices for a set of models to maximize the revenue with arbitrage-free guarantee, and train a set of models with maximum Shapley coverage given a manufacturing budget to remain competitive? For the former, we propose compensation function for each data owner based on Shapley value and privacy sensitivity, and price function for each model buyer based on Shapley coverage sensitivity and noise sensitivity. Both privacy sensitivity and noise sensitivity are measured by the level of differential privacy. For the latter, we formulate two optimization problems for model pricing and model training, and propose efficient dynamic programming algorithms. Experiment results on the real chess dataset and synthetic datasets justify the design of Dealer and verify the efficiency and effectiveness of the proposed algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447700®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃü©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/LeoB21•titleŸ4Teseo and the Analysis of Structural Dynamic Graphs.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447708©publisher±Proc. VLDB Endow.ßauthorsí´Dean De LeoÆPeter A. Boncz®Abstract⁄MWe present Teseo, a new system for the storage and analysis of dynamic structural graphs in main-memory and the addition of transactional support. Teseo introduces a novel design based on sparse arrays, large arrays interleaved with gaps, and a fat tree, where the graph is ultimately stored. Our design contrasts with early systems for the analysis of dynamic graphs, which often lack transactional support and are anchored to a vertex table as a primary index. We claim that the vertex table implies several constraints, often neglected, that can actually impair the generality, the robustness and extension opportunities of these systems. We compare Teseo with other dynamic graph systems, showing a high resilience to workload and input changes, while achieving comparable, if not superior, throughputs in updates and latencies in raw scans.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447708®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃé©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/JiangQ020•titleŸ:Scalable Structural Index Construction for JSON Analytics.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436926©publisher±Proc. VLDB Endow.ßauthorsì©Lin Jiang´Junqiao Qiu∞Zhijia Zhao 0001®Abstract⁄ÛJavaScript Object Notation (JSON) and its variants have gained great popularity in recent years. Unfortunately, the performance of their analytics is often dragged down by the expensive JSON parsing. To address this, recent work has shown that building bitwise indices on JSON data, called structural indices, can greatly accelerate querying. Despite its promise, the existing structural index construction does not scale well as records become larger and more complex, due to its (inherently) sequential construction process and the involvement of costly memory copies that grow as the nesting level increases.To address the above issues, this work introduces Pison - a more memory-efficient structural index constructor with supports of intra-record parallelism. First, Pison features a redesign of the bottleneck step in the existing solution. The new design is not only simpler but more memory-efficient. More importantly, Pison is able to build structural indices for a single bulky record in parallel, enabled by a group of customized parallelization techniques. Finally, Pison is also optimized for better data locality, which is especially critical in the scenario of bulky record processing. Our evaluation using real-world JSON datasets shows that Pison achieves 9.8X speedup (on average) over the existing structural index construction solution for bulky records and 4.6X speedup (on average) of end-to-end performance (indexing plus querying) over a state-of-the-art SIMD-based JSON parser on a 16-core machine.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436926®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃâ©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/ThorneYSS0L21•titleŸ5From Natural Language Processing to Neural Databases.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447706©publisher±Proc. VLDB Endow.ßauthorsñ¨James Thorne≠Majid YazdaniÆMarzieh Saeidi≤Fabrizio SilvestriµSebastian Riedel 0001¨Alon Y. Levy®Abstract⁄˛In recent years, neural networks have shown impressive performance gains on long-standing AI problems, such as answering queries from text and machine translation. These advances raise the question of whether neural nets can be used at the core of query processing to derive answers from facts, even when the facts are expressed in natural language. If so, it is conceivable that we could relax the fundamental assumption of database management, namely, that our data is represented as fields of a pre-defined schema. Furthermore, such technology would enable combining information from text, images, and structured data seamlessly.This paper introduces neural databases, a class of systems that use NLP transformers as localized answer derivation engines. We ground the vision in NeuralDB, a system for querying facts represented as short natural language sentences. We demonstrate that recent natural language processing models, specifically transformers, can answer select-project-join queries if they are given a set of relevant facts. However, they cannot scale to non-trivial databases nor answer set-based and aggregation queries. Based on these insights, we identify specific research challenges that are needed to build neural databases. Some of the challenges require drawing upon the rich literature in data management, and others pose new research opportunities to the NLP community. Finally, we show that with preliminary solutions, NeuralDB can already answer queries over thousands of sentences with very high accuracy.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447706®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃà©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/CasteloRSBCF21•titleŸDAuctus: A Dataset Search Engine for Data Discovery and Augmentation.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476346©publisher±Proc. VLDB Endow.ßauthorsñ≠Sonia Castelo¨R√©mi Rampin≥A√©cio S. R. Santos´Aline Bessa≤Fernando ChirigatiÆJuliana Freire®Abstract⁄˚The large volumes of structured data currently available, from Web tables to open-data portals and enterprise data, open up new opportunities for progress in answering many important scientific, societal, and business questions. However, finding relevant data is difficult. While search engines have addressed this problem for Web documents, there are many new challenges involved in supporting the discovery of structured data. We demonstrate how the Auctus dataset search engine addresses some of these challenges. We describe the system architecture and how users can explore datasets through a rich set of queries. We also present case studies which show how Auctus supports data augmentation to improve machine learning models as well as to enrich analytics.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476346®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÉ©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/IstvanPC21•titleŸfSoftware-Defined Data Protection: Low Overhead Policy Compliance at the Storage Layer is Within Reach!§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450986©publisher±Proc. VLDB Endow.ßauthorsì≠Zsolt Istv√°n≥Soujanya Ponnapalli±Vijay Chidambaram®Abstract⁄ÒMost modern data processing pipelines run on top of a distributed storage layer, and securing the whole system, and the storage layer in particular, against accidental or malicious misuse is crucial to ensuring compliance to rules and regulations. Enforcing data protection and privacy rules, however, stands at odds with the requirement to achieve higher and higher access bandwidths and processing rates in large data processing pipelines.In this work we describe our proposal for the path forward that reconciles the two goals. We call our approach "Software-Defined Data Protection" (SDP). Its premise is simple, yet powerful: decoupling often changing policies from request-level enforcement allows distributed smart storage nodes to implement the latter at line-rate. Existing and future data protection frameworks can be translated to the same hardware interface which allows storage nodes to offload enforcement efficiently both for company-specific rules and regulations, such as GDPR or CCPA.While SDP is a promising approach, there are several remaining challenges to making this vision reality. As we explain in the paper, overcoming these will require collaboration across several domains, including security, databases and specialized hardware design.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450986®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÅ©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/WangZHCCWLFQHGL20•titleŸRTempura: A General Cost-Based Optimizer Framework for Incremental Data Processing.§year§2020£doiŸ(https://doi.org/10.14778/3421424.3421427©publisher±Proc. VLDB Endow.ßauthorsù´Zuozhi Wang≠Kai Zeng 0002¨Botong Huang®Wei Chen¨Xiaozong CuißBo Wang¶Ji Liu®Liya Fan™Dachuan Qu™Zhenyu Hou®Tao Guan¨Chen Li 0001¨Jingren Zhou®Abstract⁄0Incremental processing is widely-adopted in many applications, ranging from incremental view maintenance, stream computing, to recently emerging progressive data warehouse and intermittent query processing. Despite many algorithms developed on this topic, none of them can produce an incremental plan that always achieves the best performance, since the optimal plan is data dependent. In this paper, we develop a novel cost-based optimizer framework, called Tempura, for optimizing incremental data processing. We propose an incremental query planning model called TIP based on the concept of time-varying relations, which can formally model incremental processing in its most general form. We give a full specification of Tempura, which can not only unify various existing techniques to generate an optimal incremental plan, but also allow the developer to add their rewrite rules. We study how to explore the plan space and search for an optimal incremental plan. We evaluate Tempura in various incremental processing scenarios to show its effectiveness and efficiency.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3421424.3421427®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadw©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/BandleG21•titleŸJDatabase Technology for the Masses: Sub-Operators as First-Class Entities.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476296©publisher±Proc. VLDB Endow.ßauthorsí±Maximilian Bandle´Jana Giceva®Abstract⁄„A wealth of technology has evolved around relational databases over decades that has been successfully tried and tested in many settings and use cases. Yet, the majority of it remains overlooked in the pursuit of performance (e.g., NoSQL) or new functionality (e.g., graph data or machine learning). In this paper, we argue that a wide range of techniques readily available in databases are crucial to tackling the challenges the IT industry faces in terms of hardware trends management, growing workloads, and the overall complexity of a rapidly changing application and platform landscape.However, to be truly useful, these techniques must be freed from the legacy component of database engines: relational operators. Therefore, we argue that to make databases more flexible as platforms and to extend their functionality to new data types and operations requires exposing a lower level of abstraction: instead of working with SQL it would be desirable for database engines to compile, optimize, and run a collection of sub-operators for manipulating and managing data, offering them as an external interface. In this paper, we discuss the advantages of this, provide an initial list of such sub-operators, and show how they can be used in practice.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476296®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadn©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/ZhaoMWAA21•titleŸ;KLL¬±: Approximate Quantile Sketches over Dynamic Datasets.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450990©publisher±Proc. VLDB Endow.ßauthorsï´Fuheng Zhao≠Sujaya Maiyya´Ryan Weiner¨Divy Agrawal≠Amr El Abbadi®Abstract⁄IRecently the long standing problem of optimal construction of quantile sketches was resolved by Karnin, Lang, and Liberty using the KLL sketch (FOCS 2016). The algorithm for KLL is restricted to online insert operations and no delete operations. For many real-world applications, it is necessary to support delete operations. When the data set is updated dynamically, i.e., when data elements are inserted and deleted, the quantile sketch should reflect the changes. In this paper, we propose KLL¬±, the first quantile approximation algorithm to operate in the bounded deletion model to account for both inserts and deletes in a given data stream. KLL¬± extends the functionality of KLL sketches to support arbitrary updates with small space overhead. The space bound for KLL¬± is [EQUATION], where ‚àà and Œ¥ are constants that determine precision and failure probability, and Œ± bounds the number of deletions with respect to insert operations. The experimental evaluation of KLL¬± highlights that with minimal space overhead, KLL¬± achieves comparable accuracy in quantile approximation to KLL.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450990®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadn©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/MenonNMP020•titleŸWPermutable Compiled Queries: Dynamically Adapting Compiled Queries without Recompiling.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425882©publisher±Proc. VLDB Endow.ßauthorsïØPrashanth Menon´Amadou Ngom≠Todd C. Mowry¨Andrew Pavlo´Lin Ma 0006®Abstract⁄¸Just-in-time (JIT) query compilation is a technique to improve analytical query performance in database management systems (DBMSs). But the cost of compiling each query can be significant relative to its execution time. This overhead prohibits the DBMS from employing well-known adaptive query processing (AQP) methods to generate a new plan for a query if data distributions do not match the optimizer's estimations. The optimizer could eagerly generate multiple sub-plans for a query, but it can only include a few alternatives as each addition increases the compilation time.We present a method, called Permutable Compiled Queries (PCQ), that bridges the gap between JIT compilation and AQP. It allows the DBMS to modify compiled queries without needing to recompile or including all possible variations before the query starts. With PCQ, the DBMS structures a query's code with indirection layers that enable the DBMS to change the plan even while it is running. We implement PCQ in an in-memory DBMS and compare it against non-adaptive plans in a microbenchmark and against state-of-the-art analytic DBMSs. Our evaluation shows that PCQ outperforms static plans by more than 4X and yields better performance on an analytical benchmark by more than 2X against other DBMSs.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425882®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadd©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/LiuD0Z21•titleŸTFauce: Fast and Accurate Deep Ensembles with Uncertainty for Cardinality Estimation.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476254©publisher±Proc. VLDB Endow.ßauthorsîßJie Liu¨Wenqian Dong¨Dong Li 0001≠Qingqing Zhou®Abstract⁄íCardinality estimation is a fundamental and critical problem in databases. Recently, many estimators based on deep learning have been proposed to solve this problem and they have achieved promising results. However, these estimators struggle to provide accurate results for complex queries, due to not capturing real inter-column and inter-table correlations. Furthermore, none of these estimators contain the uncertainty information about their estimations. In this paper, we present a join cardinality estimator called Fauce. Fauce learns the correlations across all columns and all tables in the database. It also contains the uncertainty information of each estimation. Among all studied learned estimators, our results are promising: (1) Fauce is a light-weight estimator, it has 10√ó faster inference speed than the state of the art estimator; (2) Fauce is robust to the complex queries, it provides 1.3√ó--6.7√ó smaller estimation errors for complex queries compared with the state of the art estimator; (3) To the best of our knowledge, Fauce is the first estimator that incorporates uncertainty information for cardinality estimation into a deep learning model.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476254®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadT©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/YadavSPDC21•titleŸJQuery-Driven Video Event Processing for the Internet of Multimedia Things.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476360©publisher±Proc. VLDB Endow.ßauthorsï¨Piyush YadavÆDhaval Salwala¥Felipe Arruda PontesØPraneet Dhingra¨Edward Curry®Abstract⁄BAdvances in Deep Neural Network (DNN) techniques have revolutionized video analytics and unlocked the potential for querying and mining video event patterns. This paper details GNOSIS, an event processing platform to perform near-real-time video event detection in a distributed setting. GNOSIS follows a serverless approach where its component acts as independent microservices and can be deployed at multiple nodes. GNOSIS uses a declarative query-driven approach where users can write customize queries for spatiotemporal video event reasoning. The system converts the incoming video streams into a continuous evolving graph stream using machine learning (ML) and DNN models pipeline and applies graph matching for video event pattern detection. GNOSIS can perform both stateful and stateless video event matching. To improve Quality of Service (QoS), recent work in GNOSIS incorporates optimization techniques like adaptive scheduling, energy efficiency, and content-driven windows. This paper demonstrates the Occupational Health and Safety query use cases to show the GNOSIS efficacy.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476360®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadH©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/MinMQXEH20•titleŸIEMOGI: Efficient Memory-access for Out-of-memory Graph-traversal In GPUs.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425883©publisher±Proc. VLDB Endow.ßauthorsñ¨Seungwon Min∑Vikram Sharma Mailthody¨Zaid Qureshi¨Jinjun XiongÆEiman Ebrahimi´Wen-Mei Hwu®Abstract⁄ïModern analytics and recommendation systems are increasingly based on graph data that capture the relations between entities being analyzed. Practical graphs come in huge sizes, offer massive parallelism, and are stored in sparse-matrix formats such as compressed sparse row (CSR). To exploit the massive parallelism, developers are increasingly interested in using GPUs for graph traversal. However, due to their sizes, graphs often do not fit into the GPU memory. Prior works have either used input data pre-processing/partitioning or unified virtual memory (UVM) to migrate chunks of data from the host memory to the GPU memory. However, the large, multi-dimensional, and sparse nature of graph data presents a major challenge to these schemes and results in significant amplification of data movement and reduced effective data throughput. In this work, we propose EMOGI, an alternative approach to traverse graphs that do not fit in GPU memory using direct cache-line-sized access to data stored in host memory.This paper addresses the open question of whether a sufficiently large number of overlapping cache-line-sized accesses can be sustained to 1) tolerate the long latency to host memory, 2) fully utilize the available bandwidth, and 3) achieve favorable execution performance. We analyze the data access patterns of several graph traversal applications in GPU over PCIe using an FPGA to understand the cause of poor external bandwidth utilization. By carefully coalescing and aligning external memory requests, we show that we can minimize the number of PCIe transactions and nearly fully utilize the PCIe bandwidth with direct cache-line accesses to the host memory. EMOGI achieves 2.60X speedup on average compared to the optimized UVM implementations in various graph traversal applications. We also show that EMOGI scales better than a UVM-based solution when the system uses higher bandwidth interconnects such as PCIe 4.0.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425883®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadH©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/PeetersB21•titleŸ7Dual-Objective Fine-Tuning of BERT for Entity Matching.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467878©publisher±Proc. VLDB Endow.ßauthorsí≠Ralph PeetersØChristian Bizer®Abstract⁄=An increasing number of data providers have adopted shared numbering schemes such as GTIN, ISBN, DUNS, or ORCID numbers for identifying entities in the respective domain. This means for data integration that shared identifiers are often available for a subset of the entity descriptions to be integrated while such identifiers are not available for others. The challenge in these settings is to learn a matcher for entity descriptions without identifiers using the entity descriptions containing identifiers as training data. The task can be approached by learning a binary classifier which distinguishes pairs of entity descriptions for the same real-world entity from descriptions of different entities. The task can also be modeled as a multi-class classification problem by learning classifiers for identifying descriptions of individual entities. We present a dual-objective training method for BERT, called JointBERT, which combines binary matching and multi-class classification, forcing the model to predict the entity identifier for each entity description in a training pair in addition to the match/non-match decision. Our evaluation across five entity matching benchmark datasets shows that dual-objective training can increase the matching performance for seen products by 1% to 5% F1 compared to single-objective Transformer-based methods, given that enough training data is available for both objectives. In order to gain a deeper understanding of the strengths and weaknesses of the proposed method, we compare JointBERT to several other BERT-based matching methods as well as baseline systems along a set of specific matching challenges. This evaluation shows that JointBERT, given enough training data for both objectives, outperforms the other methods on tasks involving seen products, while it underperforms for unseen products. Using a combination of LIME explanations and domain-specific word classes, we analyze the matching decisions of the different deep learning models and conclude that BERT-based models are better at focusing on relevant word classes compared to RNN-based models.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467878®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadB©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/PoppeABDGKNRSWA20•titleŸQSeagull: An Infrastructure for Load Prediction and Optimized Resource Allocation.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425886©publisher±Proc. VLDB Endow.ßauthors‹ ™Olga Poppe¨Tayo Amuneke≠Dalitso Banda©Aritra De©Ari GreenØManon Knoertzer≠Ehi Nosakhare±Karthik Rajendran≥Deepak Shankargouda™Meina WangßAlan Au¨Carlo CurinoßQun Guo¨Alekh Jindal´Ajay Kalhan≠Morgan OslakeÆSonia Parchani¨Vijay Ramani≠Raj Sellappan™Saikat SenØSheetal Shrotri∏Soundararajan Srinivasan®Ping Xia®Shize Xu´Alicia Yang©Yiwen Zhu®Abstract⁄µMicrosoft Azure is dedicated to guarantee high quality of service to its customers, in particular, during periods of high customer activity, while controlling cost. We employ a Data Science (DS) driven solution to predict user load and leverage these predictions to optimize resource allocation. To this end, we built the Seagull infrastructure that processes per-server telemetry, validates the data, trains and deploys ML models. The models are used to predict customer load per server (24h into the future), and optimize service operations. Seagull continually re-evaluates accuracy of predictions, fallback to previously known good models and triggers alerts as appropriate. We deployed this infrastructure in production for PostgreSQL and MySQL servers across all Azure regions, and applied it to the problem of scheduling server backups during low-load time. This minimizes interference with user-induced load and improves customer experience.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425886®Keywordsê¶Badges¿•Track∞research-article®Citation®Download>©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/KoutsoukosNKSAI21•titleŸ4Tensors: An abstraction for general data processing.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467869©publisher±Proc. VLDB Endow.ßauthorsñ¥Dimitrios KoutsoukosØSupun Nakandala∂Konstantinos Karanasos™Karla SaurÆGustavo Alonso±Matteo Interlandi®Abstract⁄ªDeep Learning (DL) has created a growing demand for simpler ways to develop complex models and efficient ways to execute them. Thus, a significant effort has gone into frameworks like PyTorch or TensorFlow to support a variety of DL models and run efficiently and seamlessly over heterogeneous and distributed hardware. Since these frameworks will continue improving given the predominance of DL workloads, it is natural to ask what else can be done with them. This is not a trivial question since these frameworks are based on the efficient implementation of tensors, which are well adapted to DL but, in principle, to nothing else. In this paper we explore to what extent Tensor Computation Runtimes (TCRs) can support non-ML data processing applications, so that other use cases can take advantage of the investments made on TCRs. In particular, we are interested in graph processing and relational operators, two use cases very different from ML, in high demand, and complement quite well what TCRs can do today. Building on HUMMINGBIRD, a recent platform converting traditional machine learning algorithms to tensor computations, we explore how to map selected graph processing and relational operator algorithms into tensor computations. Our vision is supported by the results: our code often outperforms custom-built C++ and CUDA kernels, while massively reducing the development effort, taking advantage of the cross-platform compilation capabilities of TCRs.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467869®Keywordsê¶Badges¿•Track∞research-article®Citation®Download*©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/ChanLUXC21•titleŸFFast Augmentation Algorithms for Network Kernel Density Visualization.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461540©publisher±Proc. VLDB Endow.ßauthorsï¨Tsz Nam Chan´Zhe Li 0011´Leong Hou U¨Jianliang Xu≠Reynold Cheng®Abstract⁄§Network kernel density visualization, or NKDV, has been extensively used to visualize spatial data points in various domains, including traffic accident hotspot detection, crime hotspot detection, disease outbreak detection, and business and urban planning. Due to a wide range of applications for NKDV, some geographical software, e.g., ArcGIS, can also support this operation. However, computing NKDV is very time-consuming. Although NKDV has been used for more than a decade in different domains, existing algorithms are not scalable to million-sized datasets. To address this issue, we propose three efficient methods in this paper, namely aggregate distance augmentation (ADA), interval augmentation (IA), and hybrid augmentation (HA), which can significantly reduce the time complexity for computing NKDV. In our experiments, ADA, IA and HA can achieve at least 5x to 10x speedup, compared with the state-of-the-art solutions.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461540®Keywordsê¶Badges¿•Track∞research-article®Citation®Download(©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/ChenWLZQZ21•titleŸQEfficiently Answering Reachability and Path Queries on Temporal Bipartite Graphs.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467873©publisher±Proc. VLDB Endow.ßauthorsñØXiaoshuang Chen≠Kai Wang 0037ØXuemin Lin 0001±Wenjie Zhang 0001´Lu Qin 0001ØYing Zhang 0001®Abstract⁄Bipartite graphs are naturally used to model relationships between two different types of entities, such as people-location, author-paper, and customer-product. When modeling real-world applications like disease outbreaks, edges are often enriched with temporal information, leading to temporal bipartite graphs. While reachability has been extensively studied on (temporal) unipartite graphs, it remains largely unexplored on temporal bipartite graphs. To fill this research gap, in this paper, we study the reachability problem on temporal bipartite graphs. Specifically, a vertex u reaches a vertex w in a temporal bipartite graph G if u and w axe connected through a series of consecutive wedges with time constraints. Towards efficiently answering if a vertex can reach the other vertex, we propose an index-based method by adapting the idea of 2-hop labeling. Effective optimization strategies and parallelization techniques are devised to accelerate the index construction process. To better support real-life scenarios, we further show how the index is leveraged to efficiently answer other types of queries, e.g., single-source reachability query and earliest-arrival path query. Extensive experiments on 16 real-world graphs demonstrate the effectiveness and efficiency of our proposed techniques.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467873®Keywordsê¶Badges¿•Track∞research-article®Citation®Download'©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/WangF20•titleŸ[PPQ-Trajectory: Spatio-temporal Quantization for Querying in Large Trajectory Repositories.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425891©publisher±Proc. VLDB Endow.ßauthorsí∞Shuang Wang 0005µHakan Ferhatosmanoglu®Abstract⁄≤We present PPQ-trajectory, a spatio-temporal quantization based solution for querying large dynamic trajectory data. PPQ-trajectory includes a partition-wise predictive quantizer (PPQ) that generates an error-bounded codebook with autocorrelation and spatial proximity-based partitions. The codebook is indexed to run approximate and exact spatio-temporal queries over compressed trajectories. PPQ-trajectory includes a coordinate quadtree coding for the codebook with support for exact queries. An incremental temporal partition-based index is utilised to avoid full reconstruction of trajectories during queries. An extensive set of experimental results for spatio-temporal queries on real trajectory datasets is presented. PPQ-trajectory shows significant improvements over the alternatives with respect to several performance measures, including the accuracy of results when the summary is used directly to provide approximate query results, the spatial deviation with which spatio-temporal path queries can be answered when the summary is used as an index, and the time taken to construct the summary. Superior results on the quality of the summary and the compression ratio are also demonstrated.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425891®Keywordsê¶Badges¿•Track∞research-article®Citation®Download'©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/SchleichGZS21•titleŸ7GeCo: Quality Counterfactual Explanations in Real Time.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461555©publisher±Proc. VLDB Endow.ßauthorsî≥Maximilian Schleich´Zixuan Geng¨Yihong Zhang©Dan Suciu®Abstract⁄wMachine learning is increasingly applied in high-stakes decision making that directly affect people's lives, and this leads to an increased demand for systems to explain their decisions. Explanations often take the form of counterfactuals, which consists of conveying to the end user what she/he needs to change in order to improve the outcome. Computing counterfactual explanations is challenging, because of the inherent tension between a rich semantics of the domain, and the need for real time response. In this paper we present CeCo, the first system that can compute plausible and feasible counterfactual explanations in real time. At its core, CeCo relies on a genetic algorithm, which is customized to favor searching counterfactual explanations with the smallest number of changes. To achieve real-time performance, we introduce two novel optimizations: Œî-representation of candidate counterfactuals, and partial evaluation of the classifier. We compare empirically CeCo against five other systems described in the literature, and show that it is the only system that can achieve both high quality explanations and real time answers.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461555®Keywordsê¶Badges¿•Track∞research-article®Citation®Download%©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Guo0O0K20•titleŸPScalable Mining of Maximal Quasi-Cliques: An Algorithm-System Codesign Approach.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436916©publisher±Proc. VLDB Endow.ßauthorsï©Guimu Guo´Da Yan 0001ÆM. Tamer √ñzsuÆZhe Jiang 0001¨Jalal Khalil®Abstract⁄ÙGiven a user-specified minimum degree threshold Œ≥, a Œ≥-quasiclique is a subgraph g = (Vg, Eg) where each vertex ŒΩ ‚àà Vg connects to at least Œ≥ fraction of the other vertices (i.e., ‚åàŒ≥ ¬∑ (|Vg|- 1)‚åâ vertices) in g. Quasi-clique is one of the most natural definitions for dense structures useful in finding communities in social networks and discovering significant biomolecule structures and pathways. However, mining maximal quasi-cliques is notoriously expensive.In this paper, we design parallel algorithms for mining maximal quasi-cliques on G-thinker, a distributed graph mining framework that decomposes mining into compute-intensive tasks to fully utilize CPU cores. We found that directly using G-thinker results in the straggler problem due to (i) the drastic load imbalance among different tasks and (ii) the difficulty of predicting the task running time. We address these challenges by redesigning G-thinker's execution engine to prioritize long-running tasks for execution, and by utilizing a novel timeout strategy to effectively decompose long-running tasks to improve load balancing. While this system redesign applies to many other expensive dense subgraph mining problems, this paper verifies the idea by adapting the state-of-the-art quasi-clique algorithm, Quick, to our redesigned G-thinker. Extensive experiments verify that our new solution scales well with the number of CPU cores, achieving 201√ó runtime speedup when mining a graph with 3.77M vertices and 16.5M edges in a 16-node cluster.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436916®Keywordsê¶Badges¿•Track∞research-article®Citation®Download ©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/JacobSSRDT21•titleŸIExathlon: A Benchmark for Explainable Anomaly Detection over Time Series.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476307©publisher±Proc. VLDB Endow.ßauthorsñ≠Vincent Jacob®Fei SongØArnaud Stiegler©Bijan Rad´Yanlei Diao≠Nesime Tatbul®Abstract⁄
Access to high-quality data repositories and benchmarks have been instrumental in advancing the state of the art in many experimental research domains. While advanced analytics tasks over time series data have been gaining lots of attention, lack of such community resources severely limits scientific progress. In this paper, we present Exathlon, the first comprehensive public benchmark for explainable anomaly detection over high-dimensional time series data. Exathlon has been systematically constructed based on real data traces from repeated executions of large-scale stream processing jobs on an Apache Spark cluster. Some of these executions were intentionally disturbed by introducing instances of six different types of anomalous events (e.g., misbehaving inputs, resource contention, process failures). For each of the anomaly instances, ground truth labels for the root cause interval as well as those for the extended effect interval are provided, supporting the development and evaluation of a wide range of anomaly detection (AD) and explanation discovery (ED) tasks. We demonstrate the practical utility of Exathlon's dataset, evaluation methodology, and end-to-end data science pipeline design through an experimental study with three state-of-the-art AD and ED techniques.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476307®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/NegiMKMTKA21•titleŸ6Flow-Loss: Learning Cardinality Estimates That Matter.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476259©publisher±Proc. VLDB Endow.ßauthorsóØParimarjan NegiÆRyan C. Marcus¨Andreas Kipf™Hongzi Mao≠Nesime Tatbul™Tim Kraska±Mohammad Alizadeh®Abstract⁄•Recently there has been significant interest in using machine learning to improve the accuracy of cardinality estimation. This work has focused on improving average estimation error, but not all estimates matter equally for downstream tasks like query optimization. Since learned models inevitably make mistakes, the goal should be to improve the estimates that make the biggest difference to an optimizer. We introduce a new loss function, Flow-Loss, for learning cardinality estimation models. Flow-Loss approximates the optimizer's cost model and search algorithm with analytical functions, which it uses to optimize explicitly for better query plans. At the heart of Flow-Loss is a reduction of query optimization to a flow routing problem on a certain "plan graph", in which different paths correspond to different query plans. To evaluate our approach, we introduce the Cardinality Estimation Benchmark (CEB) which contains the ground truth cardinalities for sub-plans of over 16K queries from 21 templates with up to 15 joins. We show that across different architectures and databases, a model trained with Flow-Loss improves the plan costs and query runtimes despite having worse estimation accuracy than a model trained with Q-Error. When the test set queries closely match the training queries, models trained with both loss functions perform well. However, the Q-Error-trained model degrades significantly when evaluated on slightly different queries (e.g., similar but unseen query templates), while the Flow-Loss-trained model generalizes better to such situations, achieving 4 -- 8√ó better 99th percentile runtimes on unseen templates with the same model architecture and training data.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476259®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/ChenZXQZ0JZYZ21•titleŸWSChain: A Scalable Consortium Blockchain Exploiting Intra- and Inter-Block Concurrency.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476348©publisher±Proc. VLDB Endow.ßauthorsö´Zhihao Chen¨Haizhen Zhuo´Quanqing Xu´Xiaodong Qi´Chengyu ZhuØZhao Zhang 0009´Cheqing Jin´Aoying Zhou≠Ying Yan 0002ÆHui Zhang 0002®Abstract⁄–We demonstrate SChain, a consortium blockchain that scales transaction processing to support large-scale enterprise applications. The unique advantage of SChain stems from the exploitation of both intra- and inter-block concurrency. The intra-block concurrency not only takes advantage of the multi-core processor on a single peer but also leverages the capacity of multiple peers. The interblock concurrency enables simultaneous processing across multiple blocks to increase the utilization of various peers. In our demonstration, we use real-time dashboards containing visualization based on the output of SChain to give the attendees interactive explorations of how SChain achieves intra- and inter-block concurrency.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476348®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÕô©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/0002FICA20•titleŸ4Rumble: Data Independence for Large Messy Data Sets.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436910©publisher±Proc. VLDB Endow.ßauthorsï±Ingo M√ºller 0002ØGhislain FournyØStefan Irimescu∞Can Berker CikisÆGustavo Alonso®Abstract⁄:This paper introduces Rumble, a query execution engine for large, heterogeneous, and nested collections of JSON objects built on top of Apache Spark. While data sets of this type are more and more wide-spread, most existing tools are built around a tabular data model, creating an impedance mismatch for both the engine and the query interface. In contrast, Rumble uses JSONiq, a standardized language specifically designed for querying JSON documents. The key challenge in the design and implementation of Rumble is mapping the recursive structure of JSON documents and JSONiq queries onto Spark's execution primitives based on tabular data frames. Our solution is to translate a JSONiq expression into a tree of iterators that dynamically switch between local and distributed execution modes depending on the nesting level. By overcoming the impedance mismatch in the engine, Rumble frees the user from solving the same problem for every single query, thus increasing their productivity considerably. As we show in extensive experiments, Rumble is able to scale to large and complex data sets in the terabyte range with a similar or better performance than other engines. The results also illustrate that Codd's concept of data independence makes as much sense for heterogeneous, nested data sets as it does on highly structured tables.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436910®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÕc©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/SunWL021•titleŸJBuilding Enclave-Native Storage Engines for Practical Encrypted Databases.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447705©publisher±Proc. VLDB Endow.ßauthorsî¨Yuanyuan SunØSheng Wang 0011™Huorong LiÆFeifei Li 0001®Abstract⁄‹Data confidentiality is one of the biggest concerns that hinders enterprise customers from moving their workloads to the cloud. Thanks to the trusted execution environment (TEE), it is now feasible to build encrypted databases in the enclave that can process customers' data while keeping it confidential to the cloud. Though some enclave-based encrypted databases emerge recently, there remains a large unexplored area in between about how confidentiality can be achieved in different ways and what influences are implied by them. In this paper, we first provide a broad exploration of possible design choices in building encrypted database storage engines, rendering trade-offs in security, performance and functionality. We observe that choices on different dimensions can be independent and their combination determines the overall trade-off of the entire storage. We then propose Enclage, an encrypted storage engine that makes practical trade-offs. It adopts many enclave-native designs, such as page-level encryption, reduced enclave interaction, and hierarchical memory buffer, which offer high-level security guarantee and high performance at the same time. To make better use of the limited enclave memory, we derive the optimal page size in enclave and adopt delta decryption to access large data pages with low cost. Our experiments show that Enclage outperforms the baseline, a common storage design in many encrypted databases, by over 13x in throughput and about 5x in storage savings.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447705®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃŸ©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/LiuCC21•titleŸIZen: a High-Throughput Log-Free OLTP Engine for Non-Volatile Main Memory.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446105©publisher±Proc. VLDB Endow.ßauthorsì®Gang Liu´Leying Chen´Shimin Chen®Abstract⁄Emerging &lt;u&gt;N&lt;/u&gt;on-&lt;u&gt;V&lt;/u&gt;olatile &lt;u&gt;M&lt;/u&gt;emory (NVM) technologies like 3DX-point promise significant performance potential for OLTP databases. However, transactional databases need to be redesigned because the key assumptions that non-volatile storage is orders of magnitude slower than DRAM and only supports blocked-oriented access have changed. NVMs are byte-addressable and almost as fast as DRAM. The capacity of NVM is much (4-16x) larger than DRAM. Such NVM characteristics make it possible to build OLTP database entirely in NVM main memory.This paper studies the structure of OLTP engines with hybrid NVM and DRAM memory. We observe three challenges to design an OLTP engine for NVM: tuple metadata modifications, NVM write redundancy, and NVM space management. We propose Zen, a high-throughput log-free OLTP engine for NVM. Zen addresses the three design challenges with three novel techniques: metadata enhanced tuple cache, log-free persistent transactions, and light-weight NVM space management. Experimental results on a real machine equipped with Intel Optane DC Persistent Memory show that Zen achieves up to 10.1x improvement compared with existing solutions to run an OLTP database as large as the size of NVM while achieving fast failure recovery.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446105®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ»©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/ZhangLL0LZ020•titleŸ9On-Off Sketch: A Fast and Accurate Sketch on Persistence.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425884©publisher±Proc. VLDB Endow.ßauthorsó∞Yinda Zhang 0002™Jinyang Li™Yutian LeiÆTong Yang 0003©Zhetao Li™Gong Zhang¨Bin Cui 0001®Abstract⁄@Approximate stream processing has attracted much attention recently. Prior art mostly focuses on characteristics like frequency, cardinality, and quantile. Persistence, as a new characteristic, is getting increasing attention. Unlike frequency, persistence highlights behaviors where an item appears recurrently in many time windows of a data stream. There are two typical problems with persistence - persistence estimation and finding persistent items. In this paper, we propose the On-Off sketch to address both problems. For persistence estimation, using the characteristic that the persistence of an item is increased periodically, we compress increments when multiple items are mapped to the same counter, which significantly reduces the error. Compared with the Count-Min sketch, 1) in theory, we prove that the error of the On-Off sketch is always smaller; 2) in experiments, the On-Off sketch achieves around 6.17 times smaller error and 2.2 times higher throughput. For finding persistent items, we propose a technique to separate persistent and non-persistent items, further improving the accuracy. We show that the space complexity of our On-Off sketch is much better than the state-of-the-art (PIE), and it reduces the error up to 4 orders of magnitude and achieves 2.84 times higher throughput than prior algorithms in experiments.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425884®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ∆©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/LiuLF00021•titleŸEAdaptive Data Augmentation for Supervised Learning over Missing Data.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450989©publisher±Proc. VLDB Endow.ßauthorsñ™Tongyu Liu¶Ju Fan´Yinqing Luo≠Nan Tang 0001∞Guoliang Li 0001∞Xiaoyong Du 0001®Abstract⁄õReal-world data is dirty, which causes serious problems in (supervised) machine learning (ML). The widely used practice in such scenario is to first repair the labeled source (a.k.a. train) data using rule-, statistical- or ML-based methods and then use the "repaired" source to train an ML model. During production, unlabeled target (a.k.a. test) data will also be repaired, and is then fed in the trained ML model for prediction. However, this process often causes a performance degradation when the source and target datasets are dirty with different noise patterns, which is common in practice.In this paper, we propose an adaptive data augmentation approach, for handling missing data in supervised ML. The approach extracts noise patterns from target data, and adapts the source data with the extracted target noise patterns while still preserving supervision signals in the source. Then, it patches the ML model by retraining it on the adapted data, in order to better serve the target. To effectively support adaptive data augmentation, we propose a novel generative adversarial network (GAN) based framework, called DAGAN, which works in an unsupervised fashion. DAGAN consists of two connected GAN networks. The first GAN learns the noise pattern from the target, for target mask generation. The second GAN uses the learned target mask to augment the source data, for source data adaptation. The augmented source data is used to retrain the ML model. Extensive experiments show that our method significantly improves the ML model performance and is more robust than the state-of-the-art missing data imputation solutions for handling datasets with different missing value patterns.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450989®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ¬©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/ZhangRLYCLWFWHB21•titleŸWTowards Cost-Effective and Elastic Cloud Database Deployment via Memory Disaggregation.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467877©publisher±Proc. VLDB Endow.ßauthorsõØYingqiang Zhang´Chaoyi Ruan®Cheng Li™Jimmy YangßWei CaoÆFeifei Li 0001ßBo Wang©Jing Fang™Yuhui Wang™Jingze HuoßChao Bi®Abstract⁄mIt is challenging for cloud-native relational databases to meet the ever-increasing needs of scaling compute and memory resources independently and elastically. The recent emergence of memory disaggregation architecture, relying on high-speed RDMA network, offers opportunities to build cost-effective and elastic cloud-native databases. There exist proposals to let unmodified applications run transparently on disaggregated systems. However, running relational database kernel atop such proposals experiences notable performance degradation and time-consuming failure recovery, offsetting the benefits of disaggregation.To address these challenges, in this paper, we propose a novel database architecture called LegoBase, which explores the co-design of database kernel and memory disaggregation. It pushes the memory management back to the database layer for bypassing the Linux I/O stack and re-using or designing (remote) memory access optimizations with an understanding of data access patterns. LegoBase further splits the conventional ARIES fault tolerance protocol to independently handle the local and remote memory failures for fast recovery of compute instances. We implemented LegoBase atop MySQL. We compare LegoBase against MySQL running on a standalone machine and the state-of-the-art disaggregation proposal Infiniswap. Our evaluation shows that even with a large fraction of data placed on the remote memory, LegoBase's system performance in terms of throughput (up to 9.41% drop) and P99 latency (up to 11.58% increase) is comparable to the monolithic MySQL setup, and significantly outperforms (1.99x-2.33x, respectively) the deployment of MySQL over Infiniswap. Meanwhile, LegoBase introduces an up to 3.87x and 5.48x speedup of the recovery and warm-up time, respectively, over the monolithic MySQL and MySQL over Infiniswap, when handling failures or planned re-configurations.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467877®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃΩ©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/YangYWLYSAS21•titleŸ<FlexPushdownDB: Hybrid Pushdown and Caching in a Cloud DBMS.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476265©publisher±Proc. VLDB Endow.ßauthorsò™Yifei Yang´Matt Youill±Matthew E. Woicik™Yizhou Liu´Xiangyao YuÆMarco Serafini∞Ashraf Aboulnaga≥Michael Stonebraker®Abstract⁄5Modern cloud databases adopt a storage-disaggregation architecture that separates the management of computation and storage. A major bottleneck in such an architecture is the network connecting the computation and storage layers. Two solutions have been explored to mitigate the bottleneck: caching and computation pushdown. While both techniques can significantly reduce network traffic, existing DBMSs consider them as orthogonal techniques and support only one or the other, leaving potential performance benefits unexploited.In this paper we present FlexPushdownDB (FPDB), an OLAP cloud DBMS prototype that supports fine-grained hybrid query execution to combine the benefits of caching and computation pushdown in a storage-disaggregation architecture. We build a hybrid query executor based on a new concept called separable operators to combine the data from the cache and results from the pushdown processing. We also propose a novel Weighted-LFU cache replacement policy that takes into account the cost of pushdown computation. Our experimental evaluation on the Star Schema Benchmark shows that the hybrid execution outperforms both the conventional caching-only architecture and pushdown-only architecture by 2.2X. In the hybrid architecture, our experiments show that Weighted-LFU can outperform the baseline LFU by 37%.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476265®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃµ©PaperRefsêã§Infoá§typeßarticle£keyøjournals/pvldb/TsitsulinMMKOM21•titleŸ FREDE: Anytime Graph Embeddings.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447713©publisher±Proc. VLDB Endow.ßauthorsñØAnton Tsitsulin∞Marina Munkhoeva≠Davide Mottin±Panagiotis Karras±Ivan V. Oseledets∞Emmanuel M√ºller®Abstract⁄gLow-dimensional representations, or embeddings, of a graph's nodes facilitate several practical data science and data engineering tasks. As such embeddings rely, explicitly or implicitly, on a similarity measure among nodes, they require the computation of a quadratic similarity matrix, inducing a tradeoff between space complexity and embedding quality. To date, no graph embedding work combines (i) linear space complexity, (ii) a nonlinear transform as its basis, and (iii) nontrivial quality guarantees. In this paper we introduce FREDE (FREquent Directions Embedding), a graph embedding based on matrix sketching that combines those three desiderata. Starting out from the observation that embedding methods aim to preserve the covariance among the rows of a similarity matrix, FREDE iteratively improves on quality while individually processing rows of a nonlinearly transformed PPR similarity matrix derived from a state-of-the-art graph embedding method and provides, at any iteration, column-covariance approximation guarantees in due course almost indistinguishable from those of the optimal approximation by SVD. Our experimental evaluation on variably sized networks shows that FREDE performs almost as well as SVD and competitively against state-of-the-art embedding methods in diverse data science tasks, even when it is based on as little as 10% of node similarities.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447713®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ∞©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/Pavlo21•titleŸRMake Your Database System Dream of Electric Sheep: Towards Self-Driving Operation.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476411©publisher±Proc. VLDB Endow.ßauthorsó™Andy Pavlo±Matthew Butrovich´Lin Ma 0006ØPrashanth Menon¨Wan Shen Lim≠Dana Van Aken≠William Zhang®Abstract⁄ˆDatabase management systems (DBMSs) are notoriously difficult to deploy and administer. Self-driving DBMSs seek to remove these impediments by managing themselves automatically. Despite decades of DBMS auto-tuning research, a truly autonomous, self-driving DBMS is yet to come. But recent advancements in artificial intelligence and machine learning (ML) have moved this goal closer.Given this, we present a system implementation treatise towards achieving a self-driving DBMS. We first provide an overview of the NoisePage self-driving DBMS that uses ML to predict the DBMS's behavior and optimize itself without human support or guidance. The system's architecture has three main ML-based components: (1) workload forecasting, (2) behavior modeling, and (3) action planning. We then describe the system design principles to facilitate holistic autonomous operations. Such prescripts reduce the complexity of the problem, thereby enabling a DBMS to converge to a better and more stable configuration more quickly.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476411®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ†©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/Kokoris-KogiasA20•titleŸ;CALYPSO: Private Data Management for Decentralized Ledgers.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436917©publisher±Proc. VLDB Endow.ßauthorsñ∫Eleftherios Kokoris-KogiasØEnis Ceyhun Alp¨Linus Gasser±Philipp Jovanovic®Ewa Syta™Bryan Ford®Abstract⁄ÉDistributed ledgers provide high availability and integrity, making them a key enabler for practical and secure computation of distributed workloads among mutually distrustful parties. Many practical applications also require strong confidentiality, however. This work enhances permissioned and permissionless blockchains with the ability to manage confidential data without forfeiting availability or decentralization. The proposed Calypso architecture addresses two orthogonal challenges confronting modern distributed ledgers: (a) enabling the auditable management of secrets and (b) protecting distributed computations against arbitrage attacks when their results depend on the ordering and secrecy of inputs.Calypso introduces on-chain secrets, a novel abstraction that enforces atomic deposition of an auditable trace whenever users access confidential data. Calypso provides user-controlled consent management that ensures revocation atomicity and accountable anonymity. To enable permissionless deployment, we introduce an incentive scheme and provide users with the option to select their preferred trustees. We evaluated our Calypso prototype with a confidential document-sharing application and a decentralized lottery. Our benchmarks show that transaction-processing latency increases linearly in terms of security (number of trustees) and is in the range of 0.2 to 8 seconds for 16 to 128 trustees.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436917®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃõ©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/FanHLLLLQ0WXYYY21•titleŸ6GraphScope: A Unified Engine For Big Graph Processing.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476369©publisher±Proc. VLDB Endow.ßauthors‹ ™Wenfei Fan¶Tao He´Longbin Lai¶Xue LißYong LißZhao LiÆZhengping QianÆChao Tian 0001®Lei Wang©Jingbo Xu´Youyang YaoÆQiang Yin 0002™Wenyuan Yu®Kai Zeng®Kun Zhao¨Jingren Zhou©Diwen Zhu®Rong Zhu®Abstract⁄ÀGraphScope is a system and a set of language extensions that enable a new programming interface for large-scale distributed graph computing. It generalizes previous graph processing frameworks (e.g., Pregel, GraphX) and distributed graph databases (e.g., Janus-Graph, Neptune) in two important ways: by exposing a unified programming interface to a wide variety of graph computations such as graph traversal, pattern matching, iterative algorithms and graph neural networks within a high-level programming language; and by supporting the seamless integration of a highly optimized graph engine in a general purpose data-parallel computing system.A GraphScope program is a sequential program composed of declarative data-parallel operators, and can be written using standard Python development tools. The system automatically handles the parallelization and distributed execution of programs on a cluster of machines. It outperforms current state-of-the-art systems by enabling a separate optimization (or family of optimizations) for each graph operation in one carefully designed coherent framework. We describe the design and implementation of GraphScope and evaluate system performance using several real-world applications.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476369®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃò©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/KouadriOBEPA20•titleŸBQuality of Sentiment Analysis Tools: The Reasons of Inconsistency.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436924©publisher±Proc. VLDB Endow.ßauthorsñµWissam Maamar Kouadri≠Mourad Ouziri∞Salima BenbernouØKarima EchihabiØThemis Palpanas≠Iheb Ben Amor®Abstract⁄ÊIn this paper, we present a comprehensive study that evaluates six state-of-the-art sentiment analysis tools on five public datasets, based on the quality of predictive results in the presence of semantically equivalent documents, i.e., how consistent existing tools are in predicting the polarity of documents based on paraphrased text. We observe that sentiment analysis tools exhibit intra-tool inconsistency, which is the prediction of different polarity for semantically equivalent documents by the same tool, and inter-tool inconsistency, which is the prediction of different polarity for semantically equivalent documents across different tools. We introduce a heuristic to assess the data quality of an augmented dataset and a new set of metrics to evaluate tool inconsistencies. Our results indicate that tool inconsistencies is still an open problem, and they point towards promising research directions and accuracy improvements that can be obtained if such inconsistencies are resolved.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436924®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃñ©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/LaignerZSLK21•titleŸ]Data Management in Microservices: State of the Practice, Challenges, and Research Directions.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484232©publisher±Proc. VLDB Endow.ßauthorsï≤Rodrigo N. Laigner≠Yongluan ZhouπMarcos Antonio Vaz Salles™Yijian Liu±Marcos Kalinowski®Abstract⁄gMicroservices have become a popular architectural style for data-driven applications, given their ability to functionally decompose an application into small and autonomous services to achieve scalability, strong isolation, and specialization of database systems to the workloads and data formats of each service. Despite the accelerating industrial adoption of this architectural style, an investigation of the state of the practice and challenges practitioners face regarding data management in microservices is lacking. To bridge this gap, we conducted a systematic literature review of representative articles reporting the adoption of microservices, we analyzed a set of popular open-source microservice applications, and we conducted an online survey to cross-validate the findings of the previous steps with the perceptions and experiences of over 120 experienced practitioners and researchers.Through this process, we were able to categorize the state of practice of data management in microservices and observe several foundational challenges that cannot be solved by software engineering practices alone, but rather require system-level support to alleviate the burden imposed on practitioners. We discuss the shortcomings of state-of-the-art database systems regarding microservices and we conclude by devising a set of features for microservice-oriented database systems.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484232®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃã©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/Li0JP21•titleŸHTRACE: Real-time Compression of Streaming Trajectories in Road Networks.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450987©publisher±Proc. VLDB Endow.ßauthorsîÆTianyi Li 0005¨Lu Chen 0001≥Christian S. Jensen¥Torben Bach Pedersen®Abstract⁄ìThe deployment of vehicle location services generates increasingly massive vehicle trajectory data, which incurs high storage and transmission costs. A range of studies target offline compression to reduce the storage cost. However, to enable online services such as real-time traffic monitoring, it is attractive to also reduce transmission costs by being able to compress streaming trajectories in real-time. Hence, we propose a framework called TRACE that enables compression, transmission, and querying of network-constrained streaming trajectories in a fully online fashion. We propose a compact two-stage representation of streaming trajectories: a speed-based representation removes redundant information, and a multiple-references based referential representation exploits subtrajectory similarities. In addition, the online referential representation is extended with reference selection, deletion and rewriting functions that further improve the compression performance. An efficient data transmission scheme is provided for achieving low transmission overhead. Finally, indexing and filtering techniques support efficient real-time range queries over compressed trajectories. Extensive experiments with real-life and synthetic datasets evaluate the different parts of TRACE, offering evidence that it is able to outperform the existing representative methods in terms of both compression ratio and transmission cost.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450987®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃà©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/SahaBVKB21•titleŸ4Shortest Paths and Centrality in Uncertain Networks.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450988©publisher±Proc. VLDB Endow.ßauthorsïÆArkaprava Saha±Ruben Brokkelkamp´Yllka Velaj∞Arijit Khan 0001∞Francesco Bonchi®Abstract⁄Computing the shortest path between a pair of nodes is a fundamental graph primitive, which has critical applications in vehicle routing, finding functional pathways in biological networks, survivable network design, among many others. In this work, we study shortest-path queries over uncertain networks, i.e., graphs where every edge is associated with a probability of existence. We show that, for a given path, it is #P-hard to compute the probability of it being the shortest path, and we also derive other interesting properties highlighting the complexity of computing the Most Probable Shortest Paths (MPSPs). We thus devise sampling-based efficient algorithms, with end-to-end accuracy guarantees, to compute the MPSP. As a concrete application, we show how to compute a novel concept of betweenness centrality in an uncertain graph using MPSPs. Our thorough experimental results and rich real-world case studies on sensor networks and brain networks validate the effectiveness, efficiency, scalability, and usefulness of our solution.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450988®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÉ©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/DurnerCL21•titleŸACrystal: A Unified Cache Storage System for Analytical Databases.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476292©publisher±Proc. VLDB Endow.ßauthorsìÆDominik Durner¥Badrish Chandramouli®Yinan Li®Abstract⁄:Cloud analytical databases employ a disaggregated storage model, where the elastic compute layer accesses data persisted on remote cloud storage in block-oriented columnar formats. Given the high latency and low bandwidth to remote storage and the limited size of fast local storage, caching data at the compute node is important and has resulted in a renewed interest in caching for analytics. Today, each DBMS builds its own caching solution, usually based on file-or block-level LRU. In this paper, we advocate a new architecture of a smart cache storage system called Crystal, that is co-located with compute. Crystal's clients are DBMS-specific "data sources" with push-down predicates. Similar in spirit to a DBMS, Crystal incorporates query processing and optimization components focusing on efficient caching and serving of single-table hyper-rectangles called regions. Results show that Crystal, with a small DBMS-specific data source connector, can significantly improve query latencies on unmodified Spark and Greenplum while also saving on bandwidth from remote storage.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476292®Keywordsê¶Badges¿•Track∞research-article®Citation®Download|©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/YangSX0LB20•titleŸ7Scaling Attributed Network Embedding to Massive Graphs.§year§2020£doiŸ(https://doi.org/10.14778/3421424.3421430©publisher±Proc. VLDB Endow.ßauthorsñ´Renchi Yang´Jieming Shi¨Xiaokui Xiao≠Yin Yang 0001¨Juncheng Liu≤Sourav S. Bhowmick®Abstract⁄JGiven a graph G where each node is associated with a set of attributes, attributed network embedding (ANE) maps each node v ‚àà G to a compact vector Xv, which can be used in downstream machine learning tasks. Ideally, Xv should capture node v's affinity to each attribute, which considers not only v's own attribute associations, but also those of its connected nodes along edges in G. It is challenging to obtain high-utility embeddings that enable accurate predictions; scaling effective ANE computation to massive graphs with millions of nodes pushes the difficulty of the problem to a whole new level. Existing solutions largely fail on such graphs, leading to prohibitive costs, low-quality embeddings, or both.This paper proposes PANE, an effective and scalable approach to ANE computation for massive graphs that achieves state-of-the-art result quality on multiple benchmark datasets, measured by the accuracy of three common prediction tasks: attribute inference, link prediction, and node classification. In particular, for the large MAG data with over 59 million nodes, 0.98 billion edges, and 2000 attributes, PANE is the only known viable solution that obtains effective embeddings on a single server, within 12 hours.PANE obtains high scalability and effectiveness through three main algorithmic designs. First, it formulates the learning objective based on a novel random walk model for attributed networks. The resulting optimization task is still challenging on large graphs. Second, PANE includes a highly efficient solver for the above optimization problem, whose key module is a carefully designed initialization of the embeddings, which drastically reduces the number of iterations required to converge. Finally, PANE utilizes multi-core CPUs through non-trivial parallelization of the above solver, which achieves scalability while retaining the high quality of the resulting embeddings. Extensive experiments, comparing 10 existing approaches on 8 real datasets, demonstrate that PANE consistently outperforms all existing methods in terms of result quality, while being orders of magnitude faster.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3421424.3421430®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadv©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/KangMVBZ20•titleŸNJointly Optimizing Preprocessing and Inference for DNN-based Visual Analytics.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425881©publisher±Proc. VLDB Endow.ßauthorsï´Daniel Kang¨Ankit Mathur≥Teja Veeramacheneni¨Peter Bailis≠Matei Zaharia®Abstract⁄ÉWhile deep neural networks (DNNs) are an increasingly popular way to query large corpora of data, their significant runtime remains an active area of research. As a result, researchers have proposed systems and optimizations to reduce these costs by allowing users to trade off accuracy and speed. In this work, we examine end-to-end DNN execution in visual analytics systems on modern accelerators. Through a novel measurement study, we show that the preprocessing of data (e.g., decoding, resizing) can be the bottleneck in many visual analytics systems on modern hardware.To address the bottleneck of preprocessing, we introduce two optimizations for end-to-end visual analytics systems. First, we introduce novel methods of achieving accuracy and throughput trade-offs by using natively present, low-resolution visual data. Second, we develop a runtime engine for efficient visual DNN inference. This runtime engine a) efficiently pipelines preprocessing and DNN execution for inference, b) places preprocessing operations on the CPU or GPU in a hardware- and input-aware manner, and c) efficiently manages memory and threading for high throughput execution. We implement these optimizations in a novel system, Smol, and evaluate Smol on eight visual datasets. We show that its optimizations can achieve up to 5.9X end-to-end throughput improvements at a fixed accuracy over recent work in visual analytics.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425881®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadg©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/CaiLWX21•titleŸ>Data Synthesis via Differentially Private Markov Random Field.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476272©publisher±Proc. VLDB Endow.ßauthorsî™Kuntai Cai™Xiaoyu Lei∞Jianxin Wei 0005¨Xiaokui Xiao®Abstract⁄ÿThis paper studies the synthesis of high-dimensional datasets with differential privacy (DP). The state-of-the-art solution addresses this problem by first generating a set M of noisy low-dimensional marginals of the input data D, and then use them to approximate the data distribution in D for synthetic data generation. However, it imposes several constraints on M that considerably limits the choices of marginals. This makes it difficult to capture all important correlations among attributes, which in turn degrades the quality of the resulting synthetic data.To address the above deficiency, we propose PrivMRF, a method that (i) also utilizes a set M of low-dimensional marginals for synthesizing high-dimensional data with DP, but (ii) provides a high degree of flexibility in the choices of marginals. The key idea of PrivMRF is to select an appropriate M to construct a Markov random field (MRF) that models the correlations among the attributes in the input data, and then use the MRF for data synthesis. Experimental results on four benchmark datasets show that PrivMRF consistently outperforms the state of the art in terms of the accuracy of counting queries and classification tasks conducted on the synthetic data generated.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476272®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadQ©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/LuH0F20•titleŸ6Accelerating Exact Constrained Shortest Paths on GPUs.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436914©publisher±Proc. VLDB Endow.ßauthorsî≠Shengliang Lu¨Bingsheng HeÆYuchen Li 0001¶Hao Fu®Abstract⁄>The recently emerging applications such as software-defined networks and autonomous vehicles require efficient and exact solutions for constrained shortest paths (CSP), which finds the shortest path in a graph while satisfying some user-defined constraints. Compared with the common shortest path problems without constraints, CSP queries have a significantly larger number of subproblems. The most widely used labeling algorithm becomes prohibitively slow and impractical. Other existing approaches tend to find approximate solutions and build costly indices on graphs for fast query processing, which are not suitable for emerging applications with the requirement of exact solutions. A natural question is whether and how we can efficiently find the exact solution for CSP.In this paper, we propose Vine, a framework that parallelizes the labeling algorithm to efficiently find the exact CSP solution using GPUs. The major challenge addressed in Vine is how to deal with a large number of subproblems that are mostly unpromising but require a significant amount of memory and computational resources. Our solution is twofold. First, we develop a two-level pruning approach to eliminate the subproblems by making good use of the GPU's hierarchical memory. Second, we propose an adaptive parallelism control model based on the observations that the degree of parallelism (DOP) is the key to performance optimization with the given amount of computational resources. Extensive experiments show that Vine achieves 18√ó speedup on average over the widely adopted CPU-based solution running on 40 CPU threads. Vine also has over 5√ó speedup compared with a GPU approach that statically controls the DOP. Compared to the state-of-the-art approximate solution with preprocessed indices, Vine provides exact results with competitive or even better performance.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436914®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadP©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/SunX0HL021•titleŸHFinding Group Steiner Trees in Graphs with both Vertex and Edge Weights.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450982©publisher±Proc. VLDB Endow.ßauthorsñÆYahui Sun 0001¨Xiaokui Xiao¨Bin Cui 0001≤Saman K. Halgamuge∞Theodoros Lappas¨Jun Luo 0001®Abstract⁄hGiven an undirected graph and a number of vertex groups, the group Steiner trees problem is to find a tree such that (i) this tree contains at least one vertex in each vertex group; and (ii) the sum of vertex and edge weights in this tree is minimized. Solving this problem is useful in various scenarios, ranging from social networks to knowledge graphs. Most existing work focuses on solving this problem in vertex-unweighted graphs, and not enough work has been done to solve this problem in graphs with both vertex and edge weights. Here, we develop several algorithms to address this issue. Initially, we extend two algorithms from vertex-unweighted graphs to vertex- and edge-weighted graphs. The first one has no approximation guarantee, but often produces good solutions in practice. The second one has an approximation guarantee of |Œì| - 1, where |Œì| is the number of vertex groups. Since the extended (|Œì| - 1)-approximation algorithm is too slow when all vertex groups are large, we develop two new (|Œì| - 1)-approximation algorithms that overcome this weakness. Furthermore, by employing a dynamic programming approach, we develop another (|Œì| - h + 1)-approximation algorithm, where h is a parameter between 2 and |Œì|. Experiments show that, while no algorithm is the best in all cases, our algorithms considerably outperform the state of the art in many scenarios.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450982®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadL©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/RoyJGOGRMJ21•titleŸJSparkCruise: Workload Optimization in Managed Spark Clusters at Microsoft.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476388©publisher±Proc. VLDB Endow.ßauthorsò¨Abhishek Roy¨Alekh Jindal∞Priyanka GomatamÆXiating Ouyang≠Ashit Gosalia¨Nishkam Ravi´Swinky Mann¨Prakhar Jain®Abstract⁄AToday cloud companies offer fully managed Spark services. This has made it easy to onboard new customers but has also increased the volume of users and their workload sizes. However, both cloud providers and users lack the tools and time to optimize these massive workloads. To solve this problem, we designed SparkCruise that can help understand and optimize workload instances by adding a workload-driven feedback loop to the Spark query optimizer. In this paper, we present our approach to collecting and representing Spark query workloads and use it to improve the overall performance on the workload, all without requiring any access to user data. These methods scale with the number of workloads and apply learned feedback in an online fashion. We explain one specific workload optimization developed for computation reuse. We also share the detailed analysis of production Spark workloads and contrast them with the corresponding analysis of TPC-DS benchmark. To the best of our knowledge, this is the first study to share the analysis of large-scale production Spark SQL workloads.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476388®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadE©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/WangXY021•titleŸfA Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476255©publisher±Proc. VLDB Endow.ßauthorsî≠Mengzhao Wang¨Xiaoliang Xu©Qiang Yue±Yuxiang Wang 0001®Abstract⁄pApproximate nearest neighbor search (ANNS) constitutes an important operation in a multitude of applications, including recommendation systems, information retrieval, and pattern recognition. In the past decade, graph-based ANNS algorithms have been the leading paradigm in this domain, with dozens of graph-based ANNS algorithms proposed. Such algorithms aim to provide effective, efficient solutions for retrieving the nearest neighbors for a given query. Nevertheless, these efforts focus on developing and optimizing algorithms with different approaches, so there is a real need for a comprehensive survey about the approaches' relative performance, strengths, and pitfalls. Thus here we provide a thorough comparative analysis and experimental evaluation of 13 representative graph-based ANNS algorithms via a new taxonomy and fine-grained pipeline. We compared each algorithm in a uniform test environment on eight real-world datasets and 12 synthetic datasets with varying sizes and characteristics. Our study yields novel discoveries, offerings several useful principles to improve algorithms, thus designing an optimized method that outperforms the state-of-the-art algorithms. This effort also helped us pinpoint algorithms' working portions, along with rule-of-thumb recommendations about promising research directions and suitable algorithms for practitioners in different fields.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476255®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadD©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/Kraska21•titleŸ(Towards instance-optimized data systems.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476392©publisher±Proc. VLDB Endow.ßauthorsë™Tim Kraska®Abstract⁄jIn recent years, we have seen increased interest in applying machine learning to system problems. For example, there has been work on applying machine learning to improve query optimization, indexing, storage layouts, scheduling, log-structured merge trees, sorting, compression, and sketches, among many other data management tasks. Arguably, the ideas behind these techniques are similar: machine learning is used to model the data and/or workload in order to derive a more efficient algorithm or data structure. Ultimately, these techniques will allow us to build "instance-optimized" systems: that is, systems that self-adjust to a given workload and data distribution to provide unprecedented performance without the need for tuning by an administrator. While many of these techniques promise orders-of-magnitude better performance in lab settings, there is still general skepticism about how practical the current techniques really are.The following is intended as a progress report on ML for Systems and its readiness for real-world deployments, with a focus on our projects done as part of the Data Systems and AI Lab (DSAIL) at MIT By no means is it a comprehensive overview of all existing work, which has been steadily growing over the past several years not only in the database community but also in the systems, networking, theory, PL, and many other adjacent communities.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476392®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadC©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/DingCGN21•titleŸWDSB: A Decision Support Benchmark for Workload-Driven and Traditional Database Systems.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484234©publisher±Proc. VLDB Endow.ßauthorsî™Bailu Ding±Surajit ChaudhuriØJohannes Gehrke≤Vivek R. Narasayya®Abstract⁄ÄWe describe a new benchmark, DSB, for evaluating both workload-driven and traditional database systems on modern decision support workloads. DSB is adapted from the widely-used industrial-standard TPC-DS benchmark. It enhances the TPC-DS benchmark with complex data distribution and challenging yet semantically meaningful query templates. DSB also introduces configurable and dynamic workloads to assess the adaptability of database systems. Since workload-driven and traditional database systems have different performance dimensions, including the additional resources required for tuning and maintaining the systems, we provide guidelines on evaluation methodology and metrics to report. We show a case study on how to evaluate both workload-driven and traditional database systems with the DSB benchmark. The code for the DSB benchmark is open sourced and is available at https://aka.ms/dsb.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484234®Keywordsê¶Badges¿•Track∞research-article®Citation®Download>©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/LiuZCL021•titleŸ-Automatic Data Acquisition for Deep Learning.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476333©publisher±Proc. VLDB Endow.ßauthorsï™Jiabin Liu¶Fu ZhuØChengliang Chai®Yuyu Luo≠Nan Tang 0001®Abstract⁄£Deep learning (DL) has widespread applications and has revolutionized many industries. Although automated machine learning (AutoML) can help us away from coding for DL models, the acquisition of lots of high-quality data for model training remains a main bottleneck for many DL projects, simply because it requires high human cost. Despite many works on weak supervision (i.e., adding weak labels to seen data) and data augmentation (i.e., generating more data based on seen data), automatically acquiring training data, via smartly searching a pool of training data collected from open ML benchmarks and data markets, is not explored.In this demonstration, we demonstrate a new system, automatic data acquisition (AutoData), which automatically searches training data from a heterogeneous data repository and interacts with AutoML. It faces two main challenges. (1) How to search high-quality data from a large repository for a given DL task? (2) How does AutoData interact with AutoML to guide the search? To address these challenges, we propose a reinforcement learning (RL)-based framework in AutoData to guide the iterative search process. AutoData encodes current training data and feedbacks of AutoML, learns a policy to search fresh data, and trains in iterations. We demonstrate with two real-life scenarios, image classification and relational data prediction, showing that AutoData can select high-quality data to improve the model.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476333®Keywordsê¶Badges¿•Track∞research-article®Citation®Download=©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/ZhangYWSLW021•titleŸaGrain: Improving Data Efficiency of Graph Neural Networks via Diversified Influence Maximization.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476295©publisher±Proc. VLDB Endow.ßauthorsó¨Wentao Zhang≠Zhi Yang 0001™Yexin WangßYu Shen¨Yang Li 0106™Liang Wang¨Bin Cui 0001®Abstract⁄lData selection methods, such as active learning and core-set selection, are useful tools for improving the data efficiency of deep learning models on large-scale datasets. However, recent deep learning models have moved forward from independent and identically distributed data to graph-structured data, such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has led to the emergence of Graph Neural Networks (GNNs) that go beyond the models existing data selection methods are designed for. Therefore, we present GRAIN, an efficient framework that opens up a new perspective through connecting data selection in GNNs with social influence maximization. By exploiting the common patterns of GNNs, GRAIN introduces a novel feature propagation concept, a diversified influence maximization objective with novel influence and diversity functions, and a greedy algorithm with an approximation guarantee into a unified framework. Empirical studies on public datasets demonstrate that GRAIN significantly improves both the performance and efficiency of data selection (including active learning and core-set selection) for GNNs. To the best of our knowledge, this is the first attempt to bridge two largely parallel threads of research, data selection, and social influence maximization, in the setting of GNNs, paving new ways for improving data efficiency.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476295®Keywordsê¶Badges¿•Track∞research-article®Citation®Download<©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/BegoliACHKKMS21•titleŸvWatermarks in Stream Processing Systems: Semantics and Comparative Analysis of Apache Flink and Google Cloud Dataflow.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476389©publisher±Proc. VLDB Endow.ßauthorsò¨Edmon Begoli¨Tyler AkidauÆSlava Chernyak≠Fabian HueskeÆKathryn Knight≤Kenneth L. Knowles¨Daniel Mills≠Dan Sotolongo®Abstract⁄(Streaming data processing is an exercise in taming disorder: from oftentimes huge torrents of information, we hope to extract powerful and timely analyses. But when dealing with streaming data, the unbounded and temporally disordered nature of real-world streams introduces a critical challenge: how does one reason about the completeness of a stream that never ends? In this paper, we present a comprehensive definition and analysis of watermarks, a key tool for reasoning about temporal completeness in infinite streams.First, we describe what watermarks are and why they are important, highlighting how they address a suite of stream processing needs that are poorly served by eventually-consistent approaches:‚Ä¢ Computing a single correct answer, as in notifications.‚Ä¢ Reasoning about a lack of data, as in dip detection.‚Ä¢ Performing non-incremental processing over temporal subsets of an infinite stream, as in statistical anomaly detection with cubic spline models.‚Ä¢ Safely and punctually garbage collecting obsolete inputs and intermediate state.‚Ä¢ Surfacing a reliable signal of overall pipeline health.Second, we describe, evaluate, and compare the semantically equivalent, but starkly different, watermark implementations in two modern stream processing engines: Apache Flink and Google Cloud Dataflow.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476389®Keywordsê¶Badges¿•Track∞research-article®Citation®Download0©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/LiuLLCS21•titleŸ)Towards Crowd-aware Indoor Path Planning.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457401©publisher±Proc. VLDB Endow.ßauthorsï±Tiantian Liu 0003¨Huan Li 0003´Hua Lu 0001µMuhammad Aamir Cheema™Lidan Shou®Abstract⁄Indoor venues accommodate many people who collectively form crowds. Such crowds in turn influence people's routing choices, e.g., people may prefer to avoid crowded rooms when walking from A to B. This paper studies two types of crowd-aware indoor path planning queries. The Indoor Crowd-Aware Fastest Path Query (FPQ) finds a path with the shortest travel time in the presence of crowds, whereas the Indoor Least Crowded Path Query (LCPQ) finds a path encountering the least objects en route. To process the queries, we design a unified framework with three major components. First, an indoor crowd model organizes indoor topology and captures object flows between rooms. Second, a time-evolving population estimator derives room populations for a future timestamp to support crowd-aware routing cost computations in query processing. Third, two exact and two approximate query processing algorithms process each type of query. All algorithms are based on graph traversal over the indoor crowd model and use the same search framework with different strategies of updating the populations during the search process. All proposals are evaluated experimentally on synthetic and real data. The experimental results demonstrate the efficiency and scalability of our framework and query processing algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457401®Keywordsê¶Badges¿•Track∞research-article®Citation®Download0©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/LiMJ21•titleŸ2DENOUNCER: Detection of Unfairness in Classifiers.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476328©publisher±Proc. VLDB Endow.ßauthorsì™Jinyang Li∞Yuval MoskovitchÆH. V. Jagadish®Abstract⁄KThe use of automated data-driven tools for decision-making has gained popularity in recent years. At the same time, the reported cases of algorithmic bias and discrimination increase as well, which in turn lead to an extensive study of algorithmic fairness. Numerous notions of fairness have been proposed, designed to capture different scenarios. These measures typically refer to a "protected group" in the data, defined using values of some sensitive attributes. Confirming whether a fairness definition holds for a given group is a simple task, but detecting groups that are treated unfairly by the algorithm may be computationally prohibitive as the number of possible groups is combinatorial. We present a method for detecting such groups efficiently for various fairness definitions. Our solution is implemented in a system called DENOUNCER, an interactive system that allows users to explore different fairness measures of a (trained) classifier for a given test data. We propose to demonstrate the usefulness of DENOUNCER using real-life data and illustrate the effectiveness of our method.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476328®Keywordsê¶Badges¿•Track∞research-article®Citation®Download/©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/CeredaVCD21•titleŸàCGPTuner: a Contextual Gaussian Process Bandit Approach for the Automatic Tuning of IT Configurations Under Varying Workload Conditions.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457404©publisher±Proc. VLDB Endow.ßauthorsîÆStefano Cereda≤Stefano ValladaresØPaolo Cremonesi¨Stefano Doni®Abstract⁄£Properly selecting the configuration of a database management system (DBMS) is essential to increase performance and reduce costs. However, the task is astonishingly tricky due to a large number of tunable configuration parameters and their inter-dependencies. Also, the optimal configuration depends upon the workload to which the DBMS is exposed. To extract the full potential of a DBMS, we must also consider the entire IT stack on which the DBMS is running, comprising layers like the Java virtual machine, the operating system and the physical machine. Each layer offers a multitude of parameters that we should take into account. The available parameters vary as new software versions are released, making it impractical to rely on historical knowledge bases. We present a novel tuning approach for the DBMS configuration auto-tuning that quickly finds a well-performing configuration of an IT stack and adapts it to workload variations, without having to rely on a knowledge base. We evaluate the proposed approach using the Cassandra and MongoDB DBMSs, showing that it adjusts the suggested configuration to the observed workload and is portable across different IT applications. We try to minimise the memory consumption without increasing the response time, showing that the proposed approach reduces the response time and increases the memory requirements only under heavy-load conditions, reducing it again when the load decreases.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457404®Keywordsê¶Badges¿•Track∞research-article®Citation®Download+©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/MurraySKI21•titleŸ6tf.data: A Machine Learning Data Processing Framework.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476374©publisher±Proc. VLDB Endow.ßauthorsî≥Derek Gordon Murray™Jiri Simsa¨Ana Klimovic™Ihor Indyk®Abstract⁄"Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators that can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions enable users to focus on the application logic of data processing, while tf.data's runtime ensures that pipelines run efficiently.We demonstrate that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. We show that tf.data features, such as parallelism, caching, static optimizations, and optional non-deterministic execution are essential for high performance. Finally, we characterize machine learning input pipelines for millions of jobs that ran in Google's datacenter fleet, showing that input data processing is highly diverse and consumes a significant fraction of job resources. Our analysis motivates future research directions, such as sharing computation across jobs and pushing data projection to the storage layer.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476374®Keywordsê¶Badges¿•Track∞research-article®Citation®Download'©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/MinPPGIH21•titleŸNSymmetric Continuous Subgraph Matching with Bidirectional Dynamic Programming.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457395©publisher±Proc. VLDB Endow.ßauthorsñ≠Seunghwan MinÆSung Gwan Park´Kunsoo Park∞Dora Giammarresi¥Giuseppe F. Italiano≠Wook-Shin Han®Abstract⁄ÂIn many real datasets such as social media streams and cyber data sources, graphs change over time through a graph update stream of edge insertions and deletions. Detecting critical patterns in such dynamic graphs plays an important role in various application domains such as fraud detection, cyber security, and recommendation systems for social networks. Given a dynamic data graph and a query graph, the continuous subgraph matching problem is to find all positive matches for each edge insertion and all negative matches for each edge deletion. The state-of-the-art algorithm TurboFlux uses a spanning tree of a query graph for filtering. However, using the spanning tree may have a low pruning power because it does not take into account all edges of the query graph. In this paper, we present a symmetric and much faster algorithm SymBi which maintains an auxiliary data structure based on a directed acyclic graph instead of a spanning tree, which maintains the intermediate results of bidirectional dynamic programming between the query graph and the dynamic graph. Extensive experiments with real and synthetic datasets show that SymBi outperforms the state-of-the-art algorithm by up to three orders of magnitude in terms of the elapsed time.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457395®Keywordsê¶Badges¿•Track∞research-article®Citation®Download"©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/FanTWY21•titleŸ9Parallel Discrepancy Detection and Incremental Detection.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457400©publisher±Proc. VLDB Endow.ßauthorsî™Wenfei FanÆChao Tian 0001¨Yanghao WangÆQiang Yin 0002®Abstract⁄/This paper studies how to catch duplicates, mismatches and conflicts in the same process. We adopt a class of entity enhancing rules that embed machine learning predicates, unify entity resolution and conflict resolution, and are collectively defined across multiple relations. We detect discrepancies as violations of such rules. We establish the complexity of discrepancy detection and incremental detection problems with the rules; they are both NP-complete and W[1]-hard. To cope with the intractability and scale with large datasets, we develop parallel algorithms and parallel incremental algorithms for discrepancy detection. We show that both algorithms are parallelly scalable, i.e., they guarantee to reduce runtime when more processors are used. Moreover, the parallel incremental algorithm is relatively bounded. The complexity bounds and algorithms carry over to denial constraints, a special case of the entity enhancing rules. Using real-life and synthetic datasets, we experimentally verify the effectiveness, scalability and efficiency of the algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457400®Keywordsê¶Badges¿•Track∞research-article®Citation®Download"©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/JankovYLJ21•titleŸiDistributed Numerical and Machine Learning Computations via Two-Phase Execution of Aggregated Join Trees.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450991©publisher±Proc. VLDB Endow.ßauthorsî∞Dimitrije Jankov¨Binhang Yuan´Shangyu LuoÆChris Jermaine®Abstract⁄ˆWhen numerical and machine learning (ML) computations are expressed relationally, classical query execution strategies (hash-based joins and aggregations) can do a poor job distributing the computation. In this paper, we propose a two-phase execution strategy for numerical computations that are expressed relationally, as aggregated join trees (that is, expressed as a series of relational joins followed by an aggregation). In a pilot run, lineage information is collected; this lineage is used to optimally plan the computation at the level of individual records. Then, the computation is actually executed. We show experimentally that a relational system making use of this two-phase strategy can be an excellent platform for distributed ML computations.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450991®Keywordsê¶Badges¿•Track∞research-article®Citation®Download!©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/DongHYZ021•titleŸ4Butterfly-Core Community Search over Labeled Graphs.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476258©publisher±Proc. VLDB Endow.ßauthorsï™Zheng DongÆXin Huang 0001´Guorui Yuan´Hengshu ZhuÆHui Xiong 0001®Abstract⁄ÓCommunity search aims at finding densely connected subgraphs for query vertices in a graph. While this task has been studied widely in the literature, most of the existing works only focus on finding homogeneous communities rather than heterogeneous communities with different labels. In this paper, we motivate a new problem of cross-group community search, namely Butterfly-Core Community (BCC), over a labeled graph, where each vertex has a label indicating its properties and an edge between two vertices indicates their cross relationship. Specifically, for two query vertices with different labels, we aim to find a densely connected cross community that contains two query vertices and consists of butterfly networks, where each wing of the butterflies is induced by a k-core search based on one query vertex and two wings are connected by these butterflies. We first develop a heuristic algorithm achieving 2-approximation to the optimal solution. Furthermore, we design fast techniques of query distance computations, leader pair identifications, and index-based BCC local explorations. Extensive experiments on seven real datasets and four useful case studies validate the effectiveness and efficiency of our BCC and its multi-labeled extension models.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476258®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/Ives21•titleŸ;The future of data(base) education: Is the "cow book" dead?§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476394©publisher±Proc. VLDB Endow.ßauthorsë¨Zachary Ives®Abstract⁄üThis panel encourages a debate over the future of database education and its relationship to Data Science: Are Computer Science (CS) and Data Science (DS) different disciplines about to split, and how does that effect how we teach our field? Is there a "data" course that belongs in CS that all of our students should take? Who is the traditional database course, e.g. based on the "cow book", relevant to? What traditional topics should we not be teaching in our core data course(s) and which ones should be added? What do we teach the student who has one elective for data science? How does our community position itself for leadership in CS given the popularity of DS?©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476394®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/ZouDBIYJJ21•titleŸ;Lachesis: Automated Partitioning for UDF-Centric Analytics.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457392©publisher±Proc. VLDB Endow.ßauthorsó¨Jia Zou 0001´Amitabh DasÆPratik Barhate¨Arun Iyengar¨Binhang Yuan∞Dimitrije JankovÆChris Jermaine®Abstract⁄öPartitioning is effective in avoiding expensive shuffling operations. However, it remains a significant challenge to automate this process for Big Data analytics workloads that extensively use user defined functions (UDFs), where sub-computations are hard to be reused for partitionings compared to relational applications. In addition, functional dependency that is widely utilized for partitioning selection is often unavailable in the unstructured data that is ubiquitous in UDF-centric analytics. We propose the Lachesis system, which represents UDF-centric workloads as workflows of analyzable and reusable sub-computations. Lachesis further adopts a deep reinforcement learning model to infer which sub-computations should be used to partition the underlying data. This analysis is then applied to automatically optimize the storage of the data across applications to improve the performance and users' productivity.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457392®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ZhuIRDPBSJ21•titleŸ.Phoebe: A Learning-based Checkpoint Optimizer.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476298©publisher±Proc. VLDB Endow.ßauthorsò©Yiwen Zhu±Matteo Interlandi¨Abhishek RoyØKrishnadhan Das´Hiren Patel©Malay Bag≠Hitesh Sharma¨Alekh Jindal®Abstract⁄wEasy-to-use programming interfaces paired with cloud-scale processing engines have enabled big data system users to author arbitrarily complex analytical jobs over massive volumes of data. However, as the complexity and scale of analytical jobs increase, they encounter a number of unforeseen problems, hotspots with large intermediate data on temporary storage, longer job recovery time after failures, and worse query optimizer estimates being examples of issues that we are facing at Microsoft.To address these issues, we propose Phoebe, an efficient learning-based checkpoint optimizer. Given a set of constraints and an objective function at compile-time, Phoebe is able to determine the decomposition of job plans, and the optimal set of checkpoints to preserve their outputs to durable global storage. Phoebe consists of three machine learning predictors and one optimization module. For each stage of a job, Phoebe makes accurate predictions for: (1) the execution time, (2) the output size, and (3) the start/end time taking into account the inter-stage dependencies. Using these predictions, we formulate checkpoint optimization as an integer programming problem and propose a scalable heuristic algorithm that meets the latency requirement of the production environment.We demonstrate the effectiveness of Phoebe in production workloads, and show that we can free the temporary storage on hotspots by more than 70% and restart failed jobs 68% faster on average with minimum performance impact. Phoebe also illustrates that adding multiple sets of checkpoints is not cost-efficient, which dramatically reduces the complexity of the optimization.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476298®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/0004ZXP21•titleŸ]SlimChain: Scaling Blockchain Transactions through Off-Chain Storage and Parallel Processing.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476283©publisher±Proc. VLDB Endow.ßauthorsî≠Cheng Xu 0004≠Ce Zhang 0007¨Jianliang Xu®Jian Pei®Abstract⁄!Blockchain technology has emerged as the cornerstone of many decentralized applications operating among otherwise untrusted peers. However, it is well known that existing blockchain systems do not scale well. Transactions are often executed and committed sequentially in order to maintain the same view of the total order. Furthermore, it is necessary to duplicate both transaction data and their executions in every node in the blockchain network for integrity assurance. Such storage and computation requirements put significant burdens on the blockchain system, not only limiting system scalability but also undermining system security and robustness by making the network more centralized. To tackle these problems, in this paper, we propose SlimChain, a novel blockchain system that scales transactions through off-chain storage and parallel processing. Advocating a stateless design, SlimChain maintains only the short commitments of ledger states on-chain while dedicating transaction executions and data storage to off-chain nodes. To realize SlimChain, we propose new schemes for off-chain smart contract execution, on-chain transaction validation, and state commitment. We also propose optimizations to reduce network transmissions and a new sharding technique to improve system scalability further. Extensive experiments are conducted to validate the performance of the proposed SlimChain system. Compared with the existing systems, SlimChain reduces the on-chain storage requirements by 97% ~ 99%, while also improving the peak throughput by 1.4√ó ~ 15.6√ó.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476283®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÕa©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ChapmanMST20•titleŸZCapturing and querying fine-grained provenance of preprocessing pipelines in data science.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436911©publisher±Proc. VLDB Endow.ßauthorsîØAdriane Chapman≠Paolo Missier∞Giulia Simonelli∞Riccardo Torlone®Abstract⁄wData processing pipelines that are designed to clean, transform and alter data in preparation for learning predictive models, have an impact on those models' accuracy and performance, as well on other properties, such as model fairness. It is therefore important to provide developers with the means to gain an in-depth understanding of how the pipeline steps affect the data, from the raw input to training sets ready to be used for learning. While other efforts track creation and changes of pipelines of relational operators, in this work we analyze the typical operations of data preparation within a machine learning process, and provide infrastructure for generating very granular provenance records from it, at the level of individual elements within a dataset. Our contributions include: (i) the formal definition of a core set of preprocessing operators, and the definition of provenance patterns for each of them, and (ii) a prototype implementation of an application-level provenance capture library that works alongside Python. We report on provenance processing and storage overhead and scalability experiments, carried out over both real ML benchmark pipelines and over TCP-DI, and show how the resulting provenance can be used to answer a suite of provenance benchmark queries that underpin some of the developers' debugging questions, as expressed on the Data Science Stack Exchange.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436911®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÕ©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/XiaYPD20•titleŸOTaurus: Lightweight Parallel Logging for In-Memory Database Management Systems.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425889©publisher±Proc. VLDB Endow.ßauthorsî´Yu Xia 0005´Xiangyao Yu¨Andrew Pavlo∞Srinivas Devadas®Abstract⁄Existing single-stream logging schemes are unsuitable for in-memory database management systems (DBMSs) as the single log is often a performance bottleneck. To overcome this problem, we present Taurus, an efficient parallel logging scheme that uses multiple log streams, and is compatible with both data and command logging. Taurus tracks and encodes transaction dependencies using a vector of log sequence numbers (LSNs). These vectors ensure that the dependencies are fully captured in logging and correctly enforced in recovery. Our experimental evaluation with an in-memory DBMS shows that Taurus's parallel logging achieves up to 9.9X and 2.9X speedups over single-streamed data logging and command logging, respectively. It also enables the DBMS to recover up to 22.9X and 75.6X faster than these baselines for data and command logging, respectively. We also compare Taurus with two state-of-the-art parallel logging schemes and show that the DBMS achieves up to 2.8X better performance on NVMe drives and 9.2X on HDDs.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425889®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÏ©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/GaoCLZ21•titleŸKICS-GNN: Lightweight Interactive Community Search via Graph Neural Network.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447704©publisher±Proc. VLDB Endow.ßauthorsî¨Jun Gao 0003´Jiazun Chen¨Zhao Li 0007≠Ji Zhang 0001®Abstract⁄ºSearching a community containing a given query vertex in an online social network enjoys wide applications like recommendation, team organization, etc. When applied to real-life networks, the existing approaches face two major limitations. First, they usually take two steps, i.e., crawling a large part of the network first and then finding the community next, but the entire network is usually too big and most of the data are not interesting to end users. Second, the existing methods utilize hand-crafted rules to measure community membership, while it is very difficult to define effective rules as the communities are flexible for different query vertices. In this paper, we propose an Interactive Community Search method based on Graph Neural Network (shortened by ICS-GNN) to locate the target community over a subgraph collected on the fly from an online network. Specifically, we recast the community membership problem as a vertex classification problem using GNN, which captures similarities between the graph vertices and the query vertex by combining content and structural features seamlessly and flexibly under the guide of users' labeling. We then introduce a k-sized Maximum-GNN-scores (shortened by kMG) community to describe the target community. We next discover the target community iteratively and interactively. In each iteration, we build a candidate subgraph using the crawled pages with the guide of the query vertex and labeled vertices, infer the vertex scores with a GNN model trained on the subgraph, and discover the kMG community which will be evaluated by end users to acquire more feedback. Besides, two optimization strategies are proposed to combine ranking loss into the GNN model and search more space in the target community location. We conduct the experiments in both offline and online real-life data sets, and demonstrate that ICS-GNN can produce effective communities with low overhead in communication, computation, and user labeling.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447704®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ√©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/ZhangDMYJ0S021•titleŸURefiner: A Reliable Incentive-Driven Federated Learning System Powered by Blockchain.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476313©publisher±Proc. VLDB Endow.ßauthorsò¨Zhebin Zhang™Dajie Dong©Yuhang Ma´Yilong Ying´Dawei Jiang¨Ke Chen 0005™Lidan ShouÆGang Chen 0001®Abstract⁄‰Modern mobile applications often produce decentralized data, i.e., a huge amount of privacy-sensitive data distributed over a large number of mobile devices. Techniques for learning models from decentralized data must properly handle two natures of such data, namely privacy and massive engagement. Federated learning (FL) is a promising approach for such a learning task since the technique learns models from data without exposing privacy. However, traditional FL methods assume that the participating mobile devices are honest volunteers. This assumption makes traditional FL methods unsuitable for applications where two kinds of participants are engaged: 1) self-interested participants who, without economical stimulus, are reluctant to contribute their computing resources unconditionally, and 2) malicious participants who send corrupt updates to disrupt the learning process. This paper proposes Refiner, a reliable federated learning system for tackling the challenges introduced by massive engagements of self-interested and malicious participants. Refiner is built upon Ethereum, a public blockchain platform. To engage self-interested participants, we introduce an incentive mechanism which rewards each participant in terms of the amount of its training data and the performance of its local updates. To handle malicious participants, we propose an audit scheme which employs a committee of randomly chosen validators for punishing them with no reward and preclude corrupt updates from the global model. The proposed incentive and audit scheme is implemented with cryptocurrency and smart contract, two primitives offered by Ethereum. This paper demonstrates the main features of Refiner by training a digit classification model on the MNIST dataset.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476313®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ√©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/HellingsS21•titleŸ-ByShard: Sharding in a Byzantine Environment.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476275©publisher±Proc. VLDB Endow.ßauthorsíÆJelle Hellings∞Mohammad Sadoghi®Abstract⁄‡The emergence of blockchains has fueled the development of resilient systems that can deal with Byzantine failures due to crashes, bugs, or even malicious behavior. Recently, we have also seen the exploration of sharding in these resilient systems, this to provide the scalability required by very large data-based applications. Unfortunately, current sharded resilient systems all use system-specific specialized approaches toward sharding that do not provide the flexibility of traditional sharded data management systems.To improve on this situation, we fundamentally look at the design of sharded resilient systems. We do so by introducing BYSHARD, a unifying framework for the study of sharded resilient systems. Within this framework, we show how two-phase commit and two-phase locking---two techniques central to providing atomicity and isolation in traditional sharded databases---can be implemented efficiently in a Byzantine environment, this with a minimal usage of costly Byzantine resilient primitives. Based on these techniques, we propose eighteen multi-shard transaction processing protocols. Finally, we practically evaluate these protocols and show that each protocol supports high transaction throughput and provides scalability while each striking its own trade-off between throughput, isolation level, latency, and abort rate. As such, our work provides a strong foundation for the development of ACID-compliant general-purpose and flexible sharded resilient data management systems.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476275®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃ†©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Ji0YAYS21•titleŸNDifferentially Private Binary- and Matrix-Valued Data Query: An XOR Mechanism.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446106©publisher±Proc. VLDB Endow.ßauthorsñ©Tianxi Ji´Pan Li 0001´Emre Yilmaz´Erman AydayØYanfang Ye 0001´Jinyuan Sun®Abstract⁄Differential privacy has been widely adopted to release continuous- and scalar-valued information on a database without compromising the privacy of individual data records in it. The problem of querying binary- and matrix-valued information on a database in a differentially private manner has rarely been studied. However, binary- and matrix-valued data are ubiquitous in real-world applications, whose privacy concerns may arise under a variety of circumstances. In this paper, we devise an exclusive or (XOR) mechanism that perturbs binary- and matrix-valued query result by conducting an XOR operation on the query result with calibrated noises attributed to a matrix-valued Bernoulli distribution. We first rigorously analyze the privacy and utility guarantee of the proposed XOR mechanism. Then, to generate the parameters in the matrix-valued Bernoulli distribution, we develop a heuristic approach to minimize the expected square query error rate under œµ-differential privacy constraint. Additionally, to address the intractability of calculating the probability density function (PDF) of this distribution and efficiently generate samples from it, we adapt an Exact Hamiltonian Monte Carlo based sampling scheme. Finally, we experimentally demonstrate the efficacy of the XOR mechanism by considering binary data classification and social network analysis, all in a differentially private manner. Experiment results show that the XOR mechanism notably outperforms other state-of-the-art differentially private methods in terms of utility (such as classification accuracy and F1 score), and even achieves comparable utility to the non-private mechanisms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446106®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃù©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/YanCJCSWHYCL21•titleŸSRevisiting the Design of LSM-tree Based OLTP Storage Engine with Persistent Memory.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467875©publisher±Proc. VLDB Endow.ßauthorsö™Baoyue Yan¨Xuntao Cheng®Bo Jiang´Shibin Chen≠Canfang Shang≠Jianying Wang´Kenry Huang´Xinjun YangßWei CaoÆFeifei Li 0001®Abstract⁄FThe recent byte-addressable and large-capacity commercialized persistent memory (PM) is promising to drive database as a service (DBaaS) into unchartered territories. This paper investigates how to leverage PMs to revisit the conventional LSM-tree based OLTP storage engines designed for DRAM-SSD hierarchy for DBaaS instances. Specifically we (1) propose a light-weight PM allocator named Hal-loc customized for LSM-tree, (2) build a high-performance Semi-persistent Memtable utilizing the persistent in-memory writes of PM, (3) design a concurrent commit algorithm named Reorder Ring to aschieve log-free transaction processing for OLTP workloads and (4) present a Global Index as the new globally sorted persistent level with non-blocking in-memory compaction. The design of Reorder Ring and Semi-persistent Memtable achieves fast writes without synchronized logging overheads and achieves near instant recovery time. Moreover, the design of Semi-persistent Memtable and Global Index with in-memory compaction enables the byte-addressable persistent levels in PM, which significantly reduces the read and write amplification as well as the background compaction overheads. The overall evaluation shows that the performance of our proposal over PM-SSD hierarchy outperforms the baseline by up to 3.8x in YCSB benchmark and by 2x in TPC-C benchmark.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467875®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃó©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/CormodeMM21•titleŸ6Frequency Estimation under Local Differential Privacy.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476261©publisher±Proc. VLDB Endow.ßauthorsìÆGraham CormodeÆSamuel Maddock≠Carsten Maple®Abstract⁄◊Private collection of statistics from a large distributed population is an important problem, and has led to large scale deployments from several leading technology companies. The dominant approach requires each user to randomly perturb their input, leading to guarantees in the local differential privacy model. In this paper, we place the various approaches that have been suggested into a common framework, and perform an extensive series of experiments to understand the tradeoffs between different implementation choices. Our conclusion is that for the core problems of frequency estimation and heavy hitter identification, careful choice of algorithms can lead to very effective solutions that scale to millions of users.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476261®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃí©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/GuoZLC21•titleŸ?Full Encryption: An end to end encryption mechanism in GaussDB.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476351©publisher±Proc. VLDB Endow.ßauthorsî©Liang Guo™Jinwei Zhu´Jiayang Liu©Kun Cheng®Abstract⁄˙In this paper, we present a novel mechanism called Full Encryption (FE) in GaussDB. FE-in-GaussDB provides column-level encryption for sensitive data, and secures the asset from any malicious cloud administrator or information leakage attack. It ensures not only the security of operations on ciphertext data, but also the efficiency of query execution, by combining the advantages of cryptography algorithms (i.e. software mode) and Trusted Execution Environment (i.e. hardware mode). With this, FE-in-GaussDB supports full-scene query processing including the matching, the comparison and other rich computing functionalities. We demonstrate the prototype of FE-in-GaussDB and an experimental performance evaluation to prove its availability and effectiveness.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476351®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadÃÜ©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/BaoYXD21•titleŸXCGM: An Enhanced Mechanism for Streaming Data Collectionwith Local Differential Privacy.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476277©publisher±Proc. VLDB Endow.ßauthorsî™Ergute Bao≠Yin Yang 0001¨Xiaokui Xiao™Bolin Ding®Abstract⁄‡Local differential privacy (LDP) is a well-established privacy protection scheme for collecting sensitive data, which has been integrated into major platforms such as iOS, Chrome, and Windows. The main idea is that each individual randomly perturbs her data on her local device, and only uploads the noisy version to an untrusted data aggregator. This paper focuses on the collection of streaming data consisting of regular updates, e.g., daily app usage. Such streams, when aggregated over a large population, often exhibit strong autocorrelations, e.g., the average usage of an app usually does not change dramatically from one day to the next. To our knowledge, this property has been largely neglected in existing LDP mechanisms. Consequently, data collected with current LDP methods often exhibit unrealistically violent fluctuations due to the added noise, drowning the overall trend, as shown in our experiments.This paper proposes a novel correlated Gaussian mechanism (CGM) for enforcing (œµ, Œ¥)-LDP on streaming data collection, which reduces noise by exploiting public-known autocorrelation patterns of the aggregated data. This is done through non-trivial modifications to the core of the underlying Gaussian Mechanism; in particular, CGM injects temporally correlated noise, computed through an optimization program that takes into account the given autocorrelation pattern, data value range, and utility metric. CGM comes with formal proof of correctness, and consumes negligible computational resources. Extensive experiments using real datasets from different application domains demonstrate that CGM achieves consistent and significant utility gains compared to the baseline method of repeatedly running the underlying one-shot LDP mechanism.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476277®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadz©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/JindalI21•titleŸYMachine Learning for Cloud Data Systems: the Promise, the Progress, and the Path Forward.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476408©publisher±Proc. VLDB Endow.ßauthorsí¨Alekh Jindal±Matteo Interlandi®Abstract⁄fThe goal of this tutorial is to educate the audience about the state of the art in ML for cloud data systems, both in research and in practice. The tutorial is divided in two parts: the progress, and the path forward.Part I covers the recent successes in deploying machine learning solutions for cloud data systems. We will discuss the practical considerations taken into account and the progress made at various levels. The goal is to compare and contrast the promise of ML for systems with the ground actually covered in industry.Finally, Part II discusses practical issues of machine learning in the enterprise covering the generation of explanations, model debugging, model deployment, model management, constraints on eyes-on data usage and anonymization, and a discussion of the technical debt that can accrue through machine learning and models in the enterprise.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476408®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadw©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/WangXS21•titleŸ]Heracles: An Efficient Storage Model And Data Flushing For Performance Monitoring Timeseries.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447710©publisher±Proc. VLDB Endow.ßauthorsì™Zhiqi WangßJin Xue©Zili Shao®Abstract⁄	3Performance-monitoring timeseries systems such as Prometheus and InfluxDB play a critical role in assuring reliability and operationally. These systems commonly adopt a column-oriented storage model, by which timeseries samples from different time-series are separated, and all samples (with both numeric values and timestamps) in one timeseries are grouped into chunks and stored together. As a group of timeseries are often collected from the same source with the same timestamps, managing timestamps and metrics in a group manner provides more opportunities for query and insertion optimization but posts new challenges as well. Besides, for performance monitoring systems, to support better compression and efficient queries for most recent data that are most likely accessed by users, huge volumes of data are first cached in memory and then periodically flushed to disks. Periodic data flushing incurs high IO overhead, and simply discarding flushed data, which can still serve queries, not only is a waste but also brings huge memory reclamation cost. In this paper, we propose Heracles which integrates two techniques - (1) a new storage model, which enables efficient queries on compressed data by utilizing the shared timestamp column to easily locate corresponding metric values; (2) a novel two-level epoch-based memory manager, which allows the system to gradually flush and reclaim in-memory data while unreclaimed data can still serve queries. Heracles is implemented as a standalone module that can be easily integrated into existing performance monitoring timeseries systems. We have implemented a fully functional prototype with Heracles based on Prometheus tsdb, a representative open-source performance monitoring system, and conducted extensive experiments with real and synthetic timeseries data. Experimental results show that, compared with Prometheus, Heracles can improve the insertion throughput by 171%, and reduce the query latency and space usage by 32% and 30%, respectively, on average. Besides, to compare with other state-of-the-art storage techniques, we have integrated LevelDB (for LSM-tree-based structure) and Parquet (for column stores) into Prometheus tsdb, respectively, and experimental results show Heracles outperform these two integrations. We have released the open-source code of Heracles for public access.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447710®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadq©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Yin00PZ21•titleŸGParaX: Boosting Deep Learning for Big Data Analytics on Many-Core CPUs.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447692©publisher±Proc. VLDB Endow.ßauthorsï©Lujia Yin±Yiming Zhang 0003≥Zhaoning Zhang 0001´Yuxing Peng©Peng Zhao®Abstract⁄7Despite the fact that GPUs and accelerators are more efficient in deep learning (DL), commercial clouds like Facebook and Amazon now heavily use CPUs in DL computation because there are large numbers of CPUs which would otherwise sit idle during off-peak periods. Following the trend, CPU vendors have not only released high-performance many-core CPUs but also developed efficient math kernel libraries. However, current DL platforms cannot scale well to a large number of CPU cores, making many-core CPUs inefficient in DL computation. We analyze the memory access patterns of various layers and identify the root cause of the low scalability, i.e., the per-layer barriers that are implicitly imposed by current platforms which assign one single instance (i.e., one batch of input data) to a CPU. The barriers cause severe memory bandwidth contention and CPU starvation in the access-intensive layers (like activation and BN).This paper presents a novel approach called ParaX, which boosts the performance of DL on many-core CPUs by effectively alleviating bandwidth contention and CPU starvation. Our key idea is to assign one instance to each CPU core instead of to the entire CPU, so as to remove the per-layer barriers on the executions of the many cores. ParaX designs an ultralight scheduling policy which sufficiently overlaps the access-intensive layers with the compute-intensive ones to avoid contention, and proposes a NUMA-aware gradient server mechanism for training which leverages shared memory to substantially reduce the overhead of per-iteration parameter synchronization. We have implemented ParaX on MXNet. Extensive evaluation on a two-NUMA Intel 8280 CPU shows that ParaX significantly improves the training/inference throughput for all tested models (for image recognition and natural language processing) by 1.73X ~ 2.93X.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447692®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadl©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/ShaowangJMK21•titleŸODeclarative Data Serving: The Future of Machine Learning Inference on the Edge.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476302©publisher±Proc. VLDB Endow.ßauthorsî¨Ted Shaowang´Nilesh JainØDennis MatthewsØSanjay Krishnan®Abstract⁄ÛRecent advances in computer architecture and networking have ushered in a new age of edge computing, where computation is placed close to the point of data collection to facilitate low-latency decision making. As the complexity of such deployments grow into networks of interconnected edge devices, getting the necessary data to be in "the right place at the right time" can become a challenge. We envision a future of edge analytics where data flows between edge nodes are declaratively configured through high-level constraints. Using machine learning model-serving as a prototypical task, we illustrate how the heterogeneity and specialization of edge devices can lead to complex, task-specific communication patterns even in relatively simple situations. Without a declarative framework, managing this complexity will be challenging for developers and will lead to brittle systems. We conclude with a research vision for database community that brings our perspective to the emergent area of edge computing.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476302®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadj©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/ZhouSZKP21•titleŸGAccelerating Large Scale Real-Time GNN Inference using Channel Pruning.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461547©publisher±Proc. VLDB Endow.ßauthorsï≠Hongkuan Zhou≤Ajitesh Srivastava¨Hanqing ZengØRajgopal Kannan≤Viktor K. Prasanna®Abstract⁄»Graph Neural Networks (GNNs) are proven to be powerful models to generate node embedding for downstream applications. However, due to the high computation complexity of GNN inference, it is hard to deploy GNNs for large-scale or real-time applications. In this paper, we propose to accelerate GNN inference by pruning the dimensions in each layer with negligible accuracy loss. Our pruning framework uses a novel LASSO regression formulation for GNNs to identify feature dimensions (channels) that have high influence on the output activation. We identify two inference scenarios and design pruning schemes based on their computation and memory usage for each. To further reduce the inference complexity, we effectively store and reuse hidden features of visited nodes, which significantly reduces the number of supporting nodes needed to compute the target embedding. We evaluate the proposed method with the node classification problem on five popular datasets and a real-time spam detection application. We demonstrate that the pruned GNN models greatly reduce computation and memory usage with little accuracy loss. For full inference, the proposed method achieves an average of 3.27X speedup with only 0.002 drop in F1-Micro on GPU. For batched inference, the proposed method achieves an average of 6.67X speedup with only 0.003 drop in F1-Micro on CPU. To the best of our knowledge, we are the first to accelerate large scale real-time GNN inference through channel pruning.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461547®Keywordsê¶Badges¿•Track∞research-article®Citation®Downloadb©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/RahmanBLZSKP21•titleŸNNOAH: Interactive Spreadsheet Exploration with Dynamic Hierarchical Overviews.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447701©publisher±Proc. VLDB Endow.ßauthorsóØSajjadur RahmanÆMangesh Bendre™Yuyang Liu™Shichu Zhu´Zhaoyuan Su±Karrie Karahalios∂Aditya G. Parameswaran®Abstract⁄#Spreadsheet systems are by far the most popular platform for data exploration on the planet, supporting millions of rows of data. However, exploring spreadsheets that are this large via operations such as scrolling or issuing formulae can be overwhelming and error-prone. Users easily lose context and suffer from cognitive and mechanical burdens while issuing formulae on data spanning multiple screens. To address these challenges, we introduce dynamic hierarchical overviews that are embedded alongside spreadsheets. Users can employ this overview to explore the data at various granularities, zooming in and out of the spreadsheet. They can issue formulae over data subsets without cumbersome scrolling or range selection, enabling users to gain a high or low-level perspective of the spreadsheet. An implementation of our dynamic hierarchical overview, NOAH, integrated within DataSpread, preserves spreadsheet semantics and look and feel, while introducing such enhancements. Our user studies demonstrate that NOAH makes it more intuitive, easier, and faster to navigate spreadsheet data compared to traditional spreadsheets like Microsoft Excel and spreadsheet plug-ins like Pivot Table, for a variety of exploration tasks; participants made fewer mistakes in NOAH while being faster in completing the tasks.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447701®Keywordsê¶Badges¿•Track∞research-article®Citation®Download^©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/Sarkhel021•titleŸ`Improving Information Extraction from Visually Rich Documents using Visual Span Representations.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446104©publisher±Proc. VLDB Endow.ßauthorsíÆRitesh Sarkhel∞Arnab Nandi 0001®Abstract⁄ñAlong with textual content, visual features play an essential role in the semantics of visually rich documents. Information extraction (IE) tasks perform poorly on these documents if these visual cues are not taken into account. In this paper, we present Artemis - a visually aware, machine-learning-based IE method for heterogeneous visually rich documents. Artemis represents a visual span in a document by jointly encoding its visual and textual context for IE tasks. Our main contribution is two-fold. First, we develop a deep-learning model that identifies the local context boundary of a visual span with minimal human-labeling. Second, we describe a deep neural network that encodes the multimodal context of a visual span into a fixed-length vector by taking its textual and layout-specific features into account. It identifies the visual span(s) containing a named entity by leveraging this learned representation followed by an inference task. We evaluate Artemis on four heterogeneous datasets from different domains over a suite of information extraction tasks. Results show that it outperforms state-of-the-art text-based methods by up to 17 points in F1-score.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446104®Keywordsê¶Badges¿•Track∞research-article®Citation®Download]©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/LeisK21•titleŸ3Towards Cost-Optimal Query Processing in the Cloud.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461549©publisher±Proc. VLDB Endow.ßauthorsí´Viktor LeisµMaximilian Kuschewski®Abstract⁄GPublic cloud providers offer hundreds of heterogeneous hardware instances. For analytical query processing systems, this presents a major challenge: depending on the hardware configuration, performance and cost may differ by orders of magnitude. We propose a simple and intuitive model that takes the workload, hardware, and cost into account to determine the optimal instance configuration. We discuss how such a model-based approach can significantly reduce costs and also guide the evolution of cloud-native database systems to achieve our vision of cost-optimal query processing.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461549®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadY©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/WangMOCP21•titleŸHRandomized Error Removal for Online Spread Estimation in Data Streaming.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447707©publisher±Proc. VLDB Endow.ßauthorsïØHaibo Wang 0004©Chaoyi Ma≥Olufemi O. Odegbile¨Shigang Chen≠Jih-Kwon Peir®Abstract⁄åMeasuring flow spread in real time from large, high-rate data streams has numerous practical applications, where a data stream is modeled as a sequence of data items from different flows and the spread of a flow is the number of distinct items in the flow. Past decades have witnessed tremendous performance improvement for single-flow spread estimation. However, when dealing with numerous flows in a data stream, it remains a significant challenge to measure per-flow spread accurately while reducing memory footprint. The goal of this paper is to introduce new multi-flow spread estimation designs that incur much smaller processing overhead and query overhead than the state of the art, yet achieves significant accuracy improvement in spread estimation. We formally analyze the performance of these new designs. We implement them in both hardware and software, and use real-world data traces to evaluate their performance in comparison with the state of the art. The experimental results show that our best sketch significantly improves over the best existing work in terms of estimation accuracy, data item processing throughput, and online query throughput.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447707®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadV©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/RezigOAEMS21•titleŸ2Horizon: Scalable Dependency-driven Data Cleaning.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476301©publisher±Proc. VLDB Endow.ßauthorsñÆEl Kindi RezigÆMourad Ouzzani≠Walid G. Aref≥Ahmed K. Elmagarmid∞Ahmed R. Mahmood≥Michael Stonebraker®Abstract⁄xA large class of data repair algorithms rely on integrity constraints to detect and repair errors. A well-studied class of constraints is Functional Dependencies (FDs, for short). Although there has been an increased interest in developing general data cleaning systems for a myriad of data errors, scalability has been left behind. This is because current systems assume data cleaning is performed offline and in one iteration. However, developing data science pipelines is highly iterative and requires efficient cleaning techniques to scale to millions of records in seconds/minutes, not days. In our efforts to re-think the data cleaning stack and bring it to the era of data science, we introduce Horizon, an end-to-end FD repair system to address two key challenges: (1) Accuracy: Most existing FD repair techniques aim to produce repairs that minimize changes to the data that may lead to incorrect combinations of attribute values (or patterns). Horizon leverages the interaction between the data patterns induced by the various FDs, and subsequently selects repairs that preserve the most frequent patterns found in the original data, and hence leading to a better repair accuracy. (2) Scalability: Existing data cleaning systems struggle when dealing with large-scale real-world datasets. Horizon features a linear-time repair algorithm that scales to millions of records, and is orders-of-magnitude faster than state-of-the-art cleaning algorithms. A benchmark of Horizon against state-of-the-art cleaning systems on multiple datasets and metrics shows that Horizon consistently outperforms existing techniques in repair quality and scalability.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476301®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadV©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/HouCWW21•titleŸ8Massively Parallel Algorithms for Personalized PageRank.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461554©publisher±Proc. VLDB Endow.ßauthorsî´Guanhao HouÆXingguang ChenÆSibo Wang 0001™Zhewei Wei®Abstract⁄gPersonalized PageRank (PPR) has wide applications in search engines, social recommendations, community detection, and so on. Nowadays, graphs are becoming massive and many IT companies need to deal with large graphs that cannot be fitted into the memory of most commodity servers. However, most existing state-of-the-art solutions for PPR computation only work for single-machines and are inefficient for the distributed framework since such solutions either (i) result in an excessively large number of communication rounds, or (ii) incur high communication costs in each round.Motivated by this, we present Delta-Push, an efficient framework for single-source and top-k PPR queries in distributed settings. Our goal is to reduce the number of rounds while guaranteeing that the load, i.e., the maximum number of messages an executor sends or receives in a round, can be bounded by the capacity of each executor. We first present a non-trivial combination of a redesigned parallel push algorithm and the Monte-Carlo method to answer single-source PPR queries. The solution uses pre-sampled random walks to reduce the number of rounds for the push al6gorithm. Theoretical analysis under the Massively Parallel Computing (MPC) model shows that our proposed solution bounds the communication rounds to [EQUATION] under a load of O(m/p), where m is the number of edges of the input graph, p is the number of executors, and œµ is a user-defined error parameter. In the meantime, as the number of executors increases to p' = Œ≥ ¬∑ p, the load constraint can be relaxed since each executor can hold O(Œ≥ ¬∑ m/p') messages with invariant local memory. In such scenarios, multiple queries can be processed in batches simultaneously. We show that with a load of O(Œ≥ ¬∑ m/p'), our Delta-Push can process Œ≥ queries in a batch with [EQUATION] rounds, while other baseline solutions still keep the same round cost for each batch. We further present a new top-k algorithm that is friendly to the distributed framework and reduces the number of rounds required in practice. Extensive experiments show that our proposed solution is more efficient than alternatives.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461554®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadU©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/GubnerB21•titleŸ9Charting the Design Space of Query Execution using VOILA.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447709©publisher±Proc. VLDB Endow.ßauthorsí™Tim GubnerÆPeter A. Boncz®Abstract⁄£Database architecture, while having been studied for four decades now, has delivered only a few designs with well-understood properties. These few are followed by most actual systems. Acquiring more knowledge about the design space is a very time-consuming processes that requires manually crafting prototypes with a low chance of generating material insight.We propose a framework that aims to accelerate this exploration process significantly. Our framework enables synthesizing many different engines from a description in a carefully designed domain-specific language (VOILA). We explain basic concepts and formally define the semantics of VOILA. We demonstrate VOILA's flexibility by presenting translation back-ends that allow the synthesis of state-of-the-art paradigms (data-centric compilation, vectorized execution, AVX-512), mutations and mixes thereof.We show-case VOILA's flexibility by exploring the query engine design space in an automated fashion. We generated thousands of query engines and report our findings. Queries generated by VOILA achieve similar performance as state-of-the-art hand-optimized implementations and are up to 35.5X faster than well-known systems.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447709®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadR©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/Fent021•titleŸ8A Practical Approach to Groupjoin and Nested Aggregates.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476288©publisher±Proc. VLDB Endow.ßauthorsí¨Philipp Fent≥Thomas Neumann 0001®Abstract⁄fGroupjoins, the combined execution of a join and a subsequent group by, are common in analytical queries, and occur in about 1/8 of the queries in TPC-H and TPC-DS. While they were originally invented to improve performance, efficient parallel execution of groupjoins can be limited by contention, which limits their usefulness in a many-core system. Having an efficient implementation of groupjoins is highly desirable, as groupjoins are not only used to fuse group by and join but are also introduced by the unnesting component of the query optimizer to avoid nested-loops evaluation of aggregates. Furthermore, the query optimizer needs be able to reason over the result of aggregation in order to schedule it correctly. Traditional selectivity and cardinality estimations quickly reach their limits when faced with computed columns from nested aggregates, which leads to poor cost estimations and thus, suboptimal query plans.In this paper, we present techniques to efficiently estimate, plan, and execute groupjoins and nested aggregates. We propose two novel techniques, aggregate estimates to predict the result distribution of aggregates, and parallel groupjoin execution for a scalable execution of groupjoins. The resulting system has significantly better estimates and a contention-free evaluation of groupjoins, which can speed up some TPC-H queries up to a factor of 2.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476288®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadQ©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/FujiwaraKIKU21•titleŸ(Fast Algorithm for Anchor Graph Hashing.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447696©publisher±Proc. VLDB Endow.ßauthorsï±Yasuhiro FujiwaraØSekitoshi Kanai≠Yasutoshi Ida±Atsutoshi Kumagai¨Naonori Ueda®Abstract⁄Anchor graph hashing is used in many applications such as cancer detection, web page classification, and drug discovery. It computes the hash codes from the eigenvectors of the matrix representing the similarities between data points and anchor points; anchors refer to the points representing the data distribution. In performing an approximate nearest neighbor search, the hash codes of a query data point are determined by identifying its closest anchor points. Anchor graph hashing, however, incurs high computation cost since (1) the computation cost of obtaining the eigenvectors is quadratic to the number of anchor points, and (2) the similarities of the query data point to all the anchor points must be computed. Our proposal, Tridiagonal hashing, increases the efficiency of anchor graph hashing because of its two advances: (1) we apply a graph clustering algorithm to compute the eigenvectors from the tridiagonal matrix obtained from the similarities between data points and anchor points, and (2) we detect anchor points closest to the query data point by using a dimensionality reduction approach. Experiments show that our approach is several orders of magnitude faster than the previous approaches. Besides, it yields high search accuracy than the original anchor graph hashing approach.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447696®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadO©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ShaikhhaSO21•titleŸRAn Intermediate Representation for Hybrid Database and Machine Learning Workloads.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476356©publisher±Proc. VLDB Endow.ßauthorsì≠Amir Shaikhha≥Maximilian Schleich´Dan Olteanu®Abstract⁄ÅIFAQ is an intermediate representation and compilation framework for hybrid database and machine learning workloads expressible using iterative programs with functional aggregate queries. We demonstrate IFAQ for several OLAP queries, linear algebra expressions, and learning factorization machines over training datasets defined by feature extraction queries over relational databases.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476356®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadN©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/Zalipynis21•titleŸ-Array DBMS: Past, Present, and (Near) Future.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476404©publisher±Proc. VLDB Endow.ßauthorsëŸ Ramon Antonio Rodriges Zalipynis®Abstract⁄aArray DBMSs strive to be the best systems for managing, processing, and even visualizing big N-d arrays. The last decade blossomed with R&amp;D in array DBMS, making it a young and fast-evolving area. We present the first comprehensive tutorial on array DBMS R&amp;D. We start from past impactful results that are still relevant today, then we cover contemporary array DBMSs, array-oriented systems, and state-of-the-art research in array management, flavored with numerous promising R&amp;D opportunities for future work. A great deal of our tutorial was not covered in any previous tutorial or survey article. Advanced array management research is just emerging and many R&amp;D opportunities still "lie on the surface". Hence, nowadays we have the most favorable conditions to start contributing to this research area. This tutorial will jump-start such efforts.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476404®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadM©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/WangSB20•titleŸ[On the Efficiency of K-Means Clustering: Evaluation, Optimization, and Algorithm Selection.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425887©publisher±Proc. VLDB Endow.ßauthorsìØSheng Wang 0007≠Yuan Sun 0003´Zhifeng Bao®Abstract⁄@This paper presents a thorough evaluation of the existing methods that accelerate Lloyd's algorithm for fast k-means clustering. To do so, we analyze the pruning mechanisms of existing methods, and summarize their common pipeline into a unified evaluation framework UniK. UniK embraces a class of well-known methods and enables a fine-grained performance breakdown. Within UniK, we thoroughly evaluate the pros and cons of existing methods using multiple performance metrics on a number of datasets. Furthermore, we derive an optimized algorithm over UniK, which effectively hybridizes multiple existing methods for more aggressive pruning. To take this further, we investigate whether the most efficient method for a given clustering task can be automatically selected by machine learning, to benefit practitioners and researchers.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425887®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadG©PaperRefsêã§Infoá§typeßarticle£keyøjournals/pvldb/ChengSPSWLBBCS21•titleŸKRAMP-TAO: Layering Atomic Transactions on Facebook's Online TAO Data Store.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476379©publisher±Proc. VLDB Endow.ßauthorsö¨Audrey Cheng®Xiao Shi¶Lu PanØAnthony Simpson¨Neil WheatonÆShilpa LawandeÆNathan Bronson¨Peter BailisÆNatacha Crooks™Ion Stoica®Abstract⁄MFacebook's graph store TAO, like many other distributed data stores, traditionally prioritizes availability, efficiency, and scalability over strong consistency or isolation guarantees to serve its large, read-dominant workloads. As product developers build diverse applications on top of this system, they increasingly seek transactional semantics. However, providing advanced features for select applications while preserving the system's overall reliability and performance is a continual challenge. In this paper, we first characterize developer desires for transactions that have emerged over the years and describe the current failure-atomic (i.e., write) transactions offered by TAO. We then explore how to introduce an intuitive read transaction API. We highlight the need for atomic visibility guarantees in this API with a measurement study on potential anomalies that occur without stronger isolation for reads. Our analysis shows that 1 in 1,500 batched reads reflects partial transactional updates, which complicate the developer experience and lead to unexpected results. In response to our findings, we present the RAMP-TAO protocol, a variation based on the Read Atomic Multi-Partition (RAMP) protocols that can be feasibly deployed in production with minimal overhead while ensuring atomic visibility for a read-optimized workload at scale.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476379®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadF©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/PowerPJLJRTZKTR21•titleŸbThe Cosmos Big Data Platform at Microsoft: Over a Decade of Progress and a Decade to Look Forward.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476390©publisher±Proc. VLDB Endow.ßauthorsü´Conor Power´Hiren Patel¨Alekh Jindal´Jyoti Leeka´Bob Jenkins´Michael Rys®Ed Triou©Dexin ZhuØLucky Katahanas∏Chakrapani Bhat Talapady©Josh Rowe©Fan Zhang´Rich Draves™Ivan Santa¨Amrish Kumar®Abstract⁄|The twenty-first century has been dominated by the need for large scale data processing, marking the birth of big data platforms such as Cosmos. This paper describes the evolution of the exabyte-scale Cosmos big data platform at Microsoft; our journey right from scale and reliability all the way to efficiency and usability, and our next steps towards improving security, compliance, and support for heterogeneous analytics scenarios. We discuss how the evolution of Cosmos parallels the evolution of the big data field, and how the changes in the Cosmos workloads over time parallel the changing requirements of users across industry.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476390®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadC©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/GongTYYZGYYZ21•titleŸBAutomating Incremental Graph Processing with Flexible Memoization.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461550©publisher±Proc. VLDB Endow.ßauthorsô¨Shufeng GongÆChao Tian 0001ÆQiang Yin 0002™Wenyuan Yu≠Yanfeng Zhang™Liang GengßSong Yu™Ge Yu 0001¨Jingren Zhou®Abstract⁄The ever-growing amount of dynamic graph data demands efficient techniques of incremental graph processing. However, incremental graph algorithms are challenging to develop. Existing approaches usually require users to manually design nontrivial incremental operators, or choose different memoization strategies for certain specific types of computation, limiting the usability and generality.In light of these challenges, we propose Ingress, an automated system for &lt;u&gt;in&lt;/u&gt;cremental &lt;u&gt;g&lt;/u&gt;raph proc&lt;u&gt;ess&lt;/u&gt;ing. Ingress is able to incrementalize batch vertex-centric algorithms into their incremental counterparts as a whole, without the need of redesigned logic or data structures from users. Underlying Ingress is an automated incrementalization framework equipped with four different memoization policies, to support all kinds of vertex-centric computations with optimized memory utilization. We identify sufficient conditions for the applicability of these policies. Ingress chooses the best-fit policy for a given algorithm automatically by verifying these conditions. In addition to the ease-of-use and generalization, Ingress outperforms state-of-the-art incremental graph systems by 15.93X on average (up to 147.14X) in efficiency.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461550®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadA©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/ZogajCRC21•titleŸEDoing More with Less: Characterizing Dataset Downsampling for AutoML.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476262©publisher±Proc. VLDB Endow.ßauthorsî¨Fatjon Zogaj∂Jos√© Pablo Cambronero∞Martin C. Rinard¨J√ºrgen Cito®Abstract⁄yAutomated machine learning (AutoML) promises to democratize machine learning by automatically generating machine learning pipelines with little to no user intervention. Typically, a search procedure is used to repeatedly generate and validate candidate pipelines, maximizing a predictive performance metric, subject to a limited execution time budget. While this approach to generating candidates works well for small tabular datasets, the same procedure does not directly scale to larger tabular datasets with 100,000s of observations, often producing fewer candidate pipelines and yielding lower performance, given the same execution time budget. We carry out an extensive empirical evaluation of the impact that downsampling - reducing the number of rows in the input tabular dataset - has on the pipelines produced by a genetic-programming-based AutoML search for classification tasks.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476262®Keywordsê¶Badges¿•Track∞research-article®Citation®Download@©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/BoniolPPF21a•titleŸ:SAND in Action: Subsequence Anomaly Detection for Streams.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476365©publisher±Proc. VLDB Endow.ßauthorsî´Paul BoniolØJohn PaparrizosØThemis Palpanas≥Michael J. Franklin®Abstract⁄ΩSubsequence anomaly detection in long data series is a significant problem. While the demand for real-time analytics and decision making increases, anomaly detection methods have to operate over streams and handle drifts in data distribution. Nevertheless, existing approaches either require prior domain knowledge or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. Moreover, subsequence anomaly detection methods usually require access to the entire dataset and are not able to learn and detect anomalies in streaming settings. To address these limitations, we propose SAND, a novel online system suitable for domain-agnostic anomaly detection. SAND relies on a novel steaming methodology to incrementally update a model that adapts to distribution drifts and omits obsolete data. We demonstrate our system over different streaming scenarios and compare SAND with other subsequence anomaly detection methods.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476365®Keywordsê¶Badges¿•Track∞research-article®Citation®Download;©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/LiYK21•titleŸ7Data Acquisition for Improving Machine Learning Models.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467872©publisher±Proc. VLDB Endow.ßauthorsì®Yifan LiØXiaohui Yu 0001´Nick Koudas®Abstract⁄WThe vast advances in Machine Learning (ML) over the last ten years have been powered by the availability of suitably prepared data for training purposes. The future of ML-enabled enterprise hinges on data. As such, there is already a vibrant market offering data annotation services to tailor sophisticated ML models.In this paper, inspired by the recent vision of online data markets and associated market designs, we present research on the practical problem of obtaining data in order to improve the accuracy of ML models. We consider an environment in which consumers query for data to enhance the accuracy of their models and data providers who possess data make them available for training purposes. We first formalize this interaction process laying out the suitable framework and associated parameters for data exchange. We then propose two data acquisition strategies that consider a trade-off between exploration during which we obtain data to learn about the distribution of a provider's data and exploitation during which we optimize our data inquiries utilizing the gained knowledge. In the first strategy, Estimation and Allocation (EA), we utilize queries to estimate the utilities of various predicates while learning about the distribution of the provider's data; then we proceed to the allocation stage in which we utilize those learned utility estimates to inform our data acquisition decisions. The second algorithmic proposal, named Sequential Predicate Selection (SPS), utilizes a sampling strategy to explore the distribution of the provider's data, adaptively investing more resources to parts of the data space that are statistically more promising to improve overall model accuracy.We present a detailed experimental evaluation of our proposals utilizing a variety of ML models and associated real data sets exploring all applicable parameters of interest. Our results demonstrate the relative benefits of the proposed algorithms. Depending on the models trained and the associated learning tasks we identify trade-offs and highlight the relative benefits of each algorithm to further optimize model accuracy.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467872®Keywordsê¶Badges¿•Track∞research-article®Citation®Download:©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/TakenouchiIOS21•titleŸiPATSQL: Efficient Synthesis of SQL Queries from Example Tables with Quick Inference of Projected Columns.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476253©publisher±Proc. VLDB Endow.ßauthorsî∞Keita Takenouchi≠Takashi Ishio™Joji Okada´Yuji Sakata®Abstract⁄wSQL is one of the most popular tools for data analysis, and it is now used by an increasing number of users without having expertise in databases. Several studies have proposed programming-by-example approaches to help such non-experts to write correct SQL queries. While existing methods support a variety of SQL features such as aggregation and nested query, they suffer a significant increase in computational cost as the scale of example tables increases. In this paper, we propose an efficient algorithm utilizing properties known in relational algebra to synthesize SQL queries from input and output tables. Our key insight is that a projection operator in a program sketch can be lifted above other operators by applying transformation rules in relational algebra, while preserving the semantics of the program. This enables a quick inference of appropriate columns in the projection operator, which is an essential component in synthesis but causes combinatorial explosions in prior work. We also introduce a novel form of constraints and its top-down propagation mechanism for efficient sketch completion. We implemented this algorithm in our tool PATSQL and evaluated it on 226 queries from prior benchmarks and Kaggle's tutorials. As a result, PATSQL solved 68% of the benchmarks and found 89% of the solutions within a second. Our tool is available at https://naist-se.github.io/patsql/.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476253®Keywordsê¶Badges¿•Track∞research-article®Citation®Download:©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/HeoRHLW20•titleŸQInspector Gadget: A Data Programming-based Labeling System for Industrial Images.§year§2020£doiŸ(https://doi.org/10.14778/3421424.3421429©publisher±Proc. VLDB Endow.ßauthorsï®Geon Heo®Yuji Roh∞Seonghyeon Hwang©Dayun Lee¨Steven Whang®Abstract⁄bAs machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses human knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better performance than other weak-labeling techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3421424.3421429®Keywordsê¶Badges¿•Track∞research-article®Citation®Download3©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/GarciaLSYD0HS20•titleŸ%Hindsight Logging for Model Training.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436925©publisher±Proc. VLDB Endow.ßauthorsòÆRolando Garcia®Eric Liu∞Vikram Sreekanti©Bobby Yan∞Anusha Dandamudi¥Joseph Gonzalez 0001µJoseph M. Hellerstein´Koushik Sen®Abstract⁄2In modern Machine Learning, model training is an iterative, experimental process that can consume enormous computation resources and developer time. To aid in that process, experienced model developers log and visualize program variables during training runs. Exhaustive logging of all variables is infeasible, so developers are left to choose between slowing down training via extensive conservative logging, or letting training run fast via minimalist optimistic logging that may omit key information. As a compromise, optimistic logging can be accompanied by program checkpoints; this allows developers to add log statements post-hoc, and "replay" desired log statements from checkpoint---a process we refer to as hindsight logging. Unfortunately, hindsight logging raises tricky problems in data management and software engineering. Done poorly, hindsight logging can waste resources and generate technical debt embodied in multiple variants of training code. In this paper, we present methodologies for efficient and effective logging practices for model training, with a focus on techniques for hindsight logging. Our goal is for experienced model developers to learn and adopt these practices. To make this easier, we provide an open-source suite of tools for Fast Low-Overhead Recovery (flor) that embodies our design across three tasks: (i) efficient background logging in Python, (ii) adaptive periodic checkpointing, and (iii) an instrumentation library that codifies hindsight logging for efficient and automatic record-replay of model-training. Model developers can use each flor tool separately as they see fit, or they can use flor in hands-free mode, entrusting it to instrument their code end-to-end for efficient record-replay. Our solutions leverage techniques from physiological transaction logs and recovery in database systems. Evaluations on modern ML benchmarks demonstrate that flor can produce fast checkpointing with small user-specifiable overheads (e.g. 7%), and still provide hindsight log replay times orders of magnitude faster than restarting training from scratch.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436925®Keywordsê¶Badges¿•Track∞research-article®Citation®Download2©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/MailisKCKI21•titleŸ6View Selection over Knowledge Graphs in Triple Stores.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484227©publisher±Proc. VLDB Endow.ßauthorsï∞Theofilos MailisÆYannis Kotidis∑Stamatis Christoforidis∞Evgeny Kharlamov≥Yannis E. Ioannidis®Abstract⁄◊Knowledge Graphs (KGs) are collections of interconnected and annotated entities that have become powerful assets for data integration, search enhancement, and other industrial applications. Knowledge Graphs such as DBPEDIA may contain billion of triple relations and are intensively queried with millions of queries per day. A prominent approach to enhance query answering on Knowledge Graph databases is View Materialization, ie., the materialization of an appropriate set of computations that will improve query performance.We study the problem of view materialization and propose a view selection methodology for processing query workloads with more than a million queries. Our approach heavily relies on subgraph pattern mining techniques that allow to create efficient summarizations of massive query workloads while also identifying the candidate views for materialization. In the core of our work is the correspondence between the view selection problem to that of Maximizing a Nondecreasing Submodular Set Function Subject to a Knapsack Constraint. The latter leads to a tractable view-selection process for native triple stores that allows a (1 - e---1)-approximation of the optimal selection of views. Our experimental evaluation shows that all the steps of the view-selection process are completed in a few minutes, while the corresponding rewritings accelerate 67.68% of the queries in the DBPEDIA query workload. Those queries are executed in 2.19% of their initial time on average.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484227®Keywordsê¶Badges¿•Track∞research-article®Citation®Download2©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/GeMHI21•titleŸ?Kamino: Constraint-Aware Differentially Private Data Synthesis.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467876©publisher±Proc. VLDB Endow.ßauthorsî≠Chang Ge 0002¥Shubhankar Mohapatra™Xi He 0001≠Ihab F. Ilyas®Abstract⁄ÕOrganizations are increasingly relying on data to support decisions. When data contains private and sensitive information, the data owner often desires to publish a synthetic database instance that is similarly useful as the true data, while ensuring the privacy of individual data records. Existing differentially private data synthesis methods aim to generate useful data based on applications, but they fail in keeping one of the most fundamental data properties of the structured data --- the underlying correlations and dependencies among tuples and attributes (i.e., the structure of the data). This structure is often expressed as integrity and schema constraints, or with a probabilistic generative process. As a result, the synthesized data is not useful for any downstream tasks that require this structure to be preserved.This work presents KAMINO, a data synthesis system to ensure differential privacy and to preserve the structure and correlations present in the original dataset. KAMINO takes as input of a database instance, along with its schema (including integrity constraints), and produces a synthetic database instance with differential privacy and structure preservation guarantees. We empirically show that while preserving the structure of the data, KAMINO achieves comparable and even better usefulness in applications of training classification models and answering marginal queries than the state-of-the-art methods of differentially private data synthesis.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467876®Keywordsê¶Badges¿•Track∞research-article®Citation®Download1©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/WuWD21•titleŸMCHEF: A Cheap and Fast Pipeline for Iteratively Cleaning Label Uncertainties.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476290©publisher±Proc. VLDB Endow.ßauthorsì©Yinjun Wu¨James Weimer±Susan B. Davidson®Abstract⁄RHigh-quality labels are expensive to obtain for many machine learning tasks, such as medical image classification tasks. Therefore, probabilistic (weak) labels produced by weak supervision tools are used to seed a process in which influential samples with weak labels are identified and cleaned by several human annotators to improve the model performance. To lower the overall cost and computational overhead of this process, we propose a solution called CHEF (CHEap and Fast label cleaning), which consists of the following three components. First, to reduce the cost of human annotators, we use INFL, which prioritizes the most influential training samples for cleaning and provides cleaned labels to save the cost of one human annotator. Second, to accelerate the sample selector phase and the model constructor phase, we use Increm-INFL to incrementally produce influential samples, and DeltaGrad-L to incrementally update the model. Third, we redesign the typical label cleaning pipeline so that human annotators iteratively clean smaller batch of samples rather than one big batch of samples. This yields better overall model performance and enables possible early termination when the expected model performance has been achieved. Extensive experiments show that our approach gives good model prediction performance while achieving significant speed-ups.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476290®Keywordsê¶Badges¿•Track∞research-article®Citation®Download0©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/LiuZHX21•titleŸWLocal Algorithms for Distance-generalized Core Decomposition over Large Dynamic Graphs.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461542©publisher±Proc. VLDB Endow.ßauthorsî®Qing Liu´Xuliang ZhuÆXin Huang 0001¨Jianliang Xu®Abstract⁄πThe distance-generalized core, also called (k, h)-core, is defined as the maximal subgraph in which every vertex has at least k vertices at distance no longer than h. Compared with k-core, (k, h)-core can identify more fine-grained subgraphs and, hence, is more useful for the applications such as network analysis and graph coloring. The state-of-the-art algorithms for (k, h)-core decomposition are peeling algorithms, which iteratively delete the vertex with the minimum h-degree (i.e., the least number of neighbors within h hops). However, they suffer from some limitations, such as low parallelism and incapability of supporting dynamic graphs. To address these limitations, in this paper, we revisit the problem of (k, h)-core decomposition. First, we introduce two novel concepts of pairwise h-attainability index and n-order H-index based on an insightful observation. Then, we thoroughly analyze the properties of n-order H-index and propose a parallelizable local algorithm for (k, h)-core decomposition. Moreover, several optimizations are presented to accelerate the local algorithm. Furthermore, we extend the proposed local algorithm to address the (k, h)-core maintenance problem for dynamic graphs. Experimental studies on real-world graphs show that, compared to the best existing solution, our proposed algorithms can reduce the (k, h)-core decomposition time by 1--3 orders of magnitude and save the maintenance cost by 1--2 orders of magnitude.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461542®Keywordsê¶Badges¿•Track∞research-article®Citation®Download.©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/LiuJPE21•titleŸ;Decomposed Bounded Floats for Fast Compression and Queries.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476305©publisher±Proc. VLDB Endow.ßauthorsî´Chunwei Liu©Hao JiangØJohn PaparrizosØAaron J. Elmore®Abstract⁄Modern data-intensive applications often generate large amounts of low precision float data with a limited range of values. Despite the prevalence of such data, there is a lack of an effective solution to ingest, store, and analyze bounded, low-precision, numeric data. To address this gap, we propose Buff, a new compression technique that uses a decomposed columnar storage and encoding methods to provide effective compression, fast ingestion, and high-speed in-situ adaptive query operators with SIMD support.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476305®Keywordsê¶Badges¿•Track∞research-article®Citation®Download,©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/WangGPS21•titleŸNDemonstration of Generating Explanations for Black-Box Algorithms Using Lewis.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476345©publisher±Proc. VLDB Endow.ßauthorsî¨Paul Y. Wang∞Sainyam GalhotraÆRomila Pradhan¨Babak Salimi®Abstract⁄sExplainable artificial intelligence (XAI) aims to reduce the opacity of AI-based decision-making systems, allowing humans to scrutinize and trust them. Unlike prior work that attributes the responsibility for an algorithm's decisions to its inputs as a purely associational concept, we propose a principled causality-based approach for explaining black-box decision-making systems. We present the demonstration of Lewis, a system that generates explanations for black-box algorithms at the global, contextual, and local levels, and provides actionable recourse for individuals negatively affected by an algorithm's decision. Lewis makes no assumptions about the internals of the algorithm except for the availability of its input-output data. The explanations generated by Lewis are based on probabilistic contrastive counterfactuals, a concept that can be traced back to philosophical, cognitive, and social foundations of theories on how humans generate and select explanations. We describe the system layout of Lewis wherein an end-user specifies the underlying causal model and Lewis generates explanations for particular use-cases, compares them with explanations generated by state-of-the-art approaches in XAI, and provides actionable recourse when applicable. Lewis has been developed as open-source software; the code and the demonstration video are available at lewis-system.github.io.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476345®Keywordsê¶Badges¿•Track∞research-article®Citation®Download,©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/JepsenLPSC21•titleŸ,In-Network Support for Transaction Triaging.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461551©publisher±Proc. VLDB Endow.ßauthorsï´Theo JepsenÆAlberto LernerØFernando Pedone≠Robert Soul√©∑Philippe Cudr√©-Mauroux®Abstract⁄ÛWe introduce Transaction Triaging, a set of techniques that manipulate streams of transaction requests and responses while they travel to and from a database server. Compared to normal transaction streams, the triaged ones execute faster once they reach the database. The triaging algorithms do not interfere with the transaction execution nor require adherence to any particular concurrency control method, making them easy to port across database systems.Transaction Triaging leverages recent programmable networking hardware that can perform computations on in-flight data. We evaluate our techniques on an in-memory database system using an actual programmable hardware network switch. Our experimental results show that triaging brings enough performance gains to compensate for almost all networking overheads. In high-overhead network stacks such as UDP/IP, we see throughput improvements from 2.05X to 7.95X. In an RDMA stack, the gains range from 1.08X to 1.90X without introducing significant latency.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461551®Keywordsê¶Badges¿•Track∞research-article®Citation®Download+©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/CaiB0C21•titleŸ1Optimization of Threshold Functions over Streams.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447693©publisher±Proc. VLDB Endow.ßauthorsî™Walter Cai≥Philip A. BernsteinÆWentao Wu 0001¥Badrish Chandramouli®Abstract⁄A common stream processing application is alerting, where the data stream management system (DSMS) continuously evaluates a threshold function over incoming streams. If the threshold is crossed, the DSMS raises an alarm. The threshold function is often calculated over two or more streams, such as combining temperature and humidity readings to determine if moisture will form on a machine and therefore cause it to malfunction. This requires taking a temporal join across the input streams. We show that for the broad class of functions called quasiconvex functions, the DSMS needs to retain very few tuples per-data-stream for any given time interval and still never miss an alarm. This surprising result yields a large memory savings during normal operation. That savings is also important if one stream fails, since the DSMS would otherwise have to cache all tuples in other streams until the failed stream recovers. We prove our algorithm is optimal and provide experimental evidence that validates its substantial memory savings.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447693®Keywordsê¶Badges¿•Track∞research-article®Citation®Download+©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/DhulipalaHS20•titleŸYConnectIt: A Framework for Static and Incremental Parallel Graph Connectivity Algorithms.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436923©publisher±Proc. VLDB Endow.ßauthorsì∞Laxman Dhulipala≠Changwan Hong´Julian Shun®Abstract⁄/Connected components is a fundamental kernel in graph applications. The fastest existing multicore algorithms for solving graph connectivity are based on some form of edge sampling and/or linking and compressing trees. However, many combinations of these design choices have been left unexplored. In this paper, we design the ConnectIt framework, which provides different sampling strategies as well as various tree linking and compression schemes. ConnectIt enables us to obtain several hundred new variants of connectivity algorithms, most of which extend to computing spanning forest. In addition to static graphs, we also extend ConnectIt to support mixes of insertions and connectivity queries in the concurrent setting.We present an experimental evaluation of ConnectIt on a 72-core machine, which we believe is the most comprehensive evaluation of parallel connectivity algorithms to date. Compared to a collection of state-of-the-art static multicore algorithms, we obtain an average speedup of 12.4x (2.36x average speedup over the fastest existing implementation for each graph). Using ConnectIt, we are able to compute connectivity on the largest publicly-available graph (with over 3.5 billion vertices and 128 billion edges) in under 10 seconds using a 72-core machine, providing a 3.1x speedup over the fastest existing connectivity result for this graph, in any computational setting. For our incremental algorithms, we show that our algorithms can ingest graph updates at up to several billion edges per second. To guide the user in selecting the best variants in ConnectIt for different situations, we provide a detailed analysis of the different strategies. Finally, we show how the techniques in ConnectIt can be used to speed up two important graph applications: approximate minimum spanning forest and SCAN clustering.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436923®Keywordsê¶Badges¿•Track∞research-article®Citation®Download*©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/Zhu0CJZX21•titleŸ;Budget Constrained Interactive Search for Multiple Targets.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447694©publisher±Proc. VLDB Endow.ßauthorsñ´Xuliang ZhuÆXin Huang 0001™Byron Choi¨Jiaxin Jiang¨Zhaonian Zou¨Jianliang Xu®Abstract⁄–Interactive graph search leverages human intelligence to categorize target labels in a hierarchy, which is useful for image classification, product categorization, and database search. However, many existing interactive graph search studies aim at identifying a single target optimally, and suffer from the limitations of asking too many questions and not being able to handle multiple targets.To address these two limitations, in this paper, we study a new problem of &lt;u&gt;b&lt;/u&gt;udget constrained &lt;u&gt;i&lt;/u&gt;nteractive &lt;u&gt;g&lt;/u&gt;raph &lt;u&gt;s&lt;/u&gt;earch for &lt;u&gt;m&lt;/u&gt;ultiple targets called kBM-IGS problem. Specifically, given a set of multiple targets T in a hierarchy and two parameters k and b, the goal is to identify a k-sized set of selections S, such that the closeness between selections S and targets T is as small as possible, by asking at most a budget of b questions. We theoretically analyze the updating rules and design a penalty function to capture the closeness between selections and targets. To tackle the kBM-IGS problem, we develop a novel framework to ask questions using the best vertex with the largest expected gain, which provides a balanced trade-off between target probability and benefit gain. Based on the kBM-IGS framework, we first propose an efficient algorithm STBIS to handle the SingleTarget problem, which is a special case of kBM-IGS. Then, we propose a dynamic programming based method kBM-DP to tackle the MultipleTargets problem. To further improve efficiency, we propose two heuristic but efficient algorithms, kBM-Topk and kBM-DP+. Experiments on large real-world datasets with ground-truths verify both the effectiveness and efficiency of our algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447694®Keywordsê¶Badges¿•Track∞research-article®Citation®Download)©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/Ertl21•titleŸ;SetSketch: Filling the Gap between MinHash and HyperLogLog.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476276©publisher±Proc. VLDB Endow.ßauthorsë™Otmar Ertl®Abstract⁄«MinHash and HyperLogLog are sketching algorithms that have become indispensable for set summaries in big data applications. While HyperLogLog allows counting different elements with very little space, MinHash is suitable for the fast comparison of sets as it allows estimating the Jaccard similarity and other joint quantities. This work presents a new data structure called SetSketch that is able to continuously fill the gap between both use cases. Its commutative and idempotent insert operation and its mergeable state make it suitable for distributed environments. Fast, robust, and easy-to-implement estimators for cardinality and joint quantities, as well as the ability to use SetSketch for similarity search, enable versatile applications. The presented joint estimator can also be applied to other data structures such as MinHash, HyperLogLog, or Hyper-MinHash, where it even performs better than the corresponding state-of-the-art estimators in many cases.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476276®Keywordsê¶Badges¿•Track∞research-article®Citation®Download'©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/MuslehASM21•titleŸ4QARTA: An ML-based System for Accurate Map Services.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476279©publisher±Proc. VLDB Endow.ßauthorsîÆMashaal Musleh≠Sofiane AbbarØRade Stanojevic±Mohamed F. Mokbel®Abstract⁄éMaps services are ubiquitous in widely used applications including navigation systems, ride sharing, and items/food delivery. Though there are plenty of efforts to support such services through designing more efficient algorithms, we believe that efficiency is no longer a bottleneck to these services. Instead, it is the accuracy of the underlying road network and query result. This paper presents QARTA; an open-source full-fledged system for highly accurate and scalable map services. QARTA employs machine learning techniques to construct its own highly accurate map, not only in terms of map topology but more importantly, in terms of edge weights. QARTA also employs machine learning techniques to calibrate its query answers based on contextual information, including transportation modality, location, and time of day/week. QARTA is currently deployed in all Taxis and the third largest food delivery company in the State of Qatar, replacing the commercial map service that was in use, and responding in real-time to hundreds of thousands of daily API calls. Experimental evaluation of QARTA shows its comparable or higher accuracy than commercial services.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476279®Keywordsê¶Badges¿•Track∞research-article®Citation®Download&©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Lemiesz21•titleŸ On the algebra of data sketches.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461553©publisher±Proc. VLDB Endow.ßauthorsë≠Jakub Lemiesz®Abstract⁄mWe consider the problem of designing a distributed data sketch for scenario in which data stream is observed by many independent network nodes. We require that a sketch apart from being computationally and memory efficient should also be mergeable in a way that mimics set theory operations on related data sets.For example, when monitoring network traffic, one may consider how many distinct packets passed through a given node (sum of sets for different time windows) or passed through two given nodes (intersection of sets from two locations) and what is their total size (intersection of weighted sets).In this paper we propose a sketch that allows to efficiently summarize sets constructed from a sequence of set theory operations. We also provide an analytical control over the trade-off between the accuracy and storage/computational requirements. In comparison to the previous works the proposed solution 1) allows the weights of elements, 2) allows performing set theory operations simultaneous on a large number of sketches, 3) does not require computationally expensive numerical calculations and guarantees low overheads.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461553®Keywordsê¶Badges¿•Track∞research-article®Citation®Download&©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/GencerTDDKGBGHY21•titleŸGHazelcast Jet: Low-latency Stream Processing at the 99.99th Percentile.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476387©publisher±Proc. VLDB Endow.ßauthorsü™Can GencerÆMarko Topolnik≠Viliam Durina¨Emin Demirci∞Ensar B. Kahveci¨Ali G√ºrb√ºzØJ√≥zsef Bart√≥k±Grzegorz Gierlach±Frantisek Hartman´Ufuk Yilmaz≠Ondrej Luk√°s¨Mehmet DoganØMohamed Mandouh±Marios FragkoulisµAsterios Katsifodimos®Abstract⁄ÇJet is an open source, high performance, distributed stream processor built at Hazelcast during the last five years. Jet was engineered with millisecond latency on the 99.99th percentile as its primary design goal. Originally Jet's purpose was to be an execution engine that performs complex business logic on top of streams generated by Hazelcast's In-memory Data Grid (IMDG): a set of in-memory, partitioned and replicated data structures. With time, Jet evolved into a full-fledged, scale-out stream processor that can handle out-of-order streams and provide exactly-once processing guarantees. Jet's end-to-end latency lies in the order of milliseconds, and its throughput in the order of millions of events per CPU-core. This paper presents the main design decisions we made in order to maximize the performance per CPU-core, alongside lessons learned, and an empirical performance evaluation.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476387®Keywordsê¶Badges¿•Track∞research-article®Citation®Download%©PaperRefsêã§Infoá§typeßarticle£key¥journals/pvldb/Li021•titleŸKTowards an Optimized GROUP BY Abstraction for Large-Scale Machine Learning.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476284©publisher±Proc. VLDB Endow.ßauthorsíßSide LiØArun Kumar 0001®Abstract⁄•Many applications that use large-scale machine learning (ML) increasingly prefer different models for subgroups (e.g., countries) to improve accuracy, fairness, or other desiderata. We call this emerging popular practice learning over groups, analogizing to GROUP BY in SQL, albeit for ML training instead of SQL aggregates. From the systems standpoint, this practice compounds the already data-intensive workload of ML model selection (e.g., hyperparameter tuning). Often, thousands of models may need to be trained, necessitating high-throughput parallel execution. Alas, most ML systems today focus on training one model at a time or at best, parallelizing hyperparameter tuning. This status quo leads to resource wastage, low throughput, and high runtimes. In this work, we take the first step towards enabling and optimizing learning over groups from the data systems standpoint for three popular classes of ML: linear models, neural networks, and gradient-boosted decision trees. Analytically and empirically, we compare standard approaches to execute this workload today: task-parallelism and data-parallelism. We find neither is universally dominant. We put forth a novel hybrid approach we call grouped learning that avoids redundancy in communications and I/O using a novel form of parallel gradient descent we call Gradient Accumulation Parallelism (GAP). We prototype our ideas into a system we call Kingpin built on top of existing ML tools and the flexible massively-parallel runtime Ray. An extensive empirical evaluation on large ML benchmark datasets shows that Kingpin matches or is 4x to 14x faster than state-of-the-art ML systems, including Ray's native execution and PyTorch DDP.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476284®Keywordsê¶Badges¿•Track∞research-article®Citation®Download%©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/GuptaR21•titleŸDProcedural Extensions of SQL: Understanding their usage in the wild.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457402©publisher±Proc. VLDB Endow.ßauthorsí≠Surabhi Gupta∏Karthik Ramachandra 0002®Abstract⁄÷Procedural extensions of SQL have been in existence for many decades now. However, little is known about their magnitude of usage and their complexity in real-world workloads. Procedural code executing in a RDBMS is known to have inefficiencies and limitations; as a result there have been several efforts to address this problem. However, the lack of understanding of their use in real workloads makes it challenging to (a) motivate new work in this area, (b) identify research challenges and opportunities, and (c) demonstrate impact of novel work. We aim to address these challenges with our work.In this paper, we present the results of our in-depth analysis of thousands of stored procedures, user-defined functions and triggers taken from several real workloads. We introduce SQL-ProcBench, a benchmark for procedural workloads in RDBMSs. SQL-ProcBench has been created using the insights derived from our analysis, and thus represents real workloads. Using SQL-ProcBench, we present an experimental evaluation on several database engines to understand and identify research challenges and opportunities. We emphasize the need to work on these interesting and relevant problems, and encourage researchers to contribute to this area.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457402®Keywordsê¶Badges¿•Track∞research-article®Citation®Download#©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/OrrSLGL21•titleŸRManaging ML Pipelines: Feature Stores and the Coming Wave of Embedding Ecosystems.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476402©publisher±Proc. VLDB Endow.ßauthorsï≠Laurel J. Orr∞Atindriyo Sanyal©Xiao Ling™Karan Goel±Megan Leszczynski®Abstract⁄~The industrial machine learning pipeline requires iterating on model features, training and deploying models, and monitoring deployed models at scale. Feature stores were developed to manage and standardize the engineer's workflow in this end-to-end pipeline, focusing on traditional tabular feature data. In recent years, however, model development has shifted towards using self-supervised pretrained embeddings as model features. Managing these embeddings and the downstream systems that use them introduces new challenges with respect to managing embedding training data, measuring embedding quality, and monitoring downstream models that use embeddings. These challenges are largely unaddressed in standard feature stores. Our goal in this tutorial is to introduce the feature store system and discuss the challenges and current solutions to managing these new embedding-centric pipelines.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476402®Keywordsê¶Badges¿•Track∞research-article®Citation®Download"©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/WangTB21•titleŸBUDO: Universal Database Optimization using Reinforcement Learning.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484236©publisher±Proc. VLDB Endow.ßauthorsì≠Junxiong Wang∞Immanuel TrummerÆDebabrota Basu®Abstract⁄UDO is a versatile tool for offline tuning of database systems for specific workloads. UDO can consider a variety of tuning choices, reaching from picking transaction code variants over index selections up to database system parameter tuning. UDO uses reinforcement learning to converge to near-optimal configurations, creating and evaluating different configurations via actual query executions (instead of relying on simplifying cost models). To cater to different parameter types, UDO distinguishes heavy parameters (which are expensive to change, e.g. physical design parameters) from light parameters. Specifically for optimizing heavy parameters, UDO uses reinforcement learning algorithms that allow delaying the point at which the reward feedback becomes available. This gives us the freedom to optimize the point in time and the order in which different configurations are created and evaluated (by benchmarking a workload sample). UDO uses a cost-based planner to minimize reconfiguration overheads. For instance, it aims to amortize the creation of expensive data structures by consecutively evaluating configurations using them. We evaluate UDO on Postgres as well as MySQL and on TPC-H as well as TPC-C, optimizing a variety of light and heavy parameters concurrently.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484236®Keywordsê¶Badges¿•Track∞research-article®Citation®Download"©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/SunCLH021•titleŸ1ThunderRW: An In-Memory Graph Random Walk Engine.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476257©publisher±Proc. VLDB Endow.ßauthorsï´Shixuan Sun´Yuhang Chen≠Shengliang Lu¨Bingsheng HeÆYuchen Li 0001®Abstract⁄As random walk is a powerful tool in many graph processing, mining and learning applications, this paper proposes an efficient in-memory random walk engine named ThunderRW. Compared with existing parallel systems on improving the performance of a single graph operation, ThunderRW supports massive parallel random walks. The core design of ThunderRW is motivated by our profiling results: common RW algorithms have as high as 73.1% CPU pipeline slots stalled due to irregular memory access, which suffers significantly more memory stalls than the conventional graph workloads such as BFS and SSSP. To improve the memory efficiency, we first design a generic step-centric programming model named Gather-Move-Update to abstract different RW algorithms. Based on the programming model, we develop the step interleaving technique to hide memory access latency by switching the executions of different random walk queries. In our experiments, we use four representative RW algorithms including PPR, DeepWalk, Node2Vec and MetaPath to demonstrate the efficiency and programming flexibility of ThunderRW. Experimental results show that ThunderRW outperforms state-of-the-art approaches by an order of magnitude, and the step interleaving technique significantly reduces the CPU pipeline stall from 73.1% to 15.0%.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476257®Keywordsê¶Badges¿•Track∞research-article®Citation®Download!©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/SchmidtBG21•titleŸ?A four-dimensional Analysis of Partitioned Approximate Filters.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476286©publisher±Proc. VLDB Endow.ßauthorsìÆTobias Schmidt±Maximilian Bandle´Jana Giceva®Abstract⁄With today's data deluge, approximate filters are particularly attractive to avoid expensive operations like remote data/disk accesses. Among the many filter variants available, it is non-trivial to find the most suitable one and its optimal configuration for a specific use-case. We provide open-source implementations for the most relevant filters (Bloom, Cuckoo, Morton, and Xor filters) and compare them in four key dimensions: the false-positive rate, space consumption, build, and lookup throughput.We improve upon existing state-of-the-art implementations with a new optimization, radix partitioning, which boosts the build and lookup throughput for large filters by up to 9x and 5x. Our in-depth evaluation first studies the impact of all available optimizations separately before combining them to determine the optimal filter for specific use-cases. While register-blocked Bloom filters offer the highest throughput, the new Xor filters are best suited when optimizing for small filter sizes or low false-positive rates.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476286®Keywordsê¶Badges¿•Track∞research-article®Citation®Download ©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/BansalDS21•titleŸ9Missing Value Imputation on Multidimensional Time Series.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476300©publisher±Proc. VLDB Endow.ßauthorsì∞Parikshit Bansal¥Prathamesh DeshpandeØSunita Sarawagi®Abstract⁄QWe present DeepMVI, a deep learning method for missing value imputation in multidimensional time-series datasets. Missing values are commonplace in decision support platforms that aggregate data over long time stretches from disparate sources, whereas reliable data analytics calls for careful handling of missing data. One strategy is imputing the missing values, and a wide variety of algorithms exist spanning simple interpolation, matrix factorization methods like SVD, statistical models like Kalman filters, and recent deep learning methods. We show that often these provide worse results on aggregate analytics compared to just excluding the missing data.DeepMVI expresses the distribution of each missing value conditioned on coarse and fine-grained signals along a time series, and signals from correlated series at the same time. Instead of resorting to linearity assumptions of conventional matrix factorization methods, DeepMVI harnesses a flexible deep network to extract and combine these signals in an end-to-end manner. To prevent over-fitting with high-capacity neural networks, we design a robust parameter training with labeled data created using synthetic missing blocks around available indices. Our neural network uses a modular design with a novel temporal transformer with convolutional features, and kernel regression with learned embeddings.Experiments across ten real datasets, five different missing scenarios, comparing seven conventional and three deep learning methods show that DeepMVI is significantly more accurate, reducing error by more than 50% in more than half the cases, compared to the best existing method. Although slower than simpler matrix factorization methods, we justify the increased time overheads by showing that DeepMVI provides significantly more accurate imputation that finally impacts quality of downstream analytics.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476300®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/BestaVSSKGBJHLT21•titleŸdGraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476252©publisher±Proc. VLDB Endow.ßauthors‹ ¨Maciej BestaµZur Vonarburg-Shmaria±Yannick Schaffner∞Leonardo Schwarz¥Grzegorz Kwasniewski∞Lukas GianinazziÆJakub Ber√°nek¨Kacper Janda±Tobias Holenstein≥Sebastian LeisingerØPeter TatkowskiÆEsref √ñzdemir¨Adrian Balla¨Marcin Copik¥Philipp LindenbergerØMarek Konieczny™Onur MutluØTorsten Hoefler®Abstract⁄ìWe propose GraphMineSuite (GMS): the first benchmarking suite for graph mining that facilitates evaluating and constructing high-performance graph mining algorithms. First, GMS comes with a benchmark specification based on extensive literature review, prescribing representative problems, algorithms, and datasets. Second, GMS offers a carefully designed software platform for seamless testing of different fine-grained elements of graph mining algorithms, such as graph representations or algorithm subroutines. The platform includes parallel implementations of more than 40 considered baselines, and it facilitates developing complex and fast mining algorithms. High modularity is possible by harnessing set algebra operations such as set intersection and difference, which enables breaking complex graph mining algorithms into simple building blocks that can be separately experimented with. GMS is supported with a broad concurrency analysis for portability in performance insights, and a novel performance metric to assess the throughput of graph mining algorithms, enabling more insightful evaluation. As use cases, we harness GMS to rapidly redesign and accelerate state-of-the-art baselines of core graph mining problems: degeneracy reordering (by &gt;2X), maximal clique listing (by &gt;9√ó), k-clique listing (by up to 1.1√ó), and subgraph isomorphism (by 2.5√ó), also obtaining better theoretical performance bounds.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476252®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/0003J00Z21•titleŸNA Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic Car-Hailing.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476271©publisher±Proc. VLDB Endow.ßauthorsïØPeng Cheng 0003™Jiabao Jin≠Lei Chen 0002ØXuemin Lin 0001´Libin Zheng®Abstract⁄TWith the rapid development of smart mobile devices, the car-hailing platforms (e.g., Uber or Lyft) have attracted much attention from the academia and the industry. In this paper, we consider a dynamic car-hailing problem, namely maximum revenue vehicle dispatching (MRVD), in which rider requests dynamically arrive and drivers need to serve riders such that the entire revenue of the platform is maximized. We prove that the MRVD problem is NP-hard and intractable. To handle the MRVD problem, we propose a queueing-based vehicle dispatching framework, which first uses existing machine learning models to predict the future vehicle demand of each region, then estimates the idle time periods of drivers through a double-sided queueing model for each region. With the information of the predicted vehicle demands and estimated idle time periods of drivers, we propose two batch-based vehicle dispatching algorithms to efficiently assign suitable drivers to riders such that the expected overall revenue of the platform is maximized during each batch processing. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches over both real and synthetic datasets. In summary, our methods can achieve 3% ~ 10% increase on overall revenue without sacrificing on running speed compared with the state-of-the-art solutions.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476271®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/KangGBHSZ21•titleŸGAccelerating Approximate Aggregation Queries with Expensive Predicates.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476285©publisher±Proc. VLDB Endow.ßauthorsñ´Daniel Kang´John Guibas¨Peter Bailis≥Tatsunori Hashimoto¶Yi Sun≠Matei Zaharia®Abstract⁄µResearchers and industry analysts are increasingly interested in computing aggregation queries over large, unstructured datasets with selective predicates that are computed using expensive deep neural networks (DNNs). As these DNNs are expensive and because many applications can tolerate approximate answers, analysts are interested in accelerating these queries via approximations. Unfortunately, standard approximate query processing techniques to accelerate such queries are not applicable because they assume the result of the predicates are available ahead of time. Furthermore, recent work using cheap approximations (i.e., proxies) do not support aggregation queries with predicates.To accelerate aggregation queries with expensive predicates, we develop and analyze a query processing algorithm that leverages proxies (ABAE). ABAE must account for the key challenge that it may sample records that do not satisfy the predicate. To address this challenge, we first use the proxy to group records into strata so that records satisfying the predicate are ideally grouped into few strata. Given these strata, ABAE uses pilot sampling and plugin estimates to sample according to the optimal allocation. We show that ABAE converges at an optimal rate in a novel analysis of stratified sampling with draws that may not satisfy the predicate. We further show that ABAE outperforms on baselines on six real-world datasets, reducing labeling costs by up to 2.3X.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476285®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/Renz-WielandDKG21•titleŸ5Just Move It! Dynamic Parameter Allocation in Action.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476325©publisher±Proc. VLDB Endow.ßauthorsï∂Alexander Renz-WielandØTobias Drobisch™Zoi KaoudiÆRainer Gemulla¨Volker Markl®Abstract⁄ØParameter servers (PSs) ease the implementation of distributed machine learning systems, but their performance can fall behind that of single machine baselines due to communication overhead. We demonstrate Lapse, an open source PS with dynamic parameter allocation. Previous work has shown that dynamic parameter allocation can improve PS performance by up to two orders of magnitude and lead to near-linear speed-ups over single machine baselines. This demonstration illustrates how Lapse is used and why it can provide order-of-magnitude speed-ups over other PSs. To do so, this demonstration interactively analyzes and visualizes how dynamic parameter allocation looks like in action.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476325®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/OrogatLE21•titleŸNCBench: Towards Better Evaluation of Question Answering Over Knowledge Graphs.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457398©publisher±Proc. VLDB Endow.ßauthorsì∞Abdelghny Orogat¨Isabelle Liu≠Ahmed El-Roby®Abstract⁄`Recently, there has been an increase in the number of knowledge graphs that can be only queried by experts. However, describing questions using structured queries is not straightforward for non-expert users who need to have sufficient knowledge about both the vocabulary and the structure of the queried knowledge graph, as well as the syntax of the structured query language used to describe the user's information needs. The most popular approach introduced to overcome the aforementioned challenges is to use natural language to query these knowledge graphs. Although several question answering benchmarks can be used to evaluate question-answering systems over a number of popular knowledge graphs, choosing a benchmark to accurately assess the quality of a question answering system is a challenging task.In this paper, we introduce CBench, an extensible, and more informative benchmarking suite for analyzing benchmarks and evaluating question answering systems. CBench can be used to analyze existing benchmarks with respect to several fine-grained linguistic, syntactic, and structural properties of the questions and queries in the benchmark. We show that existing benchmarks vary significantly with respect to these properties deeming choosing a small subset of them unreliable in evaluating QA systems. Until further research improves the quality and comprehensiveness of benchmarks, CBench can be used to facilitate this evaluation using a set of popular benchmarks that can be augmented with other user-provided benchmarks. CBench not only evaluates a question answering system based on popular single-number metrics but also gives a detailed analysis of the linguistic, syntactic, and structural properties of answered and unanswered questions to better help the developers of question answering systems to better understand where their system excels and where it struggles.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457398®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/KulkarniCS21•titleŸGAchieving High Throughput and Elasticity in a Larger-than-Memory Store.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457406©publisher±Proc. VLDB Endow.ßauthorsìµChinmay Kulkarni 0002¥Badrish Chandramouli≠Ryan Stutsman®Abstract⁄Millions of sensors, mobile applications and machines now generate billions of events. Specialized many-core key-value stores (KVSs) can ingest and index these events at high rates (over 100 Mops/s on one machine) if events are generated on the same machine; however, to be practical and cost-effective they must ingest events over the network and scale across cloud resources elastically.We present Shadowfax, a new distributed KVS based on FASTER, that transparently spans DRAM, SSDs, and cloud blob storage while serving 130 Mops/s/VM over commodity Azure VMs using conventional Linux TCP. Beyond high single-VM performance, Shadowfax uses a unique approach to distributed reconfiguration that avoids any server-side key ownership checks or cross-core coordination both during normal operation and migration. Hence, Shadowfax can shift load in 17 s to improve system throughput by 10 Mops/s with little disruption. Compared to the state-of-the-art, it has 8x better throughput (than Seastar+memcached) and avoids costly I/O to move cold data during migration. On 12 machines, Shadowfax retains its high throughput to perform 930 Mops/s, which, to the best of our knowledge, is the highest reported throughput for a distributed KVS used for large-scale data ingestion and indexing.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457406®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Mu0ZZLT21•titleŸJAssassin: an Automatic claSSificAtion system baSed on algorithm SelectIoN.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476336©publisher±Proc. VLDB Endow.ßauthorsñ©Tianyu Mu±Hongzhi Wang 0001≠Shenghe ZhengÆShaoqing Zhang´Cheng Liang´Haoyun Tang®Abstract⁄ÙThe increasing complexity of data analysis tasks makes it dependent on human expertise and challenging for non-experts. One of the major challenges faced in data analysis is the selection of the proper algorithm for given tasks and data sets. Motivated by this, we develop Assassin, aiming at helping users without enough expertise to automatically select optimal algorithms for classification tasks. By embedding meta-learning techniques and reinforced policy, our system can automatically extract experiences from previous tasks and train a meta-classifier to implement algorithm recommendations. Then we apply genetic search to explore hyperparameter configuration for the selected algorithm. We demonstrate Assassin with classification tasks from OpenML. The system chooses an appropriate algorithm and optimal hyperparameter configuration for them to achieve a high-level performance target. The Assassin has a user-friendly interface that allows users to customize the parameters during the search process.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476336®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/EltabakhSAANHCZ21•titleŸRNot Black-Box Anymore! Enabling Analytics-Aware Optimizations in Teradata Vantage.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476375©publisher±Proc. VLDB Endow.ßauthorsö≥Mohamed Y. Eltabakh≥Anantha Subramanian≠Awny Al-Omari±Mohammed Al-Kateb´Sanjay Nair¨Mahbub Hasan≤Wellington Cabrera≠Charles Zhang¨Amit KishoreÆSnigdha Prasad®Abstract⁄9Teradata Vantage is a platform for integrating a broad range of analytical functions and capabilities with the Teradata's SQL engine. One of the main challenges in optimizing the execution of these analytical functions is that many of them are not only black boxes, but also have polymorphic nature, i.e., their behavior and properties may change depending on the invocation context. In this paper, we first demonstrate the inherent complexity in optimizing polymorphic functions, and then present the Vantage's Collaborative Optimizer, which is a cross-platform optimizer designed for optimizing the analytical functions invoked from within the SQL engine. The Collaborative Optimizer is the industry-first effort towards enabling analytics-aware optimizations over polymorphic analytical functions. We present a novel markup language-based approach for expressing the functions' polymorphic properties via a set of well-defined instructions. The Collaborative Optimizer uses these instructions at query time to infer the corresponding properties, and then decide on the applicable optimizations. From several possible optimizations, we showcase two core optimizations, namely "projection push" and "predicate push", which aim at optimizing the data movement to and from the analytical functions. The experiments using the Teradata-MLE analytical system demonstrate the expressiveness power and flexibility of the proposed markup language. Moreover, benchmark and real-world customer queries show the significant performance gain that the Collaborative Optimizer brings to the Vantage system.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476375®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/VandevoortK0N21•titleŸ<Robustness against Read Committed for Transaction Templates.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476268©publisher±Proc. VLDB Endow.ßauthorsî±Brecht Vandevoort´Bas Ketsman≥Christoph Koch 0001´Frank Neven®Abstract⁄ŒThe isolation level Multiversion Read Committed (RC), offered by many database systems, is known to trade consistency for increased transaction throughput. Sometimes, transaction workloads can be safely executed under RC obtaining the perfect isolation of serializability at the lower cost of RC. To identify such cases, we introduce an expressive model of transaction programs to better reason about the serializability of transactional workloads. We develop tractable algorithms to decide whether any possible schedule of a workload executed under RC is serializable (referred to as the robustness problem). Our approach yields robust subsets that are larger than those identified by previous methods. We provide experimental evidence that workloads that are robust against RC can be evaluated faster under RC compared to stronger isolation levels. We discuss techniques for making workloads robust against RC by promoting selective read operations to updates. Depending on the scenario, the performance improvements can be considerable. Robustness testing and safely executing transactions under the lower isolation level RC can therefore provide a direct way to increase transaction throughput without changing DBMS internals.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476268®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/YuanCBYHC21•titleŸmTowards Plug-and-Play Visual Graph Query Interfaces: Data-driven Canned Pattern Selection for Large Networks.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476256©publisher±Proc. VLDB Endow.ßauthorsñ´Zifeng Yuan≠Huey-Eng Chua≤Sourav S. Bhowmick®Zekun Ye≠Wook-Shin Han™Byron Choi®Abstract⁄JCanned patterns (i.e., small subgraph patterns) in visual graph query interfaces (a.k.a GUI) facilitate efficient query formulation by enabling pattern-at-a-time construction mode. However, existing GUIS for querying large networks either do not expose any canned patterns or if they do then they are typically selected manually based on domain knowledge. Unfortunately, manual generation of canned patterns is not only labor intensive but may also lack diversity for supporting efficient visual formulation of a wide range of subgraph queries. In this paper, we present a novel, generic, and extensible framework called TATTOO that takes a data-driven approach to automatically select canned patterns for a GUI from large networks. Specifically, it first decomposes the underlying network into truss-infested and truss-oblivious regions. Then candidate canned patterns capturing different real-world query topologies are generated from these regions. Canned patterns based on a user-specified plug are then selected for the GUI from these candidates by maximizing coverage and diversity, and by minimizing the cognitive load of the pattern set. Experimental studies with real-world datasets demonstrate the benefits of TATTOO. Importantly, this work takes a concrete step towards realizing plug-and-play visual graph query interfaces for large networks.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476256®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/AbbarSMEM21•titleŸGA Demonstration of QARTA: An ML-based System for Accurate Map Services.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476329©publisher±Proc. VLDB Endow.ßauthorsï≠Sofiane AbbarØRade StanojevicÆMashaal Musleh≤Mohamed M. Elshrif±Mohamed F. Mokbel®Abstract⁄ This demo presents QARTA; an open-source full-fledged system for highly accurate and scalable map services. QARTA employs machine learning techniques to: (a) construct its own highly accurate map in terms of both map topology and edge weights, and (b) calibrate its query answers based on contextual information, including transportation modality, underlying algorithm, and time of day/week. The demo is based on actual deployment of QARTA in all Taxis in the State of Qatar and in the third-largest food delivery company in the country, and receiving hundreds of thousands of daily API calls with a real-time response time. Audience will be able to interact with the demo through various scenarios that show QARTA map and query accuracy as well as internals of QARTA.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476329®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ArnaoutRWP21•titleŸBWikinegata: a Knowledge Base with Interesting Negative Statements.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476350©publisher±Proc. VLDB Endow.ßauthorsî¨Hiba Arnaout∞Simon RazniewskiÆGerhard Weikum´Jeff Z. Pan®Abstract⁄ËDatabases about general-world knowledge, so-called knowledge bases (KBs), are important in applications such as search and question answering. Traditionally, although KBs use open world assumption, popular KBs only store positive information, but withhold from taking any stance towards statements not contained in them. In this demo, we show that storing and presenting noteworthy negative statements would be important to overcome current limitations in various use cases. In particular, we introduce the Wiki negata portal, a platform to explore negative statements for Wikidata entities, by implementing a peer-based ranking method for inferring interesting negations in KBs. The demo is available at http://d5demos.mpi-inf.mpg.de/negation.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476350®Keywordsê¶Badges¿•Track∞research-article®Citation®Download©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/Pandis21•titleŸ!The evolution of Amazon Redshift.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476391©publisher±Proc. VLDB Endow.ßauthorsë±Ippokratis Pandis®Abstract⁄SIn 2013, Amazon Web Services revolutionized the data warehousing industry by launching Amazon Redshift [7], the first fully managed, petabyte-scale enterprise-grade cloud data warehouse. Amazon Redshift made it simple and cost-effective to efficiently analyze large volumes of data using existing business intelligence tools. This launch was a significant leap from the traditional on-premise data warehousing solutions, which were expensive, not elastic, and required significant expertise to tune and operate. Customers embraced Amazon Redshift and it became the fastest growing service in AWS. Today, tens of thousands of customers use Amazon Redshift in AWS's global infrastructure of 25 launched Regions and 81 Availability Zones (AZs), to process exabytes of data daily.The success of Amazon Redshift inspired a lot of innovation in the analytics segment, e.g. [1, 2, 4, 10], which in turn has benefited customers. In the last few years, the use cases for Amazon Redshift have evolved and in response, Amazon Redshift continues to deliver a series of innovations that delight customers.In this paper, we give an overview of Amazon Redshift's system architecture. Amazon Redshift is a columnar MPP data warehouse [7]. As shown in Figure 1, an Amazon Redshift compute cluster consists of a coordinator node, called the leader node, and multiple compute nodes. Data is stored on Redshift Managed Storage, backed by Amazon S3, and cached in compute nodes on locally-attached SSDs in compressed columnar fashion. Tables are either replicated on every compute node or partitioned into multiple buckets that are distributed among all compute nodes. AQUA is a query acceleration layer that leverages FPGAs to improve performance. CaaS is a caching microservice of optimized generated code for the various query fragments executed in the Amazon Redshift fleet.The innovation at Amazon Redshift continues at accelerated pace. Its development is centered around four streams. First, Amazon Redshift strives to provide industry-leading data warehousing performance. Amazon Redshift's query execution blends database operators in each query fragment via code generation. It combines prefetching and vectorized execution with code generation to achieve maximum efficiency. This allows Amazon Redshift to scale linearly when processing from a few terabytes to petabytes of data. Figure 2 depicts the total execution time of the Cloud Data Warehouse Benchmark Derived from TPC-DS 2.13 [6] while scaling dataset size and hardware simultaneously. Amazon Redshift's performance remains nearly flat for a given ratio of data to hardware, as data volume increases from 30TB to 1PB. This linear scaling to the petabyte scale makes it easy, predictable and cost-efficient for customers to on-board new datasets and workloads.Second, customers needed to process more data and wanted to support an increasing number of concurrent users or independent compute clusters that are operating over the Redshift-managed data and the data in Amazon S3. We present Redshift Managed Storage, Redshift's high-performance transactional storage layer, which is disaggregated from the Redshift compute layer and allows a single database to grow to tens of petabytes. We also describe Redshift's compute scaling capabilities. In particular, we present how Redshift can scale up by elastically resizing the size of each cluster, and how Redshift can scale out and increase its throughput via multi-cluster autoscaling, called Concurrency Scaling. With Concurrency Scaling, customers can have thousands of concurrent users executing queries on the same Amazon Redshift endpoint. We also talk about data sharing, which allows users to have multiple isolated compute clusters consume the same datasets in Redshift Managed Storage. Elastic resizing, concurrency scaling and data sharing can be combined giving multiple compute scaling options to the Amazon Redshift customers.Third, as Amazon Redshift became the most widely used cloud data warehouse, its users wanted it to be even easier to use. For that, Redshift introduced ML-based autonomics. We present how Redshift automated among others workload management, physical tuning, the refresh of materialized views (MVs), along with automated MVs-based optimization that rewrites queries to use MVs. We also present how we leverage ML to improve the operational health of the service and deal with gray failures [8].Finally, as AWS offers a wide range of purpose-built services, Amazon Redshift provides seamless integration with the AWS ecosystem and novel abilities in ingesting and ELTing semistructured data (e.g., JSON) using the PartiQL extension of SQL [9]. AWS purpose-built services include the Amazon S3 object storage, transactional databases (e.g., DynamoDB [5] and Aurora [11]) and the ML services of Amazon Sagemaker. We present how AWS and Redshift make it easy for their customers to use the best service for each job and seamlessly take advantage of Redshift's best of class analytics capabilities. For example, we talk about Redshift Spectrum [3] that allows Redshift to query data in open-file formats in Amazon S3. We present how Redshift facilitates both the in-place querying of data in OLTP services, using Redshift's Federated Querying, as well as the copy of data to Redshift, using Glue Elastic Views. We also present how Redshift can leverage the catabilities of Amazon Sagemaker through SQL and without data movement.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476391®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÕ%©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/DuongHYWNA21•titleŸDEfficient Streaming Subgraph Isomorphism with Graph Neural Networks.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446097©publisher±Proc. VLDB Endow.ßauthorsñØChi Thang Duong™Dung Hoang´Hongzhi Yin±Matthias WeidlichµQuoc Viet Hung Nguyen´Karl Aberer®Abstract⁄ÍQueries to detect isomorphic subgraphs are important in graph-based data management. While the problem of subgraph isomorphism search has received considerable attention for the static setting of a single query, or a batch thereof, existing approaches do not scale to a dynamic setting of a continuous stream of queries. In this paper, we address the scalability challenges induced by a stream of subgraph isomorphism queries by caching and re-use of previous results. We first present a novel subgraph index based on graph embeddings that serves as the foundation for efficient stream processing. It enables not only effective caching and re-use of results, but also speeds-up traditional algorithms for subgraph isomorphism in case of cache misses. Moreover, we propose cache management policies that incorporate notions of reusability of query results. Experiments using real-world datasets demonstrate the effectiveness of our approach in handling isomorphic subgraph search for streams of queries.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446097®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃˆ©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/00080LWZCDHWWZR21•titleŸmOptimizing An In-memory Database System For AI-powered On-line Decision Augmentation Using Persistent Memory.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446102©publisher±Proc. VLDB Endow.ßauthorsúØCheng Chen 0008≠Jun Yang 0022ßMian Lu™Taize Wang™Zhao Zheng¨Yuqiang Chen´Wenyuan Dai¨Bingsheng He≠Weng-Fai Wong®Guoan Wu´Yuping Zhao´Andy Rudoff®Abstract⁄DOn-line decision augmentation (OLDA) has been considered as a promising paradigm for real-time decision making powered by Artificial Intelligence (AI). OLDA has been widely used in many applications such as real-time fraud detection, personalized recommendation, etc. On-line inference puts real-time features extracted from multiple time windows through a pre-trained model to evaluate new data to support decision making. Feature extraction is usually the most time-consuming operation in many OLDA data pipelines. In this work, we started by studying how existing in-memory databases can be leveraged to efficiently support such real-time feature extractions. However, we found that existing in-memory databases cost hundreds or even thousands of milliseconds. This is unacceptable for OLDA applications with strict real-time constraints. We therefore propose FEDB (&lt;u&gt;F&lt;/u&gt;eature &lt;u&gt;E&lt;/u&gt;ngineering &lt;u&gt;D&lt;/u&gt;ata&lt;u&gt;b&lt;/u&gt;ase), a distributed in-memory database system designed to efficiently support on-line feature extraction. Our experimental results show that FEDB can be one to two orders of magnitude faster than the state-of-the-art in-memory databases on real-time feature extraction. Furthermore, we explore the use of the Intel Optane DC Persistent Memory Module (PMEM) to make FEDB more cost-effective. When comparing the proposed PMEM-optimized persistent skiplist to the FEDB using DRAM+SSD, PMEM-based FEDB can shorten the tail latency up to 19.7%, reduce the recovery time up to 99.7%, and save up to 58.4% total cost of a real OLDA pipeline.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446102®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃﬁ©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/DayanRNDRBMFZAT21•titleŸ:The End of Moore's Law and the Rise of The Data Processor.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476373©publisher±Proc. VLDB Endow.ßauthorsû©Niv Dayan≠Yuval Rochman™Iddo Naiss∞Shmuel DashevskyØNoam Rabinovich∞Edward Bortnikov©Igal Maly≠Ofer Frishman≠Itai Ben ZionßAvraham¨Moshe Twitto´Uri BeitlerØEvgeni Ginzburg´Mark Mokryn®Abstract⁄éWith the end of Moore's Law, database architects are turning to hardware accelerators to offload computationally intensive tasks from the CPU. In this paper, we show that accelerators can facilitate far more than just computation: they enable algorithms and data structures that lavishly expand computation in order to optimize for disparate cost metrics. We introduce the Pliops Extreme Data Processor (XDP), a novel storage engine implemented from the ground up using customized hardware. At its core, XDP consists of an accelerated hash table to index the data in storage using less memory and fewer storage accesses for queries than the best alternative. XDP also employs an accelerated compressor, a capacitor, and a lock-free RAID sub-system to minimize storage space and recovery time while minimizing performance penalties. As a result, XDP overcomes cost contentions that have so far been inescapable.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476373®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃû©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/00010MT21•titleŸAData Augmentation for ML-driven Data Preparation and Integration.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476403©publisher±Proc. VLDB Endow.ßauthorsîØYuliang Li 0001±Xiaolan Wang 0001≠Zhengjie MiaoÆWang-Chiew Tan®Abstract⁄ãIn recent years, we have witnessed the development of novel data augmentation (DA) techniques for creating additional training data needed by machine learning based solutions. In this tutorial, we will provide a comprehensive overview of techniques developed by the data management community for data preparation and data integration. In addition to surveying task-specific DA operators that leverage rules, transformations, and external knowledge for creating additional training data, we also explore the advanced DA techniques such as interpolation, conditional generation, and DA policy learning. Finally, we describe the connection between DA and other machine learning paradigms such as active learning, pre-training, and weakly-supervised learning. We hope that this discussion can shed light on future research directions for a holistic data augmentation framework for high-quality dataset creation.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476403®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃú©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/ParkCOL21•titleŸ SaS: SSD as SQL Database System.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461538©publisher±Proc. VLDB Endow.ßauthorsîØJong-Hyeok Park™Soyee Choi©Gihwan Oh±Sang Won Lee 0001®Abstract⁄√Every database engine runs on top of an operating system in the host, strictly separated with the storage. This more-than-half-century-old IHDE (In-Host-Database-Engine) architecture, however, reveals its limitations when run on fast flash memory SSDs. In particular, the IO stacks incur significant run-time overhead and also hinder vertical optimizations between database engines and SSDs. In this paper, we envisage a new database architecture, called SaS (SSD as SQL database engine), where a full-blown SQL database engine runs inside SSD, tightly integrated with SSD architecture without intervening kernel stacks. As IO stacks are removed, SaS is free from their run-time overhead and further can explore numerous vertical optimizations between database engine and SSD. SaS evolves SSD from dummy block device to database server with SQL as its primary interface. The benefit of SaS will be more outstanding in the data centers where the distance between database engine and the storage is ever widening because of virtualization, storage disaggregation, and open software stacks. The advent of computational SSDs with more compute resource will enable SaS to be more viable and attractive database architecture.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461538®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃô©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/LiLHLLLG21•titleŸGFrequency-Hiding Order-Preserving Encryption with Small Client Storage.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484228©publisher±Proc. VLDB Endow.ßauthorsó™Dongjie LißSiyi Lv´Yanyu Huang™Yijing Liu¨Tong Li 0011©Zheli LiuÆLiang Guo 0013®Abstract⁄The range query on encrypted databases is usually implemented using the order-preserving encryption (OPE) technique which preserves the order of plaintexts. Since the frequency leakage of plaintexts makes OPE vulnerable to frequency-analyzing attacks, some frequency-hiding order-preserving encryption (FH-OPE) schemes are proposed. However, existing FH-OPE schemes require either the large client storage of size O(n) or O(log n) rounds of interactions for each query, where n is the total number of plaintexts. To this end, we propose a FH-OPE scheme that achieves the small client storage without additional client-server interactions. In detail, our scheme achieves O(N) client storage and 1 interaction per query, where N is the number of distinct plaintexts and N ‚â§ n. Especially, our scheme has a remarkable performance when N ‚â™ n. Moreover, we design a new coding tree for producing the order-preserving encoding which indicates the order of each ciphertext in the database. The coding strategy of our coding tree ensures that encodings update in the low frequency when inserting new ciphertexts. Experimental results show that the single round interaction and low-frequency encoding updates make our scheme more efficient than previous FH-OPE schemes.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484228®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃî©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/GuptaMS21•titleŸQColumnar Storage and List-based Processing for Graph Database Management Systems.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476297©publisher±Proc. VLDB Endow.ßauthorsì≠Pranjal Gupta≠Amine MhedhbiØSemih Salihoglu®Abstract⁄’We revisit column-oriented storage and query processing techniques in the context of contemporary graph database management systems (GDBMSs). Similar to column-oriented RDBMSs, GDBMSs support read-heavy analytical workloads that however have fundamentally different data access patterns than traditional analytical workloads. We first derive a set of desiderata for optimizing storage and query processors of GDBMS based on their access patterns. We then present the design of columnar storage, compression, and query processing techniques based on these desiderata. In addition to showing direct integration of existing techniques from columnar RDBMSs, we also propose novel ones that are optimized for GDBMSs. These include a novel list-based query processor, which avoids expensive data copies of traditional block-based processors under many-to-many joins, a new data structure we call single-indexed edge property pages and an accompanying edge ID scheme, and a new application of Jacobson's bit vector index for compressing NULL values and empty lists. We integrated our techniques into the GraphflowDB in-memory GDBMS. Through extensive experiments, we demonstrate the scalability and query performance benefits of our techniques.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476297®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃã©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/OuelletteSNBZPM21•titleΩRONIN: Data Lake Exploration.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476364©publisher±Proc. VLDB Endow.ßauthorsóÆPaul OuelletteØAidan Sciortino±Fatemeh NargesianπBahar Ghadiri Bashardoost™Erkang Zhu©Ken Q. Pu∞Ren√©e J. Miller®Abstract⁄!Dataset discovery can be performed using search (with a query or keywords) to find relevant data. However, the result of this discovery can be overwhelming to explore. Existing navigation techniques mostly focus on linkage graphs that enable navigation from one data set to another based on similarity or joinability of attributes. However, users often do not know which data set to start the navigation from. RONIN proposes an alternative way to navigate by building a hierarchical structure on a collection of data sets: the user navigates between groups of data sets in a hierarchical manner to narrow down to the data of interest. We demonstrate RONIN, a tool that enables user exploration of a data lake by seamlessly integrating the two common modalities of discovery: data set search and navigation of a hierarchical structure. In RONIN, a user can perform a keyword search or joinability search over a data lake, then, navigate the result using a hierarchical structure, called an organization, that is created on the fly. While navigating an organization, the user may switch to the search mode, and back to navigation on an organization that is updated based on search. This integration of search and navigation provides great power in allowing users to find and explore interesting data in a data lake.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476364®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃÇ©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/EdaraP21•titleŸ)Big Metadata : When Metadata is Big Data.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476385©publisher±Proc. VLDB Endow.ßauthorsí´Pavan Edara∞Mosha Pasumansky®Abstract⁄ÊThe rapid emergence of cloud data warehouses like Google BigQuery has redefined the landscape of data analytics. With the growth of data volumes, such systems need to scale to hundreds of EiB of data in the near future. This growth is accompanied by an increase in the number of objects stored and the amount of metadata such systems must manage. Traditionally, Big Data systems have tried to reduce the amount of metadata in order to scale the system, often compromising query performance. In Google BigQuery, we built a metadata management system that demonstrates that massive scale can be achieved without such tradeoffs. We recognized the benefits that fine grained metadata provides for query processing and we built a metadata system to manage it effectively. We use the same distributed query processing and data management techniques that we use for managing data to handle Big metadata. Today, BigQuery uses these techniques to support queries over billions of objects and their metadata.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476385®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloadu©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/PotharajuKS0NDP21•titleŸ4Hyperspace: The Indexing Subsystem of Azure Synapse.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476382©publisher±Proc. VLDB Endow.ßauthors‹ ØRahul Potharaju©Terry Kim´Eunjin SongÆWentao Wu 0001©Lev Novik¨Apoorve DaveØPouria PirzadehÆAndrew Fogarty≠Gurleen Dhody©Jiying Li≠Vidip Acharya±Sinduja Ramanujam≠Nicolas BrunoπC√©sar A. Galindo-Legaria≤Vivek R. Narasayya±Surajit Chaudhuri©Anil Nori¨Tomas Talius≤Raghu Ramakrishnan®Abstract⁄vMicrosoft recently introduced Azure Synapse Analytics, which offers an integrated experience across data ingestion, storage, and querying in Apache Spark and T-SQL over data in the lake, including files and warehouse tables. In this paper, we present our experiences with designing and implementing Hyperspace, the indexing subsystem underlying Synapse. Hyperspace enables users to build multiple types of secondary indexes on their data, maintain them through a multi-user concurrency model, and leverage them automatically---without any change to their application code---for query/workload acceleration. Many requirements of Hyperspace are based on feedback from several enterprise customers. We present the details of Hyperspace's underlying design, the user-facing APIs, its concurrency control protocol for index access, its index-aware query processing techniques, and its maintenance mechanisms for handling index updates. Evaluations over standard industry benchmarks and real customer workloads show that Hyperspace can accelerate query execution by up to 10x and in certain real-world workloads, even up to two orders of magnitude.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476382®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloadu©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/LiuWC21•titleŸPTSCache: An Efficient Flash-based Caching Scheme for Time-series Data Workloads.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484225©publisher±Proc. VLDB Endow.ßauthorsì®Jian Liu™Kefei WangÆFeng Chen 0005®Abstract⁄éTime-series databases are becoming an indispensable component in today's data centers. In order to manage the rapidly growing time-series data, we need an effective and efficient system solution to handle the huge traffic of time-series data queries. A promising solution is to deploy a high-speed, large-capacity cache system to relieve the burden on the backend time-series databases and accelerate query processing. However, time-series data is drastically different from other traditional data workloads, bringing both challenges and opportunities. In this paper, we present a flash-based cache system design for time-series data, called TSCache. By exploiting the unique properties of time-series data, we have developed a set of optimization schemes, such as a slab-based data management, a two-layered data indexing structure, an adaptive time-aware caching policy, and a low-cost compaction process. We have implemented a prototype based on Twitter's Fatcache. Our experimental results show that TSCache can significantly improve client query performance, effectively increasing the bandwidth by a factor of up to 6.7 and reducing the latency by up to 84.2%.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484225®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloadt©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/KargarN21•titleŸ<Extending the Lifetime of NVM: Challenges and Opportunities.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476406©publisher±Proc. VLDB Endow.ßauthorsí¨Saeed Kargar¨Faisal Nawab®Abstract⁄LRecently, Non-Volalile Memory (NVM) technology has revolutionized the landscape or memory systems. With many advantages, such as non volatility and near zero standby power consumption, these byte-addressable memory technologies are taking the place of DRAMs. Nonetheless, they also present some limitations, such as limited write endurance, which hinders their widespread use in today's systems. Furthermore, adjusting current data management systems to embrace these new memory technologies and all their potential is proving to be a nontrivial task. Because of this, a substantial amount of research has been done, from both the database community and the storage systems community, that tries to improve various aspects of NVMs to integrate these technologies into the memory hierarchy. In this tutorial we survey state-of-the-art work on deploying NVMs in database and storage systems communities and the ways their limitations are being handled within these communities. In particular, we focus on the challenges that are related to low write endurance and extending the lifetime of NVM devices.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476406®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloadn©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/TsarasTNP21•titleŸgCollective Influence Maximization for Multiple Competing Products with an Awareness-to-Influence Model.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450981©publisher±Proc. VLDB Endow.ßauthorsîØDimitris Tsaras±George Trimponias∞Lefteris Ntaflos±Dimitris Papadias®Abstract⁄Influence maximization (IM) is a fundamental task in social network analysis. Typically, IM aims at selecting a set of seeds for the network that influences the maximum number of individuals. Motivated by practical applications, in this paper we focus on an IM variant, where the owner of multiple competing products wishes to select seeds for each product so that the collective influence across all products is maximized. To capture the competing diffusion processes, we introduce an Awareness-to-Influence (AtI) model. In the first phase, awareness about each product propagates in the social graph unhindered by other competing products. In the second phase, a user adopts the most preferred product among those encountered in the awareness phase. To compute the seed sets, we propose GCW, a game-theoretic framework that views the various products as agents, which compete for influence in the social graph and selfishly select their individual strategy. We show that AtI exhibits monotonicity and submodularity; importantly, GCW is a monotone utility game. This allows us to develop an efficient best-response algorithm, with quality guarantees on the collective utility. Our experimental results suggest that our methods are effective, efficient, and scale well to large social networks.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450981®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloadi©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Trummer21•titleŸWThe Case for NLP-Enhanced Database Tuning: Towards Tuning Tools that "Read the Manual".§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450984©publisher±Proc. VLDB Endow.ßauthorsë∞Immanuel Trummer®Abstract⁄NA large body of knowledge on database tuning is available in the form of natural language text. We propose to leverage natural language processing (NLP) to make that knowledge accessible to automated tuning tools. We describe multiple avenues to exploit NLP for database tuning, and outline associated challenges and opportunities. As a proof of concept, we describe a simple prototype system that exploits recent NLP advances to mine tuning hints from Web documents. We show that mined tuning hints improve performance of MySQL and Postgres on TPC-H, compared to the default configuration.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450984®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloade©PaperRefsêã§Infoá§typeßarticle£keyøjournals/pvldb/AbeywickramaLT21•titleŸYOptimizing Bipartite Matching in Real-World Applications by Incremental Cost Computation.§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450983©publisher±Proc. VLDB Endow.ßauthorsìµTenindra Abeywickrama¨Victor Liang¨Kian-Lee Tan®Abstract⁄NThe Kuhn-Munkres (KM) algorithm is a classical combinatorial optimization algorithm that is widely used for minimum cost bipartite matching in many real-world applications, such as transportation. For example, a ride-hailing service may use it to find the optimal assignment of drivers to passengers to minimize the overall wait time. Typically, given two bipartite sets, this process involves computing the edge costs between all bipartite pairs and finding an optimal matching. However, existing works overlook the impact of edge cost computation on the overall running time. In reality, edge computation often significantly outweighs the computation of the optimal assignment itself, as in the case of assigning drivers to passengers which involves computation of expensive graph shortest paths. Following on from this observation, we observe common real-world settings exhibit a useful property that allows us to incrementally compute edge costs only as required using an inexpensive lower-bound heuristic. This technique significantly reduces the overall cost of assignment compared to the original KM algorithm, as we demonstrate experimentally on multiple real-world data sets, workloads, and problems. Moreover, our algorithm is not limited to this domain and is potentially applicable in other settings where lower-bounding heuristics are available.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450983®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download`©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/HelalHAM21•titleŸTA Demonstration of KGLac: A Data Discovery and Enrichment Platform for Data Science.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476317©publisher±Proc. VLDB Endow.ßauthorsî´Ahmed Helal≠Mossad Helali¨Khaled Ammar≤Essam Mansour 0001®Abstract⁄÷Data science growing success relies on knowing where a relevant dataset exists, understanding its impact on a specific task, finding ways to enrich a dataset, and leveraging insights derived from it. With the growth of open data initiatives, data scientists need an extensible set of effective discovery operations to find relevant data from their enterprise datasets accessible via data discovery systems or open datasets accessible via data portals. Existing portals and systems suffer from limited discovery support and do not track the use of a dataset and insights derived from it. We will demonstrate KGLac, a system that captures metadata and semantics of datasets to construct a knowledge graph (GLac) interconnecting data items, e.g., tables and columns. KGLac supports various data discovery operations via SPARQL queries for table discovery, unionable and joinable tables, plus annotation with related derived insights. We harness a broad range of Machine Learning (ML) approaches with GLac to enable automatic graph learning for advanced and semantic data discovery. The demo will showcase how KGLac facilitates data discovery and enrichment while developing an ML pipeline to evaluate potential gender salary bias in IT jobs.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476317®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download\©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/EchihabiPZ21•titleŸWNew Trends in High-D Vector Similarity Search: AI-driven, Progressive, and Distributed.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476407©publisher±Proc. VLDB Endow.ßauthorsìØKarima EchihabiØThemis Palpanas≥Kostas Zoumpatianos®Abstract⁄±Similarity search is a core operation of many critical applications, involving massive collections of high-dimensional (high-d) objects. Objects can be data series, text, multimedia, graphs, database tables or deep network embeddings. In this tutorial, we revisit the similarity search problem in light of the recent advances in the field and the new big data landscape. We discuss key data science applications that require efficient high-d similarity search, we survey recent approaches and share surprising insights about their strengths and weaknesses, and we discuss open research problems, including the directions of AI-driven, progressive, and distributed high-d similarity search.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476407®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadS©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/000121•titleŸ&Evolution of a Compiling Query Engine.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476410©publisher±Proc. VLDB Endow.ßauthorsë≥Thomas Neumann 0001®Abstract⁄fIn 2011 we showed how to use dynamic code generation to process queries in a data-centric manner. This execution model can produce compact and efficient code and was successfully used by both our own systems and systems of other groups. As the systems become used in practice, additional techniques were developed for shortcomings that did arrive, including low-latency compilation, multi-threading support, and others. This paper gives an overview of the evolution of our query engine within in the last ten years, and points out which problem have to be tackled to bring a compiling system into production usage.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476410®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadP©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/ChenSWFL21•titleŸ;Approximating Median Absolute Deviation with Bounded Error.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476266©publisher±Proc. VLDB Endow.ßauthorsï´Zhiwei Chen´Shaoxu Song™Ziheng Wei¨Jingyun Fang™Jiang Long®Abstract⁄ΩThe median absolute deviation (MAD) is a statistic measuring the variability of a set of quantitative elements. It is known to be more robust to outliers than the standard deviation (SD), and thereby widely used in outlier detection. Computing the exact MAD however is costly, e.g., by calling an algorithm of finding median twice, with space cost O (n) over n elements in a set. In this paper, we propose the first fully mergeable approximate MAD algorithm, OP-MAD, with one-pass scan of the data. Remarkably, by calling the proposed algorithm at most twice, namely TP-MAD, it guarantees to return an (œµ, 1)-accurate MAD, i.e., the error relative to the exact MAD is bounded by the desired œµ or 1. The space complexity is reduced to O(m) while the time complexity is O(n + m log m), where m is the size of the sketch used to compress data, related to the desired error bound œµ. To get a more accurate MAD, i.e., with smaller œµ, the sketch size m will be larger, a trade-off between effectiveness and efficiency. In practice, we often have the sketch size m ‚â™ n, leading to constant space cost O(1) and linear time cost O(n). The extensive experiments over various datasets demonstrate the superiority of our solution, e.g., 160000√ó less memory and 18x faster than the aforesaid exact method in datasets pareto and norm. Finally, we further implement and evaluate the parallelizable TP-MAD in Apache Spark, and the fully mergeable OP-MAD in Structured Streaming.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476266®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadO©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/SenRJFZLL21•titleŸ;AutoExecutor: Predictive Parallelism for Spark SQL Queries.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476362©publisher±Proc. VLDB Endow.ßauthorsó¨Rathijit Sen¨Abhishek Roy¨Alekh Jindal®Rui Fang™Jeff Zheng´Xiaolei Liu™Ruiping Li®Abstract⁄»Right-sizing resources for query execution is important for cost-efficient performance, but estimating how performance is affected by resource allocations, upfront, before query execution is difficult. We demonstrate AutoExecutor, a predictive system that uses machine learning models to predict query run times as a function of the number of allocated executors, that limits the maximum allowed parallelism, for Spark SQL queries running on Azure Synapse.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476362®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadJ©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Zhang0021•titleŸVAn Experimental Evaluation and Guideline for Path Finding in Weighted Dynamic Network.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476267©publisher±Proc. VLDB Endow.ßauthorsì≥Mengxuan Zhang 0001´Lei Li 0003≤Xiaofang Zhou 0001®Abstract⁄±Shortest path computation is a building block of various network applications. Since real-life networks evolve as time passes, the Dynamic Shortest Path (DSP) problem has drawn lots of attention in recent years. However, as DSP has many factors related to network topology, update patterns, and query characteristics, existing works only test their algorithms on limited situations without sufficient comparisons with other approaches. Thus, it is still hard to choose the most suitable method in practice. To this end, we first identify the determinant dimensions and constraint dimensions of the DSP problem and create a complete problem space to cover all possible situations. Then we evaluate the state-of-the-art DSP methods under the same implementation standard and test them systematically under a set of synthetic dynamic networks. Furthermore, we propose the concept of dynamic degree to classify the dynamic environments and use throughput to evaluate their performance. These results can serve as a guideline to find the best solution for each situation during system implementation and also identify research opportunities. Finally, we validate our findings on real-life dynamic networks.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476267®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadG©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/ZhengI20•titleŸ>Compact, Tamper-Resistant Archival of Fine-Grained Provenance.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436909©publisher±Proc. VLDB Endow.ßauthorsí©Nan Zheng©Zack Ives®Abstract⁄Data provenance tools aim to facilitate reproducible data science and auditable data analyses, by tracking the processes and inputs responsible for each result of an analysis. Fine-grained provenance further enables sophisticated reasoning about why individual output results appear or fail to appear. However, for reproducibility and auditing, we need a provenance archival system that is tamper-resistant, and efficiently stores provenance for computations computed over time (i.e., it compresses repeated results). We study this problem, developing solutions for storing fine-grained provenance in relational storage systems while both compressing and protecting it via cryptographic hashes. We experimentally validate our proposed solutions using both scientific and OLAP workloads.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436909®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadG©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/ChenWLHLMCZFLG021•titleŸUFangorn: Adaptive Execution Framework for Heterogeneous Workloads on Shared Clusters.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476376©publisher±Proc. VLDB Endow.ßauthorsû´Yingda Chen¨Jiamang Wang©Yifeng Lu®Ying Han´Zhiqiang Lv™Xuebin MinßHua Cai©Wei Zhang¨Haochuan FanßChao Li®Tao Guan¨Wei Lin 0016¨Yangqing Jia¨Jingren Zhou®Abstract⁄ÊPervasive needs for data explorations at all scales have populated modern distributed platforms with workloads of different characteristics. The growing complexities and diversities have thereafter imposed distinct challenges to execute them on shared clusters in corporate or public clouds. This paper presents Fangorn, an adaptive execution framework built on an enriched graph model. As the underlying infrastructure for core computation platforms at Alibaba, Fangorn supports various execution modes and caters to heterogeneous workloads. With the capability to orchestrate graph executions with both long-running and requested-on-demand resources at the same time, Fangorn allows exploration of tradeoffs between latency and resource efficiency, for jobs of all scales. By modeling distributed job executions as mutable graphs with pluggable components, Fangorn offers a systematic framework to adjust job executions adaptively, according to data statistics collected during run-time. Fangorn supports an array of different computation engines ranging from relational to deep learning, and is fully deployed on production clusters across Alibaba. It manages tens of millions of distributed jobs daily, with job size scaling from one to half-million.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476376®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadF©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/Zaharia21•titleŸ/Designing Production-Friendly Machine Learning.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484241©publisher±Proc. VLDB Endow.ßauthorsë≠Matei Zaharia®Abstract⁄1Building production ML applications is difficult because of their resource cost and complex failure modes. I will discuss these challenges from two perspectives: the Stanford DAWN Lab and experience with large-scale commercial ML users at Databricks. I will then present two emerging ideas to help address these challenges. The first is "ML platforms", an emerging class of software systems that standardize the interfaces used in ML applications to make them easier to build and maintain. I will give a few examples, including the open-source MLflow system from Databricks [3]. The second idea is models that are more "production-friendly" by design. As a concrete example, I will discuss retrieval-based NLP models such as Stanford's ColBERT [1, 2] that query documents from an updateable corpus to perform tasks such as question-answering, which gives multiple practical advantages, including low computational cost, high interpretability, and very fast updates to the model's "knowledge". These models are an exciting alternative to large language models such as GPT-3.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484241®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadF©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/RezigBFPVGS21•titleŸ DICE: Data Discovery by Example.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476353©publisher±Proc. VLDB Endow.ßauthorsóÆEl Kindi RezigØAnshul Bhandari´Anna FarihaÆBenjamin Price∞Allan VanterpoolØVijay Gadepally≥Michael Stonebraker®Abstract⁄™In order to conduct analytical tasks, data scientists often need to find relevant data from an avalanche of sources (e.g., data lakes, large organizational databases). This effort is typically made in an ad hoc, non-systematic manner, which makes it a daunting endeavour. Current data discovery systems typically require the users to find relevant tables manually, usually by issuing multiple queries (e.g., using SQL). However, expressing such queries is nontrivial, as it requires knowledge of the underlying structure (schema) of the data organization in advance. This issue is further exacerbated when data resides in data lakes, where there is no predefined schema that data must conform to. On the other hand, data scientists can often come up with a few example records of interest quickly. Motivated by this observation, we developed DICE---a human-in-the-loop system for &lt;u&gt;D&lt;/u&gt;ata d&lt;u&gt;I&lt;/u&gt;s&lt;u&gt;C&lt;/u&gt;overy by &lt;u&gt;E&lt;/u&gt;xample---that takes user-provided example records as input and returns more records that satisfy the user intent. DICE's key idea is to synthesize a SQL query that captures the user intent, specified via examples. To this end, DICE follows a three-step process: (1) DICE first discovers a few candidate queries by finding join paths across tables within the data lake. (2) Then DICE consults with the user for validation by presenting a few records to them, and, thus, eliminating spurious queries. (3) Based on the user feedback, DICE refines the search and repeats the process until the user is satisfied with the results. We will demonstrate how DICE can help in data discovery through an interactive, example-based interaction.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476353®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadD©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/HossainFPQSWARA21•titleŸLPerfGuard: Deploying ML-for-Systems without Performance Regressions, Almost!§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484233©publisher±Proc. VLDB Endow.ßauthorsû¥H. M. Sajjad Hossain∞Marc T. Friedman´Hiren Patel≠Shi Qiao 0001≤Soundar Srinivasan≠Markus Weimer±Remmelt Ammerlaan∞Lucas Rosenblatt∞Gilbert AntoniusÆPeter Orenberg¨Vijay Ramani¨Abhishek Roy≠Irene Shaffer¨Alekh Jindal®Abstract⁄‘Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called PerfGuard, that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484233®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadB©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/YuWQ00021•titleøOn Querying Historical K-Cores.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476260©publisher±Proc. VLDB Endow.ßauthorsñ™Michael Yu®Dong Wen´Lu Qin 0001ØYing Zhang 0001±Wenjie Zhang 0001ØXuemin Lin 0001®Abstract⁄©Many real-world relationships between entities can be modeled as temporal graphs, where each edge is associated with a timestamp or a time interval representing its occurrence. K-core is a fundamental model used to capture cohesive subgraphs in a simple graph and have drawn much research attention over the last decade. Despite widespread research, none of the existing works support the efficient querying of historical k-cores in temporal graphs. In this paper, given an integer k and a time window, we study the problem of computing all k-cores in the graph snapshot over the time window. We propose an index-based solution and several pruning strategies to reduce the index size. We also design a novel algorithm to construct this index, whose running time is linear to the final index size. Lastly, we conducted extensive experiments on several real-world temporal graphs to show the high effectiveness of our index-based solution.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476260®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadB©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/MoBZP20a•titleŸ5Towards an Efficient Weighted Random Walk Domination.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436915©publisher±Proc. VLDB Endow.ßauthorsî´Songsong Mo´Zhifeng Bao™Ping Zhang¨Zhiyong Peng®Abstract⁄´In this paper, we propose and study a new problem called the weighted random walk domination. Given a weighted graphG(V, E) and a budget B of the weighted random walk, it aims to find a k-size set S, which can minimize the total costs of the remaining nodes to access S through the weighted random walk, which is bounded by B. This problem is critical to a range of real-world applications, such as advertising in social networks and telecommunication base station selection in wireless sensor networks. We first present a dynamic programming based greedy method (DpSel) as a baseline. DpSel is time-consuming when |V| is huge. Thus, to overcome this drawback, we propose a matrix-based greedy method (MatrixSel), which can reduce the computation cost greatly. To further accelerate MatrixSel, we propose a BoundSel approach to reduce the number of the gain computations in each candidate selection by proactively estimating the upper bound of the marginal gain of the candidate node. Notably, all methods can achieve an approximation ratio of (1 - 1/e). Experiments on real datasets have been conducted to verify the efficiency, effectiveness, memory consumption and scalability of our methods.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436915®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadA©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/Xiao0WH021•titleŸLMixer: Efficiently Understanding and Retrieving Visual Content at Web-Scale.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476371©publisher±Proc. VLDB Endow.ßauthorsï¨Mengbai Xiao´An Qin 0001™Yongwei Wu¨Xinjie Huang≥Xiaodong Zhang 0001®Abstract⁄ÎVisual contents, including images and videos, are dominant on the Internet today. The conventional search engine is mainly designed for textual documents, which must be extended to process and manage increasingly high volumes of visual data objects.In this paper, we present Mixer, an effective system to identify and analyze visual contents and to extract their features for data retrievals, aiming at addressing two critical issues: (1) efficiently and timely understanding visual contents, (2) retrieving them at high precision and recall rates without impairing the performance. In Mixer, the visual objects are categorized into different classes, each of which has representative visual features. Subsystems for model production and model execution are developed. Two retrieval layers are designed and implemented for images and videos, respectively. In this way, we are able to perform aggregation retrievals of the two types in efficient ways. The experiments with Baidu's production workloads and systems show that Mixer halves the model production time and raises the feature production throughput by 9.14x. Mixer also achieves the precision and recall of video retrievals at 95% and 97%, respectively. Mixer has been in its daily operations, which makes the search engine highly scalable for visual contents at a low cost. Having observed productivity improvement of upper-level applications in the search engine, we believe our system framework would generally benefit other data processing applications.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476371®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download@©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/ChenN21•titleŸ;On the String Matching with k Differences in DNA Databases.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447695©publisher±Proc. VLDB Endow.ßauthorsí¨Yangjun Chen∞Hoang Hai Nguyen®Abstract⁄ÏIn this paper, we discuss an efficient and effective index mechanism for the string matching with k differences, by which we will find all the substrings of a target string y of length n that align with a pattern string x of length m with not more than k insertions, deletions, and mismatches. A typical application is the searching of a DNA database, where the size of a genome sequence in the database is much larger than that of a pattern. For example, n is often on the order of millions or billions while m is just a hundred or a thousand. The main idea of our method is to transform y to a BWT-array as an index, denoted as BWT(y), and search x against it. The time complexity of our method is bounded by O(k ¬∑ |T|), where T is a tree structure dynamically generated during a search of BWT(y). The average value of |T| is bounded by O(|Œ£|2k), where Œ£ is an alphabet from which we take symbols to make up target and pattern strings. This time complexity is better than previous strategies when k ‚â§ O(log|Œ£| n). The general working process consists of two steps. In the first step, x is decomposed into a series of l small subpatterns, and BWT(y) is utilized to speedup the process to figure out all the occurrences of such subpatterns with ‚åäk/l‚åã differences. In the second step, all the found occurrences in the first step will be rechecked to see whether they really match x, but with k differences. Extensive experiments have been conducted, which show that our method for this problem is promising.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447695®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download@©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/GaffneyCP21•titleŸ!Database Isolation By Scheduling.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461537©publisher±Proc. VLDB Endow.ßauthorsì∞Kevin P. GaffneyØRobert K. Claus∞Jignesh M. Patel®Abstract⁄ûTransaction isolation is conventionally achieved by restricting access to the physical items in a database. To maximize performance, isolation functionality is often packaged with recovery, I/O, and data access methods in a monolithic transactional storage manager. While this design has historically afforded high performance in online transaction processing systems, industry trends indicate a growing need for a new approach in which intertwined components of the transactional storage manager are disaggregated into modular services. This paper presents a new method to modularize the isolation component. Our work builds on predicate locking, an isolation mechanism that enables this modularization by locking logical rather than physical items in a database. Predicate locking is rarely used as the core isolation mechanism because of its high theoretical complexity and perceived overhead. However, we show that this overhead can be substantially reduced in practice by optimizing for common predicate structures.We present DIBS, a transaction scheduler that employs our predicate locking optimizations to guarantee isolation as a modular service. We evaluate the performance of DIBS as the sole isolation mechanism in a data processing system. In this setting, DIBS scales up to 10.5 million transactions per second on a TATP workload. We also explore how DIBS can be applied to existing database systems to increase transaction throughput. DIBS reduces per-transaction file system writes by 90% on TATP in SQLite, resulting in a 3X improvement in throughput. Finally, DIBS reduces row contention on YCSB in MySQL, providing serializable isolation with a 1.4X improvement in throughput.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461537®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download?©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/Weikum21•titleŸ&Knowledge Graphs 2021: A Data Odyssey.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476393©publisher±Proc. VLDB Endow.ßauthorsëÆGerhard Weikum®Abstract⁄Providing machines with comprehensive knowledge of the world's entities and their relationships has been a long-standing vision and challenge for AI. Over the last 15 years, huge knowledge bases, also known as knowledge graphs, have been automatically constructed from web data, and have become a key asset for search engines and other use cases. Machine knowledge can be harnessed to semantically interpret texts in news, social media and web tables, contributing to question answering, natural language processing and data analytics. This position paper reviews these advances and discusses lessons learned. It highlights the role of "DB thinking" in building and maintaining high-quality knowledge bases from web contents. Moreover, the paper identifies open challenges and new research opportunities. In particular, extracting quantitative measures of entities (e.g., height of buildings or energy efficiency of cars), from text and web tables, presents an opportunity to further enhance the scope and value of knowledge bases.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476393®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download>©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/LiuLZR0L0PS21•titleŸSDemonstration of Dealer: An End-to-End Model Marketplace with Differential Privacy.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476335©publisher±Proc. VLDB Endow.ßauthorsô™Jinfei LiuÆQiongqiong Lin±Jiayao Zhang 0006¨Kui Ren 0001≠Jian Lou 0001©Junxu Liu≠Li Xiong 0001®Jian PeiØJimeng Sun 0001®Abstract⁄Data-driven machine learning (ML) has witnessed great success across a variety of application domains. Since ML model training relies on a large amount of data, there is a growing demand for high-quality data to be collected for ML model training. Data markets can be employed to significantly facilitate data collection. In this work, we demonstrate Dealer, an en&lt;u&gt;D&lt;/u&gt;-to-end mod&lt;u&gt;e&lt;/u&gt;l m&lt;u&gt;a&lt;/u&gt;rketp&lt;u&gt;l&lt;/u&gt;ace with diff&lt;u&gt;e&lt;/u&gt;rential p&lt;u&gt;r&lt;/u&gt;ivacy. Dealer consists of three entities, data owners, the broker, and model buyers. Data owners receive compensation for their data usages allocated by the broker; The broker collects data from data owners, builds and sells models to model buyers; Model buyers buy their target models from the broker. We demonstrate the functionalities of the three participating entities and the abbreviated interactions between them. The demonstration allows the audience to understand and experience interactively the process of model trading. The audience can act as a data owner to control what and how the data would be compensated, can act as a broker to price machine learning models with maximum revenue, as well as can act as a model buyer to purchase target models that meet expectations.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476335®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download>©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/0001CQF21•titleŸIFrom Papers to Practice: The openclean Open-Source Data Cleaning Library.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476339©publisher±Proc. VLDB Endow.ßauthorsî≤Heiko Mueller 0001≠Sonia Castelo≠Munaf A. QaziÆJuliana Freire®Abstract⁄Data preparation is still a major bottleneck for many data science projects. Even though many sophisticated algorithms and tools have been proposed in the research literature, it is difficult for practitioners to integrate them into their data wrangling efforts. We present openclean, a open-source Python library for data cleaning and profiling, openclean integrates data profiling and cleaning tools in a single environment that is easy and intuitive to use. We designed openclean to be extensible and make it easy to add new functionality. By doing so, it will not only become easier for users to access state-of-the-art algorithms for their data wrangling efforts, but also allow researchers to integrate their work and evaluate its effectiveness in practice. We envision openclean as a first step to build a community of practitioners and researchers in the field. In our demo, we outline the main components and design decisions in the development of openclean and demonstrate the current functionality of the library on real-world use cases.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476339®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download>©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/ShengHWB21•titleŸVPR-Sketch: Monitoring Per-key Aggregation of Streaming Data with Nearly Full Accuracy.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467868©publisher±Proc. VLDB Endow.ßauthorsî¨Siyuan ShengÆQun Huang 0001ßSa Wang´Yungang Bao®Abstract⁄ˇComputing per-key aggregation is indispensable in streaming data analysis formulated as two phases, an update phase and a recovery phase. As the size and speed of data streams rise, accurate per-key information is useful in many applications like anomaly detection, attack prevention, and online diagnosis. Even though many algorithms have been proposed for per-key aggregation in stream processing, their accuracy guarantees only cover a small portion of keys. In this paper, we aim to achieve nearly full accuracy with limited resource usage. We follow the line of sketch-based techniques. We observe that existing methods suffer from high errors for most keys. The reason is that they track keys by complicated mechanism in the update phase and simply calculate per-key aggregation from some specific counter in the recovery phase. Therefore, we present PR-Sketch, a novel sketching design to address the two limitations. PR-Sketch builds linear equations between counter values and per-key aggregations to improve accuracy, and records keys in the recovery phase to reduce resource usage in the update phase. We also provide an extension called fast PR-Sketch to improve processing rate further. We derive space complexity, time complexity, and guaranteed error probability for both PR-Sketch and fast PR-Sketch. We conduct trace-driven experiments under 100K keys and 1M items to compare our algorithms with multiple state-of-the-art methods. Results demonstrate the resource efficiency and nearly full accuracy of our algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467868®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download=©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/JinYYSHX21•titleŸuUnconstrained Submodular Maximization with Modular Costs: Tight Approximation and Application to Profit Maximization.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467866©publisher±Proc. VLDB Endow.ßauthorsñ¨Tianyuan Jin¨Yu Yang 0001´Renchi Yang´Jieming Shi™Keke Huang¨Xiaokui Xiao®Abstract⁄€Given a set V, the problem of unconstrained submodular maximization with modular costs (USM-MC) asks for a subset S ‚äÜ V that maximizes f(S) - c(S), where f is a non-negative, monotone, and submodular function that gauges the utility of S, and c is a non-negative and modular function that measures the cost of S. This problem finds applications in numerous practical scenarios, such as profit maximization in viral marketing on social media.This paper presents ROI-Greedy, a polynomial time algorithm for USM-MC that returns a solution S satisfying [EQUATION], where S* is the optimal solution to USM-MC. To our knowledge, ROI-Greedy is the first algorithm that provides such a strong approximation guarantee. In addition, we show that this worst-case guarantee is tight, in the sense that no polynomial time algorithm can ensure [EQUATION], for any œµ &gt; 0. Further, we devise a non-trivial extension of ROI-Greedy to solve the profit maximization problem, where the precise value of f(S) for any set S is unknown and can only be approximated via sampling. Extensive experiments on benchmark datasets demonstrate that ROI-Greedy significantly outperforms competing methods in terms of the tradeoff between efficiency and solution quality.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467866®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download;©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/FangPCDG21•titleŸ\MDTP: A Multi-source Deep Traffic Prediction Framework over Spatio-Temporal Trajectory Data.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457394©publisher±Proc. VLDB Endow.ßauthorsï´Ziquan Fang¶Lu Pan¨Lu Chen 0001ÆYuntao Du 0002™Yunjun Gao®Abstract⁄YTraffic prediction has drawn increasing attention for its ubiquitous real-life applications in traffic management, urban computing, public safety, and so on. Recently, the availability of massive trajectory data and the success of deep learning motivate a plethora of deep traffic prediction studies. However, the existing neural-network-based approaches tend to ignore the correlations between multiple types of moving objects located in the same spatio-temporal traffic area, which is suboptimal for traffic prediction analytics.In this paper, we propose a multi-source deep traffic prediction framework over spatio-temporal trajectory data, termed as MDTP. The framework includes two phases: spatio-temporal feature modeling and multi-source bridging. We present an enhanced graph convolutional network (GCN) model combined with long short-term memory network (LSTM) to capture the spatial dependencies and temporal dynamics of traffic in the feature modeling phase. In the multi-source bridging phase, we propose two methods, Sum and Concat, to connect the learned features from different trajectory data sources. Extensive experiments on two real-life datasets show that MDTP i) has superior efficiency, compared with classical time-series methods, machine learning methods, and state-of-the-art neural-network-based approaches; ii) offers a significant performance improvement over the single-source traffic prediction approach; and iii) performs traffic predictions in seconds even on tens of millions of trajectory data. we develop MDTP+, a user-friendly interactive system to demonstrate traffic prediction analysis.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457394®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download;©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/GaleM20•titleΩExplaining Ranking Functions.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436922©publisher±Proc. VLDB Endow.ßauthorsí¨Abraham GaleÆAm√©lie Marian®Abstract⁄lRanking functions are commonly used to assist in decision-making in a wide variety of applications. As the general public realizes the significant societal impacts of the widespread use of algorithms in decision-making, there has been a push towards explainability and transparency in decision processes and results, as well as demands to justify the fairness of the processes. In this paper, we focus on providing metrics towards explainability and transparency of ranking functions, with a focus towards making the ranking process understandable, a priori, so that decision-makers can make informed choices when designing their ranking selection process. We propose transparent participation metrics to clarify the ranking process, by assessing the contribution of each parameter used in the ranking function in the creation of the final ranked outcome, using information about the ranking functions themselves, as well as observations of the underlying distributions of the parameter values involved in the ranking. To evaluate the outcome of the ranking process, we propose diversity and disparity metrics to measure how similar the selected objects are to each other, and to the underlying data distribution. We evaluate the behavior of our metrics on synthetic data, as well as on data and ranking functions on two real-world scenarios: high school admissions and decathlon scoring.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436922®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download:©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/LemaitreKHQM21•titleŸXIn the Land of Data Streams where Synopses are Missing, One Framework to Bring Them All.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467871©publisher±Proc. VLDB Endow.ßauthorsïµRudi Poepsel Lemaitre≠Martin KieferØJoscha Von Hein∫Jorge-Arnulfo Quian√©-Ruiz¨Volker Markl®Abstract⁄In pursuit of real-time data analysis, approximate summarization structures, i.e., synopses, have gained importance over the years. However, existing stream processing systems, such as Flink, Spark, and Storm, do not support synopses as first class citizens, i.e., as pipeline operators. Synopses' implementation is upon users. This is mainly because of the diversity of synopses, which makes a unified implementation difficult. We present Condor, a framework that supports synopses as first class citizens. Condor facilitates the specification and processing of synopsis-based streaming jobs while hiding all internal processing details. Condor's key component is its model that represents synopses as a particular case of windowed aggregate functions. An inherent divide and conquer strategy allows Condor to efficiently distribute the computation, allowing for high-performance and linear scalability. Our evaluation shows that Condor outperforms existing approaches by up to a factor of 75x and that it scales linearly with the number of cores.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467871®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download9©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/UstaKU21•titleŸkDBTagger: Multi-Task Learning for Keyword Mapping in NLIDBs Using Bi-Directional Recurrent Neural Networks.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446103©publisher±Proc. VLDB Endow.ßauthorsì©Arif Usta≤Akifhan KarakayaliÆ√ñzg√ºr Ulusoy®Abstract⁄)Translating Natural Language Queries (NLQs) to Structured Query Language (SQL) in interfaces deployed in relational databases is a challenging task, which has been widely studied in database community recently. Conventional rule based systems utilize series of solutions as a pipeline to deal with each step of this task, namely stop word filtering, tokenization, stemming/lemmatization, parsing, tagging, and translation. Recent works have mostly focused on the translation step overlooking the earlier steps by using adhoc solutions. In the pipeline, one of the most critical and challenging problems is keyword mapping; constructing a mapping between tokens in the query and relational database elements (tables, attributes, values, etc.). We define the keyword mapping problem as a sequence tagging problem, and propose a novel deep learning based supervised approach that utilizes POS tags of NLQs. Our proposed approach, called DBTagger (DataBase Tagger), is an end-to-end and schema independent solution, which makes it practical for various relational databases. We evaluate our approach on eight different datasets, and report new state-of-the-art accuracy results, 92.4% on the average. Our results also indicate that DBTagger is faster than its counterparts up to 10000 times and scalable for bigger databases.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446103®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download8©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/KoutrasPSIFBK21•titleŸ4Valentine in Action: Matching Tabular Data at Scale.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476366©publisher±Proc. VLDB Endow.ßauthorsó∞Christos Koutras±Kyriakos Psarakis∞George Siachamis≠Andra Ionescu±Marios FragkoulisØAngela BonifatiµAsterios Katsifodimos®Abstract⁄FCapturing relationships among heterogeneous datasets in large data lakes - traditionally termed schema matching - is one of the most challenging problems that corporations and institutions face nowadays. Discovering and integrating datasets heavily relies on the effectiveness of the schema matching methods in use. However, despite the wealth of research, evaluation of schema matching methods is still a daunting task: there is a lack of openly-available datasets with ground truth, reference method implementations, and comprehensible GUIs that would facilitate development of both novel state-of-the-art schema matching techniques and novel data discovery methods.Our recently proposed Valentine is the first system to offer an open-source experiment suite to organize, execute and orchestrate large-scale matching experiments. In this demonstration we present its functionalities and enhancements: i) a scalable system, with a user-centric GUI, that enables the fabrication of datasets and the evaluation of matching methods on schema matching scenarios tailored to the scope of tabular dataset discovery, ii) a scalable holistic matching system that can receive tabular datasets from heterogeneous sources and provide with similarity scores among their columns, in order to facilitate modern procedures in data lakes, such as dataset discovery.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476366®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download8©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/CunninghamCFS21•titleŸ>Real-World Trajectory Sharing with Local Differential Privacy.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476280©publisher±Proc. VLDB Endow.ßauthorsî∞Teddy CunninghamÆGraham CormodeµHakan Ferhatosmanoglu±Divesh Srivastava®Abstract⁄xSharing trajectories is beneficial for many real-world applications, such as managing disease spread through contact tracing and tailoring public services to a population's travel patterns. However, public concern over privacy and data protection has limited the extent to which this data is shared. Local differential privacy enables data sharing in which users share a perturbed version of their data, but existing mechanisms fail to incorporate user-independent public knowledge (e.g., business locations and opening times, public transport schedules, geo-located tweets). This limitation makes mechanisms too restrictive, gives unrealistic outputs, and ultimately leads to low practical utility. To address these concerns, we propose a local differentially private mechanism that is based on perturbing hierarchically-structured, overlapping n-grams (i.e., contiguous subsequences of length n) of trajectory data. Our mechanism uses a multi-dimensional hierarchy over publicly available external knowledge of real-world places of interest to improve the realism and utility of the perturbed, shared trajectories. Importantly, including real-world public data does not negatively affect privacy or efficiency. Our experiments, using real-world data and a range of queries, each with real-world application analogues, demonstrate the superiority of our approach over a range of alternative methods.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476280®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download7©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/XuBFLLLQWWYZ21•titleŸ5GraphScope: A One-Stop Large Graph Processing System.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476324©publisher±Proc. VLDB Endow.ßauthorsõ©Jingbo Xu¨Zhanning Bai™Wenfei Fan´Longbin Lai¶Xue LißZhao LiÆZhengping Qian®Lei Wang´Yanyan Wang™Wenyuan Yu¨Jingren Zhou®Abstract⁄@Due to diverse graph data and algorithms, programming and orchestration of complex computation pipelines have become the major challenges to making use of graph applications for Web-scale data analysis. GraphScope aims to provide a one-stop and efficient solution for a wide range of graph computations at scale. It extends previous systems by offering a unified and high-level programming interface and allowing the seamless integration of specialized graph engines in a general data-parallel computing environment.As we will show in this demo, GraphScope enables developers to write sequential graph programs in Python and provides automatic parallel execution on a cluster. This further allows GraphScope to seamlessly integrate with existing data processing systems in PyData ecosystem. To validate GraphScope's efficiency, we will compare a complex, multi-staged processing pipeline for a real-life fraud detection task with a manually assembled implementation comprising multiple systems. GraphScope achieves a 2.86√ó speedup on a trillion-scale graph in real production at Alibaba.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476324®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download7©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/ShangZBEKMRK21•titleŸ<Davos: A System for Interactive Data-Driven Decision Making.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476370©publisher±Proc. VLDB Endow.ßauthorsò¨Zeyuan Shang∞Emanuel Zgraggen±Benedetto Buratti∞Philipp Eichmann±Navid Karimeddiny≠Charlie MeyerÆWesley Runnels™Tim Kraska®Abstract⁄WRecently, a new horizon in data analytics, prescriptive analytics, is becoming more and more important to make data-driven decisions. As opposed to the progress of democratizing data acquisition and access, making data-driven decisions remains a significant challenge for people without technical expertise. In this regard, existing tools for data analytics which were designed decades ago still present a high bar for domain experts, and removing this bar requires a fundamental rethinking of both interface and backend.At Einblick, an MIT/Brown spin-off based on the Northstar project, we have been building the next generation analytics tool in the last few years. To overcome the shortcomings of existing processing engines, we propose Davos, Einblick's novel backend. Davos combines aspects of progressive computation, approximate query processing and sampling, with a specific focus on supporting user-defined operations. Moreover, Davos optimizes multi-tenant scenarios to promote collaboration. Both empirical evaluation and user study verify that Davos can greatly empower data analytics for new needs.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476370®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download5©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/HernandezGH21•titleŸ@Computing How-Provenance for SPARQL Queries via Query Rewriting.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484235©publisher±Proc. VLDB Endow.ßauthorsì∂Daniel Hern√°ndez 0002ØLuis Gal√°rraga™Katja Hose®Abstract⁄5Over the past few years, we have witnessed the emergence of large knowledge graphs built by extracting and combining information from multiple sources. This has propelled many advances in query processing over knowledge graphs, however the aspect of providing provenance explanations for query results has so far been mostly neglected. We therefore propose a novel method, SPARQLprov, based on query rewriting, to compute how-provenance polynomials for SPARQL queries over knowledge graphs. Contrary to existing works, SPARQLprov is system-agnostic and can be applied to standard and already deployed SPARQL engines without the need of customized extensions. We rely on spm-semirings to compute polynomial annotations that respect the property of commutation with homomorphisms on monotonic and non-monotonic SPARQL queries without aggregate functions. Our evaluation on real and synthetic data shows that SPARQLprov over standard engines incurs an acceptable runtime overhead w.r.t. the original query, competing with state-of-the-art solutions for how-provenance computation.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484235®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download5©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/BharadwajGBG21•titleŸ"Discovering Related Data At Scale.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457403©publisher±Proc. VLDB Endow.ßauthorsîØSagar Bharadwaj≠Praveen GuptaØRanjita Bhagwan∞Saikat Guha 0002®Abstract⁄AAnalysts frequently require data from multiple sources for their tasks, but finding these sources is challenging in exabyte-scale data lakes. In this paper, we address this problem for our enterprise's data lake by using machine-learning to identify related data sources. Leveraging queries made to the data lake over a month, we build a relevance model that determines whether two columns across two data streams are related or not. We then use the model to find relations at scale across tens of millions of column-pairs and thereafter construct a data relationship graph in a scalable fashion, processing a data lake that has 4.5 Petabytes of data in approximately 80 minutes. Using manually labeled datasets as ground-truth, we show that our techniques show improvements of at least 23% when compared to state-of-the-art methods.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457403®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download4©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/Zhang0MR21•titleŸ'LANCET: Labeling Complex Data at Scale.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476269©publisher±Proc. VLDB Endow.ßauthorsî´Huayi Zhang¨Lei Cao 0004≠Samuel MaddenµElke A. Rundensteiner®Abstract⁄˙Cutting-edge machine learning techniques often require millions of labeled data objects to train a robust model. Because relying on humans to supply such a huge number of labels is rarely practical, automated methods for label generation are needed. Unfortunately, critical challenges in auto-labeling remain unsolved, including the following research questions: (1) which objects to ask humans to label, (2) how to automatically propagate labels to other objects, and (3) when to stop labeling. These three questions are not only each challenging in their own right, but they also correspond to tightly interdependent problems. Yet existing techniques provide at best isolated solutions to a subset of these challenges. In this work, we propose the first approach, called LANCET, that successfully addresses all three challenges in an integrated framework. LANCET is based on a theoretical foundation characterizing the properties that the labeled dataset must satisfy to train an effective prediction model, namely the Covariate-shift and the Continuity conditions. First, guided by the Covariate-shift condition, LANCET maps raw input data into a semantic feature space, where an unlabeled object is expected to share the same label with its near-by labeled neighbor. Next, guided by the Continuity condition, LANCET selects objects for labeling, aiming to ensure that unlabeled objects always have some sufficiently close labeled neighbors. These two strategies jointly maximize the accuracy of the automatically produced labels and the prediction accuracy of the machine learning models trained on these labels. Lastly, LANCET uses a distribution matching network to verify whether both the Covariate-shift and Continuity conditions hold, in which case it would be safe to terminate the labeling process. Our experiments on diverse public data sets demonstrate that LANCET consistently outperforms the state-of-the-art methods from Snuba to GOGGLES and other baselines by a large margin - up to 30 percentage points increase in accuracy.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476269®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download3©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/YanDGZWJX21•titleŸXFlashP: An Analytical Pipeline for Real-time Forecasting of Time-Series Relational Data.§year§2021£doiŸ(https://doi.org/10.14778/3446095.3446096©publisher±Proc. VLDB Endow.ßauthorsó´Shuyuan Yan™Bolin DingßWei Guo¨Jingren Zhou™Zhewei Wei≠Xiaowei Jiang®Sheng Xu®Abstract⁄‰Interactive response time is important in analytical pipelines for users to explore a sufficient number of possibilities and make informed business decisions. We consider a forecasting pipeline with large volumes of high-dimensional time series data. Real-time forecasting can be conducted in two steps. First, we specify the part of data to be focused on and the measure to be predicted by slicing, dicing, and aggregating the data. Second, a forecasting model is trained on the aggregated results to predict the trend of the specified measure. While there are a number of forecasting models available, the first step is the performance bottleneck. A natural idea is to utilize sampling to obtain approximate aggregations in real time as the input to train the forecasting model. Our scalable real-time forecasting system FlashP (Flash Prediction) is built based on this idea, with two major challenges to be resolved in this paper: first, we need to figure out how approximate aggregations affect the fitting of forecasting models, and forecasting results; and second, accordingly, what sampling algorithms we should use to obtain these approximate aggregations and how large the samples are. We introduce a new sampling scheme, called GSW sampling, and analyze error bounds for estimating aggregations using GSW samples. We introduce how to construct compact GSW samples with the existence of multiple measures to be analyzed. We conduct experiments to evaluate our solution its alternatives on real data.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3446095.3446096®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download0©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/WangMGXO21•titleŸ=MP-RW-LSH: An Efficient Multi-Probe LSH Solution to ANNS-L_1.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484226©publisher±Proc. VLDB Endow.ßauthorsï™Huayi Wang¨Jingfan Meng©Long Gong´Jun Xu 0014±Mitsunori Ogihara®Abstract⁄NApproximate Nearest Neighbor Search (ANNS) is a fundamental algorithmic problem, with numerous applications in many areas of computer science. Locality-Sensitive Hashing (LSH) is one of the most popular solution approaches for ANNS. A common shortcoming of many LSH schemes is that since they probe only a single bucket in a hash table, they need to use a large number of hash tables to achieve a high query accuracy. For ANNS-L2, a multi-probe scheme was proposed to overcome this drawback by strategically probing multiple buckets in a hash table. In this work, we propose MP-RW-LSH, the first and so far only multi-probe LSH solution to ANNS in L1 distance, and show that it achieves a better tradeoff between scalability and query efficiency than all existing LSH-based solutions. We also explain why a state-of-the-art ANNS-L1 solution called Cauchy projection LSH (CP-LSH) is fundamentally not suitable for multi-probe extension. Finally, as a use case, we construct, using MP-RW-LSH as the underlying "ANNS-L1 engine", a new ANNS-E (E for edit distance) solution that beats the state of the art.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484226®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download/©PaperRefsêã§Infoá§typeßarticle£keyøjournals/pvldb/KoutsoukosMMKA21•titleŸQModularis: Modular Relational Analytics over Heterogeneous Distributed Platforms.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484229©publisher±Proc. VLDB Endow.ßauthorsï¥Dimitrios Koutsoukos±Ingo M√ºller 0002±Renato Marroqu√≠n¨Ana KlimovicÆGustavo Alonso®Abstract⁄¶The enormous quantity of data produced every day together with advances in data analytics has led to a proliferation of data management and analysis systems. Typically, these systems are built around highly specialized monolithic operators optimized for the underlying hardware. While effective in the short term, such an approach makes the operators cumbersome to port and adapt, which is increasingly required due to the speed at which algorithms and hardware evolve. To address this limitation, we present Modularis, an execution layer for data analytics based on sub-operators, i.e., composable building blocks resembling traditional database operators but at a finer granularity. To demonstrate the feasibility and advantages of our approach, we use Modularis to build a distributed query processing system supporting relational queries running on an RDMA cluster, a serverless cloud platform, and a smart storage engine. Modularis requires minimal code changes to execute queries across these three diverse hardware platforms, showing that the sub-operator approach reduces the amount and complexity of the code to maintain. In fact, changes in the platform affect only those sub-operators that depend on the underlying hardware (in our use cases, mainly the sub-operators related to network communication). We show the end-to-end performance of Modularis by comparing it with a framework for SQL processing (Presto), a commercial cluster database (SingleStore), as well as Query-as-a-Service systems (Athena, BigQuery). Modularis outperforms all these systems, proving that the design and architectural advantages of a modular design can be achieved without degrading performance. We also compare Modularis with a hand-optimized implementation of a join for RDMA clusters. We show that Modularis has the advantage of being easily extensible to a wider range of join variants and group by queries, all of which are not supported in the hand-tuned join.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484229®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download/©PaperRefsêã§Infoá§typeßarticle£keyøjournals/pvldb/CarlssonXMWPRV21•titleŸ@Demonstration of Marius: Graph Embeddings with a Single Machine.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476338©publisher±Proc. VLDB Endow.ßauthorsóØAnders Carlsson®Anze Xie≠Jason Mohoney≠Roger Waleffe≠Shanan Peters¥Theodoros RekatsinasµShivaram Venkataraman®Abstract⁄;Graph embeddings have emerged as the de facto representation for modern machine learning over graph data structures. The goal of graph embedding models is to convert high-dimensional sparse graphs into low-dimensional, dense and continuous vector spaces that preserve the graph structure properties. However, learning a graph embedding model is a resource intensive process, and existing solutions rely on expensive distributed computation to scale training to instances that do not fit in GPU memory. This demonstration showcases Marius: a new open-source engine for learning graph embedding models over billion-edge graphs on a single machine. Marius is built around a recently-introduced architecture for machine learning over graphs that utilizes pipelining and a novel data replacement policy to maximize GPU utilization and exploit the entire memory hierarchy (including disk, CPU, and GPU memory) to scale to large instances. The audience will experience how to develop, train, and deploy graph embedding models using Marius' configuration-driven programming model. Moreover, the audience will have the opportunity to explore Marius' deployments on applications including link-prediction on WikiKG90M and reasoning queries on a paleobiology knowledge graph. Marius is available as open source software at https://marius-project.org.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476338®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download.©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/WangLMHYFX21•titleŸcAutoGR: Automated Geo-Replication with Fast System Performance and Preserved Application Semantics.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461541©publisher±Proc. VLDB Endow.ßauthorsó´Jiawei Wang≠Cheng Li 0001¶Kai Ma™Jingze Huo≠Feng Yan 0001ØXinyu Feng 0001™Yinlong Xu®Abstract⁄JGeo-replication is essential for providing low latency response and quality Internet services. However, designing fast and correct geo-replicated services is challenging due to the complex trade-off between performance and consistency semantics in optimizing the expensive cross-site coordination. State-of-the-art solutions rely on programmers to derive sufficient application-specific invariants and code specifications, which is both time-consuming and error-prone. In this paper, we propose an end-to-end geo-replication deployment framework AUTOGR (AUTOmated Geo-Replication) to free programmers from such label-intensive tasks. AutoGR enables the geo-replication features for non-replicated, serializable applications in an automated way with optimized performance and correct application semantics. Driven by a novel static analyzer RIGI, AUTOGR can extract application invariants by verifying whether their geo-replicated versions obey the serializable semantics of the non-replicated application. RIGI takes application codes as inputs and infers a set of side effects and path conditions possibly leading to consistency violations. RIGI employs the Z3 theorem prover to identify pairs of conflicting side effects and feed them to a geo-replication framework for automated across-site deployment. We evaluate AUTOGR by transforming four serializable and originally non-replicated DB-compliant applications to geo-replicated ones across 3 sites. Compared with state-of-the-art human-intervention-free automated approaches (e.g., strong consistency), AUTOGR reduces up to 61.8% latency and achieves up to 2.12X higher peak throughput. Compared with state-of-the-art approaches relying on a manual analysis (e.g., PoR), AUTOGR can quickly enable the geo-replication feature with zero human intervention while offering similarly low latency and high throughput.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461541®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download-©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/FunkeT21•titleŸ7Low-Latency Compilation of SQL Queries to Machine Code.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476321©publisher±Proc. VLDB Endow.ßauthorsí≠Henning Funke¨Jens Teubner®Abstract⁄SQuery compilation has proven to be one of the most efficient query processing techniques. Despite its fast processing speed, the additional compilation times of the technique limit its applicability. This is because the approach is most beneficial only when the improvements in processing time clearly exceed the additional compilation time.Recently the feasibility of query compilers with very low compilation times has been shown. This may prove query compilation as a merely universal approach. In this article and in the corresponding live demo, we show the capabilities of the ReSQL database system, which uses the intermediate representation Flounder IR to achieve very low compilation times. ReSQL reduces the compilation times from SQL to machine code compared to existing LLVM-based techniques by up to 101.1x for real-world analytic queries.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476321®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download,©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/WoltmannOHHL21•titleŸMPostCENN: PostgreSQL with Machine Learning Models for Cardinality Estimation.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476327©publisher±Proc. VLDB Endow.ßauthorsïÆLucas Woltmann≠Dominik Olwig∞Claudio Hartmann´Dirk HabichØWolfgang Lehner®Abstract⁄In this demo, we present PostCENN, an enhanced PostgreSQL database system with an end-to-end integration of machine learning (ML) models for cardinality estimation. In general, cardinality estimation is a topic with a long history in the database community. While traditional models like histograms are extensively used, recent works mainly focus on developing new approaches using ML models. However, traditional as well as ML models have their own advantages and disadvantages. With PostCENN, we aim to combine both to maximize their potentials for cardinality estimation by introducing ML models as a novel means to increase the accuracy of the cardinality estimation for certain parts of the database schema. To achieve this, we integrate ML models as first class citizen in PostgreSQL with a well-defined end-to-end life cycle. This life cycle consists of creating ML models for different sub-parts of the database schema, triggering the training, using ML models within the query optimizer in a transparent way, and deleting ML models.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476327®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download,©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/MaiyyaNAA21•titleŸZErrata for "Unifying Consensus and Atomic Commitment for Effective Cloud Data Management".§year§2021£doiŸ(https://doi.org/10.14778/3450980.3450985©publisher±Proc. VLDB Endow.ßauthorsî≠Sujaya Maiyya¨Faisal Nawab¨Divy Agrawal≠Amr El Abbadi®Abstract⁄KThis errata article discusses and corrects a minor error in our work published in VLDB 2019. The discrepancy specifically pertains to Algorithms 3 and 4. The algorithms presented in the paper are biased towards a commit decision in a specific failure scenario. We explain the error using an example before correcting the algorithm.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3450980.3450985®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download+©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/MandamadiotisKE21•titleŸ:DatAgent: The Imminent Age of Intelligent Data Assistants.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476352©publisher±Proc. VLDB Endow.ßauthorsñµAntonis Mandamadiotis∞Georgia Koutrika∂Stavroula Eleftherakis∞Apostolis Glenis∂Dimitrios Skoutas 0001∞Yannis Stavrakas®Abstract⁄˘In this demonstration, we present DatAgent, an intelligent data assistant system that allows users to ask queries in natural language, and can respond in natural language as well. Moreover, the system actively guides the user using different types of recommendations and hints, and learns from user actions. We will demonstrate different exploration scenarios that show how the system and the user engage in a human-like interaction inspired by the interaction paradigm of chatbots and virtual assistants.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476352®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download*©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/Koutra21•titleŸmThe Power of Summarization in Graph Mining and Learning: Smaller Data, Faster Methods, More Interpretability.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484238©publisher±Proc. VLDB Endow.ßauthorsë¨Danai Koutra®Abstract⁄dOur ability to generate, collect, and archive data related to everyday activities, such as interacting on social media, browsing the web, and monitoring well-being, is rapidly increasing. Getting the most benefit from this large-scale data requires analysis of patterns it contains, which is computationally intensive or even intractable. Summarization techniques produce compact data representations (summaries) that enable faster processing by complex algorithms and queries.This talk will cover summarization of interconnected data (graphs) [3], which can represent a variety of natural processes (e.g., friendships, communication). I will present an overview of my group's work on bridging the gap between research on summarized network representations and real-world problems. Examples include summarization of massive knowledge graphs for refinement [2] and on-device querying [4], summarization of graph streams for persistent activity detection [1], and summarization within graph neural networks for fast, interpretable classification [5]. I will conclude with open challenges and opportunities for future research.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484238®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download*©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/TsamouraCMU21•titleŸ1Materializing Knowledge Bases via Trigger Graphs.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447699©publisher±Proc. VLDB Endow.ßauthorsî±Efthymia Tsamoura¨David CarralÆEnrico Malizia≠Jacopo Urbani®Abstract⁄°The chase is a well-established family of algorithms used to materialize Knowledge Bases (KBs) for tasks like query answering under dependencies or data cleaning. A general problem of chase algorithms is that they might perform redundant computations. To counter this problem, we introduce the notion of Trigger Graphs (TGs), which guide the execution of the rules avoiding redundant computations. We present the results of an extensive theoretical and empirical study that seeks to answer when and how TGs can be computed and what are the benefits of TGs when applied over real-world KBs. Our results include introducing algorithms that compute (minimal) TGs. We implemented our approach in a new engine, called GLog, and our experiments show that it can be significantly more efficient than the chase enabling us to materialize Knowledge Graphs with 17B facts in less than 40 min using a single machine with commodity hardware.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447699®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download*©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/CaoLLR21•titleŸ5Cryptanalysis of An Encrypted Database in SIGMOD '14.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467865©publisher±Proc. VLDB Endow.ßauthorsî©Xinle Cao≠Jian Liu 0012¶Hao Lu¨Kui Ren 0001®Abstract⁄Encrypted database is an innovative technology proposed to solve the data confidentiality issue in cloud-based DB systems. It allows a data owner to encrypt its database before uploading it to the service provider; and it allows the service provider to execute SQL queries over the encrypted data. Most of existing encrypted databases (e.g., CryptDB in SOSP '11) do not support data interoperability: unable to process complex queries that require piping the output of one operation to another.To the best of our knowledge, SDB (SIGMOD '14) is the only encrypted database that achieves data interoperability. Unfortunately, we found SDB is not secure! In this paper, we revisit the security of SDB and propose a ciphertext-only attack named co-prime attack. It successfully attacks the common operations supported by SDB, including addition, comparison, sum, equi-join and group-by. We evaluate our attack in three real-world benchmarks. For columns that support addition and comparison, we recover 84.9% -- 99.9% plaintexts. For columns that support sum, equi-join and group-by, we recover 100% plaintexts.Besides, we provide potential countermeasures that can prevent the attacks against sum, equi-join, group-by and addition. It is still an open problem to prevent the attack against comparison.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467865®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download)©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/BeedkarBQM21•titleŸ4Compliant Geo-distributed Data Processing in Action.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476359©publisher±Proc. VLDB Endow.ßauthorsî∞Kaustubh BeedkarØDavid Brekardin∫Jorge-Arnulfo Quian√©-Ruiz¨Volker Markl®Abstract⁄DIn this paper we present our work on compliant geo-distributed data processing. Our work focuses on the new dimension of dataflow constraints that regulate the movement of data across geographical or institutional borders. For example, European directives may regulate transferring only certain information fields (such as non personal information) or aggregated data. Thus, it is crucial for distributed data processing frameworks to consider compliance with respect to dataflow constraints derived from these regulations. We have developed a compliance-based data processing framework, which (i) allows for the declarative specification of dataflow constraints, (ii) determines if a query can be translated into a compliant distributed query execution plan, and (iii) executes the compliant plan over distributed SQL databases. We demonstrate our framework using a geo-distributed adaptation of the TPC-H benchmark data. Our framework provides an interactive dashboard, which allows users to specify dataflow constraints, and analyze and execute compliant distributed query execution plans.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476359®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download)©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/SuzukiHKSNS21•titleŸúApproaching DRAM performance by using microsecond-latency flash memory for small-sized random read accesses: a new access method and its graph applications.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457397©publisher±Proc. VLDB Endow.ßauthorsñ≠Tomoya SuzukiØKazuhiro Hiwada≤Hirotsugu Kajihara≠Shintaro Sano¨Shuou NomuraØTatsuo Shiozawa®Abstract⁄]For applications in which small-sized random accesses frequently occur for datasets that exceed DRAM capacity, placing the datasets on SSD can result in poor application performance. For the read-intensive case we focus on in this paper, low latency flash memory with microsecond read latency is a promising solution. However, when they are used in large numbers to achieve high IOPS (Input/Output operations Per Second), the CPU processing involved in IO requests is an overhead. To tackle the problem, we propose a new access method combining two approaches: 1) optimizing issuance and completion of the IO requests to reduce the CPU overhead. 2) utilizing many contexts with lightweight context switches by stackless coroutines. These reduce the CPU overhead per request to less than 10 ns, enabling read access with DRAM-like overhead, while the access latency longer than DRAM can be hidden by the context switches. We apply the proposed method to graph algorithms such as BFS (Breadth First Search), which involves many small-sized random read accesses. In our evaluation, the large graph data is placed on microsecond-latency flash memories within prototype boards, and it is accessed by the proposed method. As a result, for the synthetic and real-world graphs, the execution times of the graph algorithms are 88--141% of those when all the data are placed in DRAM.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457397®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download)©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/CetorelliACM21•titleŸ The Smallest Extraction Problem.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476293©publisher±Proc. VLDB Endow.ßauthorsî±Valerio Cetorelli¨Paolo Atzeni∞Valter Crescenzi∞Franco Milicchio®Abstract⁄ºWe introduce landmark grammars, a new family of context-free grammars aimed at describing the HTML source code of pages published by large and templated websites and therefore at effectively tackling Web data extraction problems. Indeed, they address the inherent ambiguity of HTML, one of the main challenges of Web data extraction, which, despite over twenty years of research, has been largely neglected by the approaches presented in literature.We then formalize the Smallest Extraction Problem (SEP), an optimization problem for finding the grammar of a family that best describes a set of pages and contextually extract their data.Finally, we present an unsupervised learning algorithm to induce a landmark grammar from a set of pages sharing a common HTML template, and we present an automatic Web data extraction system. The experiments on consolidated benchmarks show that the approach can substantially contribute to improve the state-of-the-art.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476293®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download)©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/Sinthong021•titleŸEPolyFrame: A Retargetable Query-based Approach to Scaling Dataframes.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476281©publisher±Proc. VLDB Endow.ßauthorsí≤Phanwadee SinthongµMichael J. Carey 0001®Abstract⁄îIn the last few years, the field of data science has been growing rapidly as various businesses have adopted statistical and machine learning techniques to empower their decision-making and applications. Scaling data analyses to large volumes of data requires the utilization of distributed frameworks. This can lead to serious technical challenges for data analysts and reduce their productivity. AFrame, a data analytics library, is implemented as a layer on top of Apache AsterixDB, addressing these issues by providing the data scientists' familiar interface, Pandas Dataframe, and transparently scaling out the evaluation of analytical operations through a Big Data management system. While AFrame is able to leverage data management facilities (e.g., indexes and query optimization) and allows users to interact with a large volume of data, the initial version only generated SQL++ queries and only operated against AsterixDB. In this work, we describe a new design that retargets AFrame's incremental query formation to other query-based database systems, making it more flexible for deployment against other data management systems with composable query languages.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476281®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download(©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/WangZA000Z21•titleŸFCquirrel: Continuous Query Processing over Acyclic Relational Schemas.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476315©publisher±Proc. VLDB Endow.ßauthorsó∞Qichen Wang 0001±Chaoqi Zhang 0002ÆDanish Alsayed™Ke Yi 0001´Bin Wu 0003ÆFeifei Li 0001¨Chaoqun Zhan®Abstract⁄àWe will demonstrate Cquirrel, a continuous query processing engine built on top of Flink. Cquirrel assumes a relational schema where the foreign-key constraints form a directed acyclic graph, and supports any selection-projection-join-aggregation query where all join conditions are between a primary key and a foreign key. It allows arbitrary updates to any of the relations, and outputs the deltas in the query answers in real-time. It provides much better support for multi-way joins than the native join operator in Flink. Meanwhile, it offers better performance, scalability, and fault tolerance than other continuous query processing engines.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476315®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download(©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/LiN021•titleŸNIntermittent Human-in-the-Loop Model Selection using Cerebro: A Demonstration.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476320©publisher±Proc. VLDB Endow.ßauthorsì™Liangde LiªSupun Chathuranga NakandalaØArun Kumar 0001®Abstract⁄æDeep learning (DL) is revolutionizing many fields. However, there is a major bottleneck for the wide adoption of DL: the pain of model selection, which requires exploring a large config space of model architecture and training hyper-parameters before picking the best model. The two existing popular paradigms for exploring this config space pose a false dichotomy. AutoML-based model selection explores configs with high-throughput but uses human intuition minimally. Alternatively, interactive human-in-the-loop model selection completely relies on human intuition to explore the config space but often has very low throughput. To mitigate the above drawbacks, we propose a new paradigm for model selection that we call intermittent human-in-the-loop model selection. In this demonstration, we will showcase our approach using five real-world DL model selection workloads. A short video of our demonstration can be found here: https://youtu.be/K3THQy5McXc.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476320®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download(©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/LouWGFCY21•title∑Time-Topology Analysis.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484230©publisher±Proc. VLDB Endow.ßauthorsñ™Yunkai Lou¨Chaokun Wang™Tiankai Gu®Hao Feng®Jun Chen≠Jeffrey Xu Yu®Abstract⁄ZMany real-world networks have been evolving, and are finely modeled as temporal graphs from the viewpoint of the graph theory. A temporal graph is informative, and always contains two types of information, i.e., the temporal information and topological information, where the temporal information reflects the time when the relationships are established, and the topological information focuses on the structure of the graph. In this paper, we perform time-topology analysis on temporal graphs to extract useful information. Firstly, a new metric named T-cohesiveness is proposed to evaluate the cohesiveness of a temporal subgraph. It defines the cohesiveness of a temporal subgraph from the time and topology dimensions jointly. Specifically, given a temporal graph Gs = (Vs, ŒµEs), cohesiveness in the time dimension reflects whether the connections in Gs happen in a short period of time, while cohesiveness in the topology dimension indicates whether the vertices in Vs are densely connected and have few connections with vertices out of Gs. Then, T-cohesiveness is utilized to perform time-topology analysis on temporal graphs, and two time-topology analysis methods are proposed. In detail, T-cohesiveness evolution tracking traces the evolution of the T-cohesiveness of a subgraph, and combo searching finds out all the subgraphs that contain the query vertex and have T-cohesiveness larger than a given threshold. Moreover, a pruning strategy is proposed to improve the efficiency of combo searching. Experimental results confirm the efficiency of the proposed time-topology analysis methods and the pruning strategy.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484230®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download'©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/JustoYSPK21•titleŸ/Towards A Polyglot Framework for Factorized ML.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476372©publisher±Proc. VLDB Endow.ßauthorsï´David Justo´Shaoqing Yi≠Lukas Stadler±Nadia Polikarpova™Arun Kumar®Abstract⁄∞Optimizing machine learning (ML) workloads on structured data is a key concern for data platforms. One class of optimizations called "factorized ML" helps reduce ML runtimes over multi-table datasets by pushing ML computations down through joins, avoiding the need to materialize such joins. The recent Morpheus system automated factorized ML to any ML algorithm expressible in linear algebra (LA). But all such prior factorized ML/LA stacks are restricted by their chosen programming language (PL) and runtime environment, limiting their reach in emerging industrial data science environments with many PLs (R, Python, etc.) and even cross-PL analytics workflows. Re-implementing Morpheus from scratch in each PL/environment is a massive developability overhead for implementation, testing, and maintenance. We tackle this challenge by proposing a new system architecture, Trinity, to enable factorized LA logic to be written only once and easily reused across many PLs/LA tools in one go. To do this in an extensible and efficient manner without costly data copies, Trinity leverages and extends an emerging industrial polyglot compiler and runtime, Oracle's GraalVM. Trinity enables factorized LA in multiple PLs and even cross-PL workflows. Experiments with real datasets show that Trinity is significantly faster than materialized execution (&gt; 8x speedups in some cases), while being largely competitive to a prior single PL-focused Morpheus stack.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476372®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download'©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/YanW21•titleŸCPath Advisor: A Multi-Functional Campus Map Tool for Shortest Path.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476319©publisher±Proc. VLDB Endow.ßauthorsí´Yinzhao YanµRaymond Chi-Wing Wong®Abstract⁄ØThe shortest path in both the two dimensional (2D) plane and the three dimensional (3D) terrain is extensively used both in industry and academia. Although there are some map visualization tools for viewing the shortest path in 2D and 3D views, we find two limitations: (1) they are not applicable for map applications with obstacles (such as the wall in a building), and (2) they look unrealistic and strange when a road network approach is blindly adopted. Motivated by this, we developed a web-based multi-functional campus map tool called Path Advisor, which allows users to visualize the shortest path in the 2D view, the bird's eye view and the virtual reality view (VR view). Path Advisor uses Dijkstra's shortest path algorithm and breadth-first tree in the 2D view, and the weighted shortest surface path algorithm in the bird's eye view and the VR view. We shot a video for demonstrating Path Advisor at https://youtu.be/ZgdjyXXHwqg.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476319®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download'©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/FariasBFMMS20•titleŸTLocal Dampening: Differential Privacy for Non-numeric Queries via Local Sensitivity.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436912©publisher±Proc. VLDB Endow.ßauthorsñ∂Victor A. E. de FariasØFelipe T. Brito¨Cheryl Flynn∞Javam C. Machado≥Subhabrata Majumdar±Divesh Srivastava®Abstract⁄˝Differential privacy is the state-of-the-art formal definition for data release under strong privacy guarantees. A variety of mechanisms have been proposed in the literature for releasing the noisy output of numeric queries (e.g., using the Laplace mechanism), based on the notions of global sensitivity and local sensitivity. However, although there has been some work on generic mechanisms for releasing the output of non-numeric queries using global sensitivity (e.g., the Exponential mechanism), the literature lacks generic mechanisms for releasing the output of non-numeric queries using local sensitivity to reduce the noise in the query output.In this work, we remedy this shortcoming and present the local dampening mechanism. We adapt the notion of local sensitivity for the non-numeric setting and leverage it to design a generic non-numeric mechanism. We illustrate the effectiveness of the local dampening mechanism by applying it to two diverse problems: (i) Influential node analysis. Given an influence metric, we release the top-k most influential nodes while preserving the privacy of the relationship between nodes in the network; (ii) Decision tree induction. We provide a private adaptation to the ID3 algorithm to build decision trees from a given tabular dataset. Experimental results show that we could reduce the use of privacy budget by 3 to 4 orders of magnitude for Influential node analysis and increase accuracy up to 12% for Decision tree induction when compared to global sensitivity based approaches.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436912®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download'©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/GongL00O020•titleŸWSpace- and Computationally-Efficient Set Reconciliation via Parity Bitmap Sketch (PBS).§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436906©publisher±Proc. VLDB Endow.ßauthorsñ©Long Gong™Ziheng LiuÆLiang Liu 0013´Jun Xu 0014±Mitsunori OgiharaÆTong Yang 0003®Abstract⁄`Set reconciliation is a fundamental algorithmic problem that arises in many networking, system, and database applications. In this problem, two large sets A and B of objects (bitcoins, files, records, etc.) are stored respectively at two different network-connected hosts, which we name Alice and Bob respectively. Alice and Bob communicate with each other to learn AŒîB, the difference between A and B, and as a result the reconciled set A‚à™B.Current set reconciliation schemes are based on either invertible Bloom filters (IBF) or error-correction codes (ECC). The former has a low computational complexity of O(d), where d is the cardinality of AŒîB, but has a high communication overhead that is several times larger than the theoretical minimum. The latter has a low communication overhead close to the theoretical minimum, but has a much higher computational complexity of O(d2). In this work, we propose Parity Bitmap Sketch (PBS), an ECC-based set reconciliation scheme that gets the better of both worlds: PBS has both a low computational complexity of O(d) just like IBF-based solutions and a low communication overhead of roughly twice the theoretical minimum. A separate contribution of this work is a novel rigorous analytical framework that can be used for the precise calculation of various performance metrics and for the near-optimal parameter tuning of PBS.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436906®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download&©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/LinGDCHB21•titleŸVDemonstration of Apperception: A Database Management System for Geospatial Video Data.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476340©publisher±Proc. VLDB Endow.ßauthorsñ´Vanessa Lin´Yongming Ge¨Maureen Daum¨Alvin CheungÆBrandon Haynes¥Magdalena Balazinska®Abstract⁄Many recent video applications---including traffic monitoring, drone analytics, autonomous driving, and virtual reality---require piecing together, combining, and operating over many related video streams. Despite the massive data volumes involved and the need to jointly reason (both spatially and temporally) about these videos, current techniques to store and manipulate such data are often limited to file systems and simple video processing frameworks that reason about a single video in isolation.We present Apperception, a new type of database management system optimized for geospatial video applications. Apperception comes with an easy to use data model to reason about multiple geospatial video data streams, and a programming interface for developers to collectively reason about the entities observed in those videos. Our demo will let users write queries over video using Apperception and retrieve (in real-time) both metadata and rendered video data. Users can also compare results and observe speedups achieved by using Apperception.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476340®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download%©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/BanerjeeL020•titleŸ;Maximizing Social Welfare in a Competitive Diffusion Model.§year§2020£doiŸ(https://doi.org/10.14778/3436905.3436920©publisher±Proc. VLDB Endow.ßauthorsìØPrithu BanerjeeµLaks V. S. Lakshmanan≠Wei Chen 0013®Abstract⁄:Influence maximization (IM) has garnered a lot of attention in the literature owing to applications such as viral marketing and infection containment. It aims to select a small number of seed users to adopt an item such that adoption propagates to a large number of users in the network. Competitive IM focuses on the propagation of competing items in the network. Existing works on competitive IM have several limitations. (1) They fail to incorporate economic incentives in users' decision making in item adoptions. (2) Majority of the works aim to maximize the adoption of one particular item, and ignore the collective role that different items play. (3) They focus mostly on one aspect of competition - pure competition. To address these concerns we study competitive IM under a utility-driven propagation model called UIC, and study social welfare maximization. The problem in general is not only NP-hard but also NP-hard to approximate within any constant factor. We, therefore, devise instant dependent efficient approximation algorithms for the general case as well as a (1 - 1/e - ‚àà)-approximation algorithm for a restricted setting. Our algorithms outperform different baselines on competitive IM, both in terms of solution quality and running time on large real networks under both synthetic and real utility configurations.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3436905.3436920®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download%©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/AlsaudiAMY21•titleŸQTQEL: Framework for Query-Driven Linking of Top-K Entities in Social Media Blogs.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476309©publisher±Proc. VLDB Endow.ßauthorsî≥Abdulrahman AlsaudiÆYasser AltowimØSharad Mehrotra©Yaming Yu®Abstract⁄åSocial media analysis over blogs (such as tweets) often requires determining top-k mentions of a certain category (e.g., movies) in a collection (e.g., tweets collected over a given day). Such queries require entity linking (EL) function to be executed that is often expensive. We propose TQEL, a framework that minimizes the joint cost of EL calls and top-k query processing. The paper presents two variants - TQEL-exact and TQEL-approximate that retrieve the exact / approximate top-k results. TQEL-approximate, using a weaker stopping condition, achieves significantly improved performance (with the fraction of the cost of TQEL-exact) while providing strong probabilistic guarantees (over 2 orders of magnitude lower EL calls with 95% confidence threshold compared to TQEL-exact). TQEL-exact itself is orders of magnitude better compared to a naive approach that calls EL functions on the entire dataset.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476309®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download$©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/WeiTA21•titleŸ_Robust Voice Querying with MUVE: Optimally Visualizing Results of Phonetically Similar Queries.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476289©publisher±Proc. VLDB Endow.ßauthorsì©Ziyun Wei∞Immanuel Trummer¥Connor Anderson 0002®Abstract⁄úRecently proposed voice query interfaces translate voice input into SQL queries. Unreliable speech recognition on top of the intrinsic challenges of text-to-SQL translation makes it hard to reliably interpret user input. We present MUVE (Multiplots for Voice quEries), a system for robust voice querying. MUVE reduces the impact of ambiguous voice queries by filling the screen with multiplots, capturing results of phonetically similar queries. It maps voice input to a probability distribution over query candidates, executes a selected subset of queries, and visualizes their results in a multiplot.Our goal is to maximize probability to show the correct query result. Also, we want to optimize the visualization (e.g., by coloring a subset of likely results) in order to minimize expected time until users find the correct result. Via a user study, we validate a simple cost model estimating the latter overhead. The resulting optimization problem is NP-hard. We propose an exhaustive algorithm, based on integer programming, as well as a greedy heuristic. As shown in a corresponding user study, MUVE enables users to identify accurate results faster, compared to prior work.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476289®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download$©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/0004XYM021•titleŸTATLANTIC: Making Database Differentially Private and Faster with Accuracy Guarantee.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476337©publisher±Proc. VLDB Endow.ßauthorsï¨Lei Cao 0004≠Dongqing Xiao™Yizhou Yan≠Samuel Madden∞Guoliang Li 0001®Abstract⁄Differential privacy promises to enable data sharing and general data analytics while protecting individual privacy. Because the private data is often stored in the form of relational database that supports SQL queries, making SQL-based analytics differentially private is thus critical. However, the existing SQL-based differentially private systems either only focus on specific type of SQL queries such as COUNT or substantially modify the database engine, thus obstructing adoption in practice. Worse yet, these systems often do not guarantee the desired accuracy by the applications. In this demonstration, using the driving trace workload from Cambridge Mobile Telematics (CMT), we show that our ATLANTIC system, as a database middleware, enforces differential privacy for real-world SQL queries with provable accuracy guarantees and is compatible with existing databases. Moreover, using a sampling-based technique, ATLANTIC significantly speeds up the query execution, yet effectively amplifying the privacy guarantee.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476337®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download"©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/TataPWCNG21•titleŸ7Glean: Structured Extractions from Templatic Documents.§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447703©publisher±Proc. VLDB Endow.ßauthorsñ¨Sandeep Tata≠Navneet PottiÆJames B. Wendt¥Lauro Beltr√£o Costa´Marc Najork´Beliz Gunel®Abstract⁄ÆExtracting structured information from templatic documents is an important problem with the potential to automate many real-world business workflows such as payment, procurement, and payroll. The core challenge is that such documents can be laid out in virtually infinitely different ways. A good solution to this problem is one that generalizes well not only to known templates such as invoices from a known vendor, but also to unseen ones.We developed a system called Glean to tackle this problem. Given a target schema for a document type and some labeled documents of that type, Glean uses machine learning to automatically extract structured information from other documents of that type. In this paper, we describe the overall architecture of Glean, and discuss three key data management challenges : 1) managing the quality of ground truth data, 2) generating training data for the machine learning model using labeled documents, and 3) building tools that help a developer rapidly build and improve a model for a given document type. Through empirical studies on a real-world dataset, we show that these data management techniques allow us to train a model that is over 5 F1 points better than the exact same model architecture without the techniques we describe. We argue that for such information-extraction problems, designing abstractions that carefully manage the training data is at least as important as choosing a good model architecture.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447703®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download"©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/KoutroumanisNDV21•titleŸ8A Demonstration of NoDA: Unified Access to NoSQL Stores.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476361©publisher±Proc. VLDB Endow.ßauthorsîµNikolaos Koutroumanis¥Nikolaos Kousathanas¥Christos DoulkeridisÆAkrivi Vlachou®Abstract⁄zIn this demo paper, we present a system prototype, called NoDA, that unifies access to NoSQL stores, by exposing a single interface to big data developers. This hides the heterogeneity of NoSQL stores, in terms of different query languages, non-standardized access, and different data models. NoDA comprises a layer positioned on top of NoSQL stores that defines a set of basic data access operators (filter, project, aggregate, etc.), implemented for different NoSQL engines. The provision of generic data access operators enables a declarative interface using SQL as query language. Furthermore, NoDA is extended to provide more complex operators, such as geospatial operators, which are only partially supported by NoSQL stores. We demonstrate NoDA by showcasing that the exact same query can be processed by different NoSQL stores, without any modification or transformation whatsoever.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476361®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download"©PaperRefsêã§Infoá§typeßarticle£keyøjournals/pvldb/KaratzoglidiKK21•titleŸ6Automated energy consumption forecasting with EnForce.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476341©publisher±Proc. VLDB Endow.ßauthorsì±Mary KaratzoglidiµParaskevas KerasiotisÆVerena Kantere®Abstract⁄ÃThe need to reduce energy consumption on a global scale has been of high importance during the last years. Research has created methods to make highly accurate forecasts on the energy consumption of buildings and there have been efforts towards the provision of automated forecasting for time series prediction problems. EnForce is a novel system that provides fully automatic forecasting on time series data, referring to the energy consumption of buildings. It uses statistical techniques and deep learning methods to make predictions on univariate or multivariate time series data, so that exogenous factors, such as outside temperature, are taken into account. Moreover, the proposed system provides automatic data preprocessing and, therefore, handles noisy data, with missing values and outliers. EnForce includes full API support and can be used both by experts and non-experts. The proposed demonstration showcases the advantages and technical features of EnForce.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476341®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download"©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/JangJK21•titleŸ4RealGraph-Web: A Graph Analysis Platform on the Web.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476342©publisher±Proc. VLDB Endow.ßauthorsìØMyung-Hwan Jang¨Yong-Yeon Jo≠Sang-Wook Kim®Abstract⁄ÕIn this demo, we present RealGraphWeb, a web-based platform that provides various kinds of graph analysis services. RealGraphWeb is based on RealGraph, a graph engine that addresses the problem of performance degradation in processing real-world big graphs, achieving great performance improvement up to 44 times over existing state-of-the-art graph engines. RealGraphWeb runs on a single machine with a web-based interface, thereby allowing users to easily and conveniently enjoy graph analysis services and perform various graph algorithms anywhere on the web. In this demo, we present how a user can analyze a graph on RealGraphWeb in three steps and get the analysis result quickly via a graphical user interface.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476342®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download"©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/PastorGBA21•titleªHow Divergent Is Your Data?§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476357©publisher±Proc. VLDB Endow.ßauthorsî≠Eliana Pastor∞Andrew Gavgavian≠Elena BaralisÆLuca de Alfaro®Abstract⁄øWe present DivExplorer, a tool that enables users to explore datasets and find subgroups of data for which a classifier behaves in an anomalous manner. These subgroups, denoted as divergent subgroups, may exhibit, for example, higher-than-normal false positive or negative rates. DivExplorer can be used to analyze and debug classifiers. If the data has ethical or social implications, DivExplorer can be also used to identify bias in classifiers.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476357®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download!©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/GhoshNDTAB21•titleŸ&Interactive Demonstration of SQLCHECK.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476343©publisher±Proc. VLDB Endow.ßauthorsñ≠Arthita Ghosh∞Arpit NarechaniaŸ!Visweswara Sai Prashanth DintyalaÆSu Timurturkan´Joy Arulraj¨Deven Bansod®Abstract⁄ØWe will demonstrate a prototype of sqlcheck, a holistic toolchain for automatically finding and fixing anti-patterns in database applications. The advent of modern database-as-a-service platforms has made it easy for developers to quickly create scalable applications. However, it is still challenging for developers to design performant, maintainable, and accurate applications. This is because developers may unknowingly introduce anti-patterns in the application's SQL statements. These anti-patterns are design decisions that are intended to solve a problem, but often lead to other problems by violating fundamental design principles.sqlcheck leverages techniques for automatically: (1) detecting anti-patterns with high accuracy, (2) ranking them based on their impact on performance, maintainability, and accuracy of applications, and (3) suggesting alternative queries and changes to the database design to fix these anti-patterns. We will demonstrate that sqlcheck enables developers to create more performant, maintainable, and accurate applications. We will show the prevalence of these anti-patterns in a large collection of queries and databases collected from open-source repositories.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476343®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/OrogatE21•titleŸáCBench: Demonstrating Comprehensive Evaluation of Question Answering Systems over Knowledge Graphs Through Deep Analysis of Benchmarks.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476326©publisher±Proc. VLDB Endow.ßauthorsí∞Abdelghny Orogat≠Ahmed El-Roby®Abstract⁄ØA plethora of question answering (QA) systems that retrieve answers to natural language questions from knowledge graphs have been developed in recent years. However, choosing a benchmark to accurately assess the quality of a question answering system is a challenging task due to the high degree of variations among the available benchmarks with respect to their fine-grained properties.In this demonstration, we introduce CBench, an extensible, and more informative benchmarking suite for analyzing benchmarks and evaluating QA systems. CBench can be used to analyze existing benchmarks with respect to several fine-grained linguistic, syntactic, and structural properties of the questions and queries in the benchmarks. Moreover, CBench can be used to facilitate the evaluation of QA systems using a set of popular benchmarks that can be augmented with other user-provided benchmarks. CBench not only evaluates a QA system based on popular single-number metrics but also gives a detailed analysis of the linguistic, syntactic, and structural properties of answered and unanswered questions to help the developers of QA systems to better understand where their system excels and where it struggles.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476326®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/CongCYP21•titleŸEComprehensible Counterfactual Explanation on Kolmogorov-Smirnov Test.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461546©publisher±Proc. VLDB Endow.ßauthorsî™Zicun Cong¨Lingyang Chu¨Yu Yang 0001®Jian Pei®Abstract⁄®The Kolmogorov-Smirnov (KS) test is popularly used in many applications, such as anomaly detection, astronomy, database security and AI systems. One challenge remained untouched is how we can obtain an explanation on why a test set fails the KS test. In this paper, we tackle the problem of producing counterfactual explanations for test data failing the KS test. Concept-wise, we propose the notion of most comprehensible counterfactual explanations, which accommodates both the KS test data and the user domain knowledge in producing explanations. Computation-wise, we develop an efficient algorithm MOCHE (for &lt;u&gt;MO&lt;/u&gt;st &lt;u&gt;C&lt;/u&gt;ompre&lt;u&gt;H&lt;/u&gt;ensible &lt;u&gt;E&lt;/u&gt;xplanation) that avoids enumerating and checking an exponential number of subsets of the test set failing the KS test. MOCHE not only guarantees to produce the most comprehensible counterfactual explanations, but also is orders of magnitudes faster than the baselines. Experiment-wise, we present a systematic empirical study on a series of benchmark real datasets to verify the effectiveness, efficiency and scalability of most comprehensible counterfactual explanations and MOCHE.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461546®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/YangHC21•titleŸ[Auto-Pipeline: Synthesize Data Pipelines By-Target Using Reinforcement Learning and Search.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476303©publisher±Proc. VLDB Endow.ßauthorsì´Junwen YangßYeye He±Surajit Chaudhuri®Abstract⁄2Recent work has made significant progress in helping users to automate single data preparation steps, such as string-transformations and table-manipulation operators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to automate multiple such steps end-to-end, by synthesizing complex data-pipelines with both string-transformations and table-manipulation operators.We propose a novel by-target paradigm that allows users to easily specify the desired pipeline, which is a significant departure from the traditional by-example paradigm. Using by-target, users would provide input tables (e.g., csv or json files), and point us to a "target table" (e.g., an existing database table or BI dashboard) to demonstrate how the output from the desired pipeline would schematically "look like". While the problem is seemingly under-specified, our unique insight is that implicit table constraints such as FDs and keys can be exploited to significantly constrain the space and make the problem tractable. We develop an AUTO-PIPELINE system that learns to synthesize pipelines using deep reinforcement-learning (DRL) and search. Experiments using a benchmark of 700 real pipelines crawled from GitHub and commercial vendors suggest that AUTO-PIPELINE can successfully synthesize around 70% of complex pipelines with up to 10 steps.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476303®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/StoddardMG21•titleŸjTanium Reveal: A Federated Search Engine for Querying Unstructured File Data on Large Enterprise Networks.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476386©publisher±Proc. VLDB Endow.ßauthorsì≤Joshua F. Stoddard¨Adam Mustafa¨Naveen Goela®Abstract⁄oTanium Reveal is a federated search engine deployed on large-scale enterprise networks that is capable of executing data queries across billions of private data files within 60 seconds. Data resides at the edge of networks, potentially distributed on hundreds of thousands of endpoints. The anatomy of the search engine consists of local inverse indexes on each endpoint and a global communication platform called Tanium for issuing search queries to all endpoints. Reveal enables asynchronous parsing and indexing on endpoints without noticeable impact to the endpoints' primary functionality. The engine harnesses the Tanium platform, which is based on a self-organizing, fault-tolerant, scalable, linear chain communication scheme. We demonstrate a multi-tier workflow for executing search queries across a network and for viewing matching snippets of text on any endpoint. We analyze metrics for federated indexing and searching in multiple environments including a production network with 1.05 billion searchable files distributed across 4236 endpoints. While primarily focusing on Boolean, phrase, and similarity query types, Reveal is compatible with further automation (e.g., semantic classification based on machine learning). Lastly, we discuss safeguards for sensitive information within Reveal including cryptographic hashing of private text and role-based access control (RBAC).©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476386®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/GroppeKW21•titleŸCSound of Databases: Sonification of a Semantic Web Database Engine.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476322©publisher±Proc. VLDB Endow.ßauthorsì´Sven Groppe±Rico KlinckenbergØBenjamin Warnke®Abstract⁄éSonifications map data to auditory dimensions and offer a new audible experience to their listeners. We propose a sonification of query processing paired with a corresponding visualization both integrated in a web application. In this demonstration we show that the sonification of different types of relational operators generates different sound patterns, which can be recognized and identified by listeners increasing their understanding of the operators' functionality and supports easy remembering of requirements like merge joins work on sorted input. Furthermore, new ways of analyzing query processing are possible with the sonification approach.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476322®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/LinKMV21•titleŸ]T-Cove: An exposure tracing System based on Cleaning Wi-Fi Events on Organizational Premises.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476344©publisher±Proc. VLDB Endow.ßauthorsî™Yiming LinµPramod P. KhargonekarØSharad MehrotraπNalini Venkatasubramanian®Abstract⁄ŒWiFi connectivity events, generated when a mobile device connects to WiFi access points can serve as a robust, passive, (almost) zero-cost indoor localization technology. The challenge is the coarse level localization it offers that limits its usefulness. We recently developed a novel data cleaning based approach, LOCATER, that exploits patterns in the network data to achieve accuracy as high as 90% at room level granularity making it possible to use network data to support a much larger class of applications. In this paper, we demonstrate one such application to help organizations track levels of occupancy, and potential exposure of the inhabitants of the buildings to others possibly infected on their premises. The system, entitled T-Cove, is in operational use at over 20 buildings at UCI and has now become part of the reopening procedure of the schools. The demonstration will highlight T-Cove functionalities over both live data and data captured in the past.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476344®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyŸ"journals/pvldb/Faure--giovagnoli21•titleŸ<Assessing the Existence of a Model in your Data with ADESIT.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476318©publisher±Proc. VLDB Endow.ßauthorsî∑Pierre Faure-GiovagnoliØMarie Le Guilly∑Vasile-Marian ScuturiciØJean-Marc Petit®Abstract⁄Thanks to the numerous machine learning tools available to us nowadays, it is easier than ever to derive a model from a dataset in the frame of a supervised learning problem. However, when this model behaves poorly compared with an expected performance, the underlying question of the existence of such a model is often underlooked and one might just be tempted to try different parameters or just choose another model architecture. This is why the quality of the learning examples should be considered as early as possible as it acts as a go/no go signal for the following potentially costly learning process. With ADESIT, we provide a way to evaluate the ability of a dataset to perform well for a given supervised learning problem through statistics and visual exploration. Notably, we base our work on recent studies proposing the use of functional dependencies and specifically counterexample analysis to provide dataset cleanliness statistics but also a theoretical upper bound on the prediction accuracy directly linked to the problem settings (measurement uncertainty, expected generalization...). In brief, ADESIT is intended to be part of an iterative data refinement process right after data selection and right before the machine learning process itself. With further analysis for a given problem, the user can characterize, clean and export dynamically selected subsets, allowing to better understand what regions of the data could be refined and where the data precision must be improved by using, for example, new or more precise sensors.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476318®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/SchiavioBB21•titleŸCLanguage-Agnostic Integrated Queries in a Managed Polyglot Runtime.§year§2021£doiŸ(https://doi.org/10.14778/3457390.3457405©publisher±Proc. VLDB Endow.ßauthorsì∞Filippo SchiavioØDaniele Bonetta≠Walter Binder®Abstract⁄;Language-integrated query (LINQ) frameworks offer a convenient programming abstraction for processing in-memory collections of data, allowing developers to concisely express declarative queries using general-purpose programming languages. Existing LINQ frameworks rely on the well-defined type system of statically-typed languages such as C# or Java to perform query compilation and execution. As a consequence of this design, they do not support dynamic languages such as Python, R, or JavaScript. Such languages are however very popular among data scientists, who would certainly benefit from LINQ frameworks in data analytics applications.In this work we bridge the gap between dynamic languages and LINQ frameworks. We introduce DynQ, a novel query engine designed for dynamic languages. DynQ is language-agnostic, since it is able to execute SQL queries in a polyglot language runtime. Moreover, DynQ can execute queries combining data from multiple sources, namely in-memory object collections as well as on-file data and external database systems. Our evaluation of DynQ shows performance comparable with equivalent hand-optimized code, and in line with common data-processing libraries and embedded databases, making DynQ an appealing query engine for standalone analytics applications and for data-intensive server-side workloads.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3457390.3457405®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/AnastasiouCCS21•titleŸSEPICGen: An Experimental Platform for Indoor Congestion Generation and Forecasting.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476349©publisher±Proc. VLDB Endow.ßauthorsîπChrysovalantis Anastasiou≤Constantinos Costa¥Panos K. Chrysanthis≠Cyrus Shahabi®Abstract⁄Effectively and accurately forecasting the congestion in indoor spaces has become particularly important during the pandemic in order to reduce the risk of exposure to airborne viruses. However, there is a lack of readily available indoor congestion data to train such models. Therefore, in this demo paper we propose EPICGen, an experimental platform for indoor congestion generation to support congestion forecasting in indoor spaces. EPICGen consists of two components: (i) Grid Overlayer, which models the floor plans of buildings; and (ii) Congestion Generator, a realistic indoor congestion generator. We demonstrate EPICGen through an intuitive map-based user interface that enables end-users to customize the parameters of the system and visualize generated datasets.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476349®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/ZhengBCCF0G0J21•titleŸ_SpeakNav: Voice-based Route Description Language Understanding for Template Driven Path Search.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476383©publisher±Proc. VLDB Endow.ßauthorsô¨Bolong Zheng´Lei Bi 0005®Juan Cao®Hua Chai®Jun Fang¨Lu Chen 0001™Yunjun Gao≤Xiaofang Zhou 0001≥Christian S. Jensen®Abstract⁄ÏMany navigation applications take natural language speech as input, which avoids users typing in words and thus improves traffic safety. However, navigation applications often fail to understand a user's free-form description of a route. In addition, they only support input of a specific source or destination, which does not enable users to specify additional route requirements. We propose a SpeakNav framework that enables users to describe intended routes via speech and then recommends appropriate routes. Specifically, we propose a novel Route Template based Bidirectional Encoder Representation from Transformers (RT-BERT) model that supports the understanding of natural language route descriptions. The model enables extraction of information of intended POI keywords and related distances. Then we formalize a template-driven path query that uses the extracted information. To enable efficient query processing, we develop a hybrid label index for computing network distances between POIs, and we propose a branch-and-bound algorithm along with a pivot reverse B-tree (PB-tree) index. Experiments with real and synthetic data indicate that RT-BERT offers high accuracy and that the proposed algorithm is capable of outperforming baseline algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476383®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/UotilaLGLDP21•titleŸ]MultiCategory: Multi-model Query Processing Meets Category Theory and Functional Programming.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476314©publisher±Proc. VLDB Endow.ßauthorsñ≠Valter Uotila™Jiaheng LuÆDieter Gawlick¨Zhen Hua LiuÆSouripriya Das≥Gregory Pogossiants®Abstract⁄±The variety of data is one of the important issues in the era of Big Data. The data are naturally organized in different formats and models, including structured data, semi-structured data, and unstructured data. Prior research has envisioned an approach to abstract multi-model data with a schema category and an instance category by using category theory. In this paper, we demonstrate a system, called MultiCategory, which processes multi-model queries based on category theory and functional programming. This demo is centered around four main scenarios to show a tangible system. First, we show how to build a schema category and an instance category by loading different models of data, including relational, XML, key-value, and graph data. Second, we show a few examples of query processing by using the functional programming language Haskell. Third, we demo the flexible outputs with different models of data for the same input query. Fourth, to better understand the category theoretical structure behind the queries, we offer a variety of graphical hooks to explore and visualize queries as graphs with respect to the schema category, as well as the query processing procedure with Haskell.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476314®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ChenX00MQZ21•titleŸ*HyMAC: A Hybrid Matrix Computation System.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476323©publisher±Proc. VLDB Endow.ßauthorsó™Zihao Chen™Zhizhen Xu¨Chen Xu 0001ÆJuan Soto 0001¨Volker Markl¨Weining Qian´Aoying Zhou®Abstract⁄òDistributed matrix computation is common in large-scale data processing and machine learning applications. Iterative-convergent algorithms involving matrix computation share a common property: parameters converge non-uniformly. This property can be exploited to avoid redundant computation via incremental evaluation. Unfortunately, existing systems that support distributed matrix computation, like SystemML, do not employ incremental evaluation. Moreover, incremental evaluation does not always outperform classical matrix computation, which we refer to as a full evaluation. To leverage the benefit of increments, we propose a new system called HyMAC, which performs hybrid plans to balance the trade-off between full and incremental evaluation at each iteration. In this demonstration, attendees will have an opportunity to experience the effect that full, incremental, and hybrid plans have on iterative algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476323®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/NakandalaZK21•titleŸPErrata for "Cerebro: A Data System for Optimized Deep Learning Model Selection".§year§2021£doiŸ(https://doi.org/10.14778/3447689.3447691©publisher±Proc. VLDB Endow.ßauthorsìØSupun Nakandala´Yuhao ZhangØArun Kumar 0001®Abstract⁄ÜWe discovered that there was an inconsistency in the communication cost formulation for the decentralized fine-grained training method in Table 2 of our paper [1]. We used Horovod as the archetype for decentralized fine-grained approaches, and its correct communication cost is higher than what we had reported. So, we amend the communication cost of decentralized fine-grained to[EQUATION]©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3447689.3447691®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/DiestelkamperLG21•titleŸMDebugging Missing Answers for Spark Queries over Nested Data with Breadcrumb.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476331©publisher±Proc. VLDB Endow.ßauthorsî≥Ralf Diestelk√§mper™Seokki Lee¨Boris Glavic∞Melanie Herschel®Abstract⁄ŒWe present Breadcrumb, a system that aids developers in debugging queries through query-based explanations for missing answers. Given as input a query and an expected, but missing, query result, Breadcrumb identifies operators in the input query that are responsible for the failure to derive the missing answer. These operators form explanations that guide developers who can then focus their debugging efforts on fixing these parts of the query. Breadcrumb is implemented on top of Apache Spark. Our approach is the first that scales to big data dimensions and is capable of finding explanations for common errors in queries over nested and de-normalized data, e.g., errors based on misinterpreting schema semantics.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476331®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/HuSPPR20•titleŸGAggregated Deletion Propagation for Counting Conjunctive Query Answers.§year§2020£doiŸ(https://doi.org/10.14778/3425879.3425892©publisher±Proc. VLDB Endow.ßauthorsï¨Xiao Hu 0005¨Shouzhuo Sun¨Shweta Patwa≤Debmalya Panigrahi´Sudeepa Roy®Abstract⁄ëWe investigate the computational complexity of minimizing the source side-effect in order to remove a given number of tuples from the output of a conjunctive query. This is a variant of the well-studied deletion propagation problem, the difference being that we are interested in removing the smallest subset of input tuples to remove a given number of output tuples while deletion propagation focuses on removing a specific output tuple. We call this the Aggregated Deletion Propagation problem. We completely characterize the poly-time solvability of this problem for arbitrary conjunctive queries without self-joins. This includes a poly-time algorithm to decide solvability, as well as an exact structural characterization of NP-hard instances. We also provide a practical algorithm for this problem (a heuristic for NP-hard instances) and evaluate its experimental performance on real and synthetic datasets.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3425879.3425892®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/GomesOCB21•titleŸARailgun: managing large streaming windows under MAD requirements.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476384©publisher±Proc. VLDB Endow.ßauthorsîØAna Sofia Gomes±Jo√£o Oliveirinha≠Pedro Cardoso≠Pedro Bizarro®Abstract⁄èSome mission critical systems, e.g., fraud detection, require accurate, real-time metrics over long time sliding windows on applications that demand high throughput and low latencies. As these applications need to run "forever" and cope with large, spiky data loads, they further require to be run in a distributed setting. We are unaware of any streaming system that provides all those properties. Instead, existing systems take large simplifications, such as implementing sliding windows as a fixed set of overlapping windows, jeopardizing metric accuracy (violating regulatory rules) or latency (breaching service agreements). In this paper, we propose Railgun, a fault-tolerant, elastic, and distributed streaming system supporting real-time sliding windows for scenarios requiring high loads and millisecond-level latencies. We benchmarked an initial prototype of Railgun using real data, showing significant lower latency than Flink and low memory usage independent of window size. Further, we show that Railgun scales nearly linearly, respecting our msec-level latencies at high percentiles (&lt;250ms @ 99.9%) even under a load of 1 million events per second.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476384®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/LiuKTDH21•titleŸnCatch a Blowfish Alive: A Demonstration of Policy-Aware Differential Privacy for Interactive Data Exploration.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476363©publisher±Proc. VLDB Endow.ßauthorsï¨Jiaxiang Liu™Karl Knopf™Yiqing Tan™Bolin Ding™Xi He 0001®Abstract⁄6Policy-aware differential privacy (DP) frameworks such as Blowfish privacy enable more accurate query answers than standard DP. In this work, we build the first policy-aware DP system for interactive data exploration, BlowfishDB, that aims to (i) provide bounded and flexible privacy guarantees to the data curators of sensitive data and (ii) support accurate and efficient data exploration by data analysts. However, the specification and processing of customized privacy policies incur additional performance cost, especially for datasets with a large domain. To address this challenge, we propose dynamic Blowfish privacy which allows for the dynamic generation of smaller privacy policies and their data representations at query time. BlowfishDB ensures same levels of accuracy and privacy as one would get working on the static privacy policy. In this demonstration of BlowfishDB, we show how a data curator can fine-tune privacy policies for a sensitive dataset and how a data analyst can retrieve accuracy-bounded query answers efficiently without being a privacy expert.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476363®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyŸ journals/pvldb/AgiwalLMRSZZCCD21•titleŸQNapa: Powering Scalable Data Warehousing with Robust Query Performance at Google.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476377©publisher±Proc. VLDB Endow.ßauthors‹ ,¨Ankur Agiwal©Kevin LaiπGokul Nath Babu Manoharan±Indrajit Roy 0001∂Jagan Sankaranarayanan©Hao ZhangßTao Zou®Jim Chen®Min Chen®Ming Dai®Thanh Do©Haoyu Gao´Haoyan Geng¨Raman Grover®Bo Huang¨Yanlai HuangßAdam Li¨Jianyi LiangßTao Lin¶Li LiußYao Liu¶Xi Mao©Maya MengØPrashant Mishra©Jay Patel©Rajesh Sr≤Vijayshankar Raman≠Sourashis Roy∂Mayank Singh Shishodia¨Tianhang Sun´Justin Tang¨Jun Tatemura¨Sagar TrehanØRamkumar VadaliªPrasanna Venkatasubramanian™Joey Zhang´Kefei Zhang™Yupu Zhang≠Zeleng Zhuang¨Goetz Graefe¨Divy Agrawal≥Jeffrey F. NaughtonÆSujata Kosalge±Hakan Hacig√ºm√ºs®Abstract⁄ Google services continuously generate vast amounts of application data. This data provides valuable insights to business users. We need to store and serve these planet-scale data sets under the extremely demanding requirements of scalability, sub-second query response times, availability, and strong consistency; all this while ingesting a massive stream of updates from applications used around the globe. We have developed and deployed in production an analytical data management system, Napa, to meet these requirements. Napa is the backend for numerous clients in Google. These clients have a strong expectation of variance-free, robust query performance. At its core, Napa's principal technologies for robust query performance include the aggressive use of materialized views, which are maintained consistently as new data is ingested across multiple data centers. Our clients also demand flexibility in being able to adjust their query performance, data freshness, and costs to suit their unique needs. Robust query processing and flexible configuration of client databases are the hallmark of Napa design.Most of the related work in this area takes advantage of full flexibility to design the whole system without the need to support a diverse set of preexisting use cases. In comparison, a particular challenge we faced is that Napa needs to deal with hard constraints from existing applications and infrastructure, so we could not do a "green field" system, but rather had to satisfy existing constraints. These constraints led us to make particular design decisions and also devise new techniques to meet the challenges. In this paper, we share our experiences in designing, implementing, deploying, and running Napa in production with some of Google's most demanding applications.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476377®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/GuanMCW21•titleŸ5GEDet: Detecting Erroneous Nodes with A Few Examples.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476367©publisher±Proc. VLDB Endow.ßauthorsî™Sheng Guan™Hanchao Ma±Sutanay Choudhury™Yinghui Wu®Abstract⁄CDetecting nodes with erroneous values in real-world graphs remains challenging due to the lack of examples and various error scenarios. We demonstrate GEDet, an error detection engine that can detect erroneous nodes in graphs with a few examples. The GEDet framework tackles error detection as a few-shot node classification problem. We invite the attendees to experience the following unique features. (1) Few-shot detection. Users only need to provide a few examples of erroneous nodes to perform error detection with GEDet. GEDet achieves desirable accuracy with (a) a graph augmentation module, which automatically generates synthetic examples to learn the classifier, and (b) an adversarial detection module, which improves classifiers to better distinguish erroneous nodes from both cleaned nodes and synthetic examples. We show that GEDet significantly improves the state-of-the-art error detection methods. (2) Diverse error scenarios. GEDet profiles data errors with a built-in library of transformation functions from correct values to errors. Users can also easily "plug in" new error types or examples. (3) User-centric detection. GEDet supports (a) an active learning mode to engage users to verify detected results, and adapts the error detection process accordingly; and (b) visual interfaces to interpret and track detected errors.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476367®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/MaoYCDSC21•titleŸ3DeFiHap: Detecting and Fixing HiveQL Anti-Patterns.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476316©publisher±Proc. VLDB Endow.ßauthorsñ´Yuetian Mao™Shuai YuanßNan Cui´Tianjiao Du´Beijun Shen´Yuting Chen®Abstract⁄›The emergence of Hive greatly facilitates the management of massive data stored in various places. Meanwhile, data scientists face challenges during HiveQL programming - they may not use correct and/or efficient HiveQL statements in their programs; developers may also introduce anti-patterns indeliberately into HiveQL programs, leading to poor performance, low maintainability, and/or program crashes. This paper presents an empirical study on HiveQL programming, in which 38 HiveQL anti-patterns are revealed. We then design and implement DeFiHap, the first tool for automatically detecting and fixing HiveQL anti-patterns. DeFiHap detects HiveQL anti-patterns via analyzing the abstract syntax trees of HiveQL statements and Hive configurations, and generates fix suggestions by rule-based rewriting and performance tuning techniques. The experimental results show that DeFiHap is effective. In particular, DeFiHap detects 25 anti-patterns and generates fix suggestions for 17 of them.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476316®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/Li0K21•titleŸ1LES3: Learning-based exact set similarity search.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476263©publisher±Proc. VLDB Endow.ßauthorsì®Yifan LiØXiaohui Yu 0001´Nick Koudas®Abstract⁄$Set similarity search is a problem of central interest to a wide variety of applications such as data cleaning and web search. Past approaches on set similarity search utilize either heavy indexing structures, incurring large search costs or indexes that produce large candidate sets. In this paper, we design a learning-based exact set similarity search approach, LES3. Our approach first partitions sets into groups, and then utilizes a light-weight bitmap-like indexing structure, called token-group matrix (TGM), to organize groups and prune out candidates given a query set. In order to optimize pruning using the TGM, we analytically investigate the optimal partitioning strategy under certain distributional assumptions. Using these results, we then design a learning-based partitioning approach called L2P and an associated data representation encoding, PTR, to identify the partitions. We conduct extensive experiments on real and synthetic datasets to fully study LES3, establishing the effectiveness and superiority over other applicable approaches.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476263®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/LeoFB21•titleŸ@Errata for "Teseo and the Analysis of Structural Dynamic Graph".§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476278©publisher±Proc. VLDB Endow.ßauthorsì´Dean De Leo©Per FuchsÆPeter A. Boncz®Abstract⁄÷In our paper [4], we experimentally evaluated our work, Teseo, together with five other systems under the LDBC Graphalytics benchmark [6]. We developed and publicly released [2] an ad-hoc driver for the purpose. Since the time the paper was published, a bug [1] in the driver has been found. Due to this bug, we discovered that the completion times for Graphalytics have been incorrectly measured by 1 second or slightly more than their actual values. This issue involves the results of Table 2, Figure 8 and Table 3 reported in our paper [4]. Still, because the bug equally affected all systems evaluated and it is only related to the measurements, most of the comparisons and the general conclusions in the paper still hold.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476278®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/BernauEGKK21•titleŸ`Quantifying identifiability to choose and audit epsilon in differentially private deep learning.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484231©publisher±Proc. VLDB Endow.ßauthorsï≠Daniel Bernau≠G√ºnther Eibl∂Philip-William Grassal≠Hannah Keller≤Florian Kerschbaum®Abstract⁄±Differential privacy allows bounding the influence that training data records have on a machine learning model. To use differential privacy in machine learning, data scientists must choose privacy parameters (œµ, Œ¥). Choosing meaningful privacy parameters is key, since models trained with weak privacy parameters might result in excessive privacy leakage, while strong privacy parameters might overly degrade model utility. However, privacy parameter values are difficult to choose for two main reasons. First, the theoretical upper bound on privacy loss (œµ, Œ¥) might be loose, depending on the chosen sensitivity and data distribution of practical datasets. Second, legal requirements and societal norms for anonymization often refer to individual identifiability, to which (œµ, Œ¥) are only indirectly related.We transform (œµ, Œ¥) to a bound on the Bayesian posterior belief of the adversary assumed by differential privacy concerning the presence of any record in the training dataset. The bound holds for multidimensional queries under composition, and we show that it can be tight in practice. Furthermore, we derive an identifiability bound, which relates the adversary assumed in differential privacy to previous work on membership inference adversaries. We formulate an implementation of this differential privacy adversary that allows data scientists to audit model training and compute empirical identifiability scores and empirical (œµ, Œ¥).©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3484224.3484231®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/RehmanHE21•titleŸYA Demonstration of Relic: A System for REtrospective Lineage InferenCe of Data Workflows.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476347©publisher±Proc. VLDB Endow.ßauthorsì∂Mohammed Suhail Rehman™Silu HuangØAaron J. Elmore®Abstract⁄›The ad-hoc, heterogeneous process of modern data science typically involves loading, cleaning, and mutating dataset(s) into multiple versions recorded as artifacts by various tools within a single data science workflow. Lineage information, including the source datasets, data transformation programs or scripts, or manual annotations, is rarely captured, making it difficult to infer the relationships between artifacts in a given workflow retrospectively. We demonstrate Relic, a tool to retrospectively infer the lineage of data artifacts generated as a result of typical data science workflows, with an interactive demonstration that allows users to input artifact files and visualize the inferred lineage in a web-based setting.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476347®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/PujolWFM21•titleŸ6Budget Sharing for Multi-Analyst Differential Privacy.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467870©publisher±Proc. VLDB Endow.ßauthorsî´David Pujol≠Yikai Wu 0001¨Brandon Fain∂Ashwin Machanavajjhala®Abstract⁄>Large organizations that collect data about populations (like the US Census Bureau) release summary statistics that are used by multiple stakeholders for resource allocation and policy making problems. These organizations are also legally required to protect the privacy of individuals from whom they collect data. Differential Privacy (DP) provides a solution to release useful summary data while preserving privacy. Most DP mechanisms are designed to answer a single set of queries. In reality, there are often multiple stakeholders that use a given data release and have overlapping but not-identical queries. This introduces a novel joint optimization problem in DP where the privacy budget must be shared among different analysts.We initiate study into the problem of DP query answering across multiple analysts. To capture the competing goals and priorities of multiple analysts, we formulate three desiderata that any mechanism should satisfy in this setting - The Sharing Incentive, Non-interference, and Adaptivity - while still optimizing for overall error. We demonstrate how existing DP query answering mechanisms in the multi-analyst settings fail to satisfy at least one of the desiderata. We present novel DP algorithms that provably satisfy all our desiderata and empirically show that they incur low error on realistic tasks.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467870®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/HerodotouK21•titleŸKTrident: Task Scheduling over Tiered Storage Systems in Big Data Platforms.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461545©publisher±Proc. VLDB Endow.ßauthorsí≥Herodotos HerodotouÆElena Kakoulli®Abstract⁄FThe recent advancements in storage technologies have popularized the use of tiered storage systems in data-intensive compute clusters. The Hadoop Distributed File System (HDFS), for example, now supports storing data in memory, SSDs, and HDDs, while OctopusFS and hatS offer fine-grained storage tiering solutions. However, the task schedulers of big data platforms (such as Hadoop and Spark) will assign tasks to available resources only based on data locality information, and completely ignore the fact that local data is now stored on a variety of storage media with different performance characteristics. This paper presents Trident, a principled task scheduling approach that is designed to make optimal task assignment decisions based on both locality and storage tier information. Trident formulates task scheduling as a minimum cost maximum matching problem in a bipartite graph and uses a standard solver for finding the optimal solution. In addition, Trident utilizes two novel pruning algorithms for bounding the size of the graph, while still guaranteeing optimality. Trident is implemented in both Spark and Hadoop, and evaluated extensively using a realistic workload derived from Facebook traces as well as an industry-validated benchmark, demonstrating significant benefits in terms of application performance and cluster efficiency.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461545®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/CiacciaMT21•titleŸ*Preference Queries over Taxonomic Domains.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467874©publisher±Proc. VLDB Endow.ßauthorsì≠Paolo Ciaccia≤Davide Martinenghi∞Riccardo Torlone®Abstract⁄TWhen composing multiple preferences characterizing the most suitable results for a user, several issues may arise. Indeed, preferences can be partially contradictory, suffer from a mismatch with the level of detail of the actual data, and even lack natural properties such as transitivity. In this paper we formally investigate the problem of retrieving the best results complying with multiple preferences expressed in a logic-based language. Data are stored in relational tables with taxonomic domains, which allow the specification of preferences also over values that are more generic than those in the database. In this framework, we introduce two operators that rewrite preferences for enforcing the important properties of transitivity, which guarantees soundness of the result, and specificity, which solves all conflicts among preferences. Although, as we show, these two properties cannot be fully achieved together, we use our operators to identify the only two alternatives that ensure transitivity and minimize the residual conflicts. Building on this finding, we devise a technique, based on an original heuristics, for selecting the best results according to the two possible alternatives. We finally show, with a number of experiments over both synthetic and real-world datasets, the effectiveness and practical feasibility of the overall approach.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467874®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/RemisL21•titleŸ+Using VDMS to Index and Search 100M Images.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476381©publisher±Proc. VLDB Endow.ßauthorsí™Luis Remis≥Chaunte W. Lacewell®Abstract⁄„Data scientists spend most of their time dealing with data preparation, rather than doing what they know best: build machine learning models and algorithms to solve previously unsolvable problems. In this paper, we describe the Visual Data Management System (VDMS), and demonstrate how it can be used to simplify the data preparation process and consequently gain in efficiency simply because we are using a system designed for the job. To demonstrate this, we use one of the largest available public datasets (YFCC100M), with 100 million images and videos, plus additional data including machine-generated tags, for a total of about ~12TB of data. VDMS differs from existing data management systems due to its focus on supporting machine learning and data analytics pipelines that rely on images, videos, and feature vectors, treating these as first class citizens. We demonstrate how VDMS outperforms well-known and widely used systems for data management by up to ~364x, with an average improvement of about 85x for our use-cases, and particularly at scale, for a image search engine implementation. At the same time, VDMS simplifies the process of data preparation and data access, and provides functionalities non-existent in alternative options.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476381®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∂journals/pvldb/ZhaoT21•titleºMinimum Vertex Augmentation.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461536©publisher±Proc. VLDB Endow.ßauthorsí¨Jianwen ZhaoÆYufei Tao 0001®Abstract⁄YThis paper introduces a class of graph problems named minimum vertex augmentation (MVA). Given an input graph G where each vertex carries a binary color 0 or 1, we want to flip the colors of the fewest 0-vertices such that the subgraph induced by all the (original and new) 1-vertices satisfies a user-defined predicate œÄ. In other words, the goal is to minimally augment the subset of 1-vertices to uphold the property œÄ. Different formulations of œÄ instantiate the framework into concrete problems at the core of numerous applications. We first describe a suite of techniques for solving MVA problems with strong performance guarantees, and then present a generic algorithmic paradigm that a user can instantiate to deal with ad-hoc MVA problems. The effectiveness and efficiency of our solutions are verified with an extensive experimental evaluation.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461536®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/SmithBMN21•titleŸ4TraNCE: Transforming Nested Collections Efficiently.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476330©publisher±Proc. VLDB Endow.ßauthorsî¨Jaclyn Smith∞Michael Benedikt≠Brandon Moore≤Milos Nikolic 0001®Abstract⁄uNested relational query languages have long been seen as an attractive tool for scenarios involving large hierarchical datasets. There has been a resurgence of interest in nested relational languages. One driver has been the affinity of these languages for large-scale processing platforms such as Spark and Flink.This demonstration gives a tour of TraNCE, a new system for processing nested data on top of distributed processing systems. The core innovation of the system is a compiler that processes nested relational queries in a series of transformations; these include variants of two prior techniques, shredding and unnesting, as well as a materialization transformation that customizes the way levels of the nested output are generated. The TraNCE platform builds on these techniques by adding components for users to create and visualize queries, as well as data exploration and notebook execution targets to facilitate the construction of large-scale data science applications. The demonstration will both showcase the system from the viewpoint of usability by data scientists and illustrate the data management techniques employed.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476330®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/ZeighamiSK21•titleŸSEstimating Spread of Contact-Based Contagions in a Population Through Sub-Sampling.§year§2021£doiŸ(https://doi.org/10.14778/3461535.3461544©publisher±Proc. VLDB Endow.ßauthorsì∞Sepanta Zeighami≠Cyrus Shahabi™John Krumm®Abstract⁄'Various phenomena such as viruses, gossips, and physical objects (e.g., packages and marketing pamphlets) can be spread through physical contacts. The spread depends on how people move, i.e., their mobility patterns. In practice, mobility patterns of an entire population is never available, and we usually have access to location data of a subset of individuals. In this paper, we formalize and study the problem of estimating the spread of a phenomena in a population, given that we only have access to sub-samples of location visits of some individuals in the population. We show that simple solutions that estimate the spread in the sub-sample and scale it to the population, or more sophisticated solutions that rely on modeling location visits of individuals do not perform well in practice. Instead, we directly model the co-locations between the individuals. We introduce PollSpreader and PollSusceptible, two novel approaches that model the co-locations between individuals using a contact network, and infer the properties of the contact network using the sub-sample to estimate the spread of the phenomena in the entire population. We analytically show that our estimates provide an upper bound and a lower bound on the spread of the disease in expectation. Finally, using a large high-resolution real-world mobility dataset, we experimentally show that our estimates are accurate in practice, while other methods that do not correctly account for co-locations between individuals result in entirely wrong observations (e.g, premature prediction of herd-immunity).©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3461535.3461544®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyæjournals/pvldb/RazniewskiAGS21•titleŸcOn the Limits of Machine Knowledge: Completeness, Recall and Negation in Web-scale Knowledge Bases.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476401©publisher±Proc. VLDB Endow.ßauthorsî∞Simon Razniewski¨Hiba ArnaoutÆShrestha Ghosh≤Fabian M. Suchanek®Abstract⁄General-purpose knowledge bases (KBs) are an important component of several data-driven applications. Pragmatically constructed from available web sources, these KBs are far from complete, which poses a set of challenges in curation as well as consumption.In this tutorial we discuss how completeness, recall and negation in DBs and KBs can be represented, extracted, and inferred. We proceed in 5 parts: (i) We introduce the logical foundations of knowledge representation and querying under partial closed-world semantics. (ii) We show how information about recall can be identified in KBs and in text, and (iii) how it can be estimated via statistical patterns. (iv) We show how interesting negative statements can be identified, and (v) how recall can be targeted in a comparative notion.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476401®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/TziavelisGR21•titleŸ:Beyond Equi-joins: Ranking, Enumeration and Factorization.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476306©publisher±Proc. VLDB Endow.ßauthorsì≤Nikolaos Tziavelis¥Wolfgang GatterbauerØMirek Riedewald®Abstract⁄¨We study theta-joins in general and join predicates with conjunctions and disjunctions of inequalities in particular, focusing on ranked enumeration where the answers are returned incrementally in an order dictated by a given ranking function. Our approach achieves strong time and space complexity properties: with n denoting the number of tuples in the database, we guarantee for acyclic full join queries with inequality conditions that for every value of k, the k top-ranked answers are returned in O(n polylog n + k log k) time. This is within a polylogarithmic factor of O(n + k log k), i.e., the best known complexity for equi-joins, and even of O(n + k), i.e., the time it takes to look at the input and return k answers in any order. Our guarantees extend to join queries with selections and many types of projections (namely those called "free-connex" queries and those that use bag semantics). Remarkably, they hold even when the number of join results is n‚Ñì for a join of ‚Ñì relations. The key ingredient is a novel O(n polylog n)-size factorized representation of the query output, which is constructed on-the-fly for a given query and database. In addition to providing the first nontrivial theoretical guarantees beyond equi-joins, we show in an experimental study that our ranked-enumeration approach is also memory-efficient and fast in practice, beating the running time of state-of-the-art database systems by orders of magnitude.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476306®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyøjournals/pvldb/SchuhknechtPHS21•titleŸTAnyOLAP: Analytical Processing of Arbitrary Data-Intensive Applications without ETL.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476354©publisher±Proc. VLDB Endow.ßauthorsî∏Felix Martin Schuhknecht≤Aaron Priesterroth∞Justus HennebergØReza Salkhordeh®Abstract⁄äThe volume of data that is processed and produced by modern data-intensive applications is constantly increasing. Of course, along with the volume, the interest in analyzing and interpreting this data increases as well. As a consequence, more and more DBMSs and processing frameworks are specialized towards the efficient execution of long-running, read-only analytical queries. Unfortunately, to enable analysis, the data first has to be moved from the source application to the analytics tool via a lengthy ETL process, which increases the runtime and complexity of the analysis pipeline.In this work, we advocate to simply skip ETL altogether. With AnyOLAP, we can perform online analysis of data directly within the source application and while it is running. In the proposed demonstration, the audience will get the chance to put AnyOLAP to the test on a set of data-intensive applications that are supposed to be analyzed while they are up and running. As the entire analysis pipeline of AnyOLAP will be exposed to the audience in form of live and interactive visualizations, users will be able to experience the benefits of true online analysis firsthand.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476354®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyºjournals/pvldb/JacobSSRDT21a•titleŸXA Demonstration of the Exathlon Benchmarking Platform for Explainable Anomaly Detection.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476355©publisher±Proc. VLDB Endow.ßauthorsñ≠Vincent Jacob®Fei SongØArnaud Stiegler©Bijan Rad´Yanlei Diao≠Nesime Tatbul®Abstract⁄ıIn this demo, we introduce Exathlon - a new benchmarking platform for explainable anomaly detection over high-dimensional time series. We designed Exathlon to support data scientists and researchers in developing and evaluating learned models and algorithms for detecting anomalous patterns as well as discovering their explanations. This demo will showcase Exathlon's curated anomaly dataset, novel benchmarking methodology, and end-to-end data science pipeline in action via example usage scenarios.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476355®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∏journals/pvldb/ShiDELM21•titleŸAScalable Community Detection via Parallel Correlation Clustering.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476282©publisher±Proc. VLDB Endow.ßauthorsï´Jessica Shi∞Laxman DhulipalaØDavid Eisenstat´Jakub Lacki±Vahab S. Mirrokni®Abstract⁄ŒGraph clustering and community detection are central problems in modern data mining. The increasing need for analyzing billion-scale data calls for faster and more scalable algorithms for these problems. There are certain trade-offs between the quality and speed of such clustering algorithms. In this paper, we design scalable algorithms that achieve high quality when evaluated based on ground truth.We develop a generalized sequential and shared-memory parallel framework based on the LAMBDACC objective (introduced by Veldt et al.), which encompasses modularity and correlation clustering. Our framework consists of highly-optimized implementations that scale to large data sets of billions of edges and that obtain high-quality clusters compared to ground-truth data, on both unweighted and weighted graphs. Our empirical evaluation shows that this framework improves the state-of-the-art trade-offs between speed and quality of scalable community detection. For example, on a 30-core machine with two-way hyper-threading, our implementations achieve orders of magnitude speedups over other correlation clustering baselines, and up to 28.44√ó speedups over our own sequential baselines while maintaining or improving quality.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476282®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/WuSLCH21•titleŸCDemonstration of Panda: A Weakly Supervised Entity Matching System.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476332©publisher±Proc. VLDB Endow.ßauthorsï©Renzhi Wu´Prem SakalaßPeng Li¶Xu ChußYeye He®Abstract⁄˜Entity matching (EM) refers to the problem of identifying tuple pairs in one or more relations that refer to the same real world entities. Supervised machine learning (ML) approaches, and deep learning based approaches in particular, typically achieve state-of-the-art matching results. However, these approaches require many labeled examples, in the form of matching and non-matching pairs, which are expensive and time-consuming to label.In this paper, we introduce Panda, a weakly supervised system specifically designed for EM. Panda uses the same labeling function abstraction as Snorkel, where labeling functions (LF) are user-provided programs that can generate large amounts of (somewhat noisy) labels quickly and cheaply, which can then be combined via a labeling model to generate accurate final predictions. To support users developing LFs for EM, Panda provides an integrated development environment (IDE) that lives in a modern browser architecture. Panda's IDE facilitates the development, debugging, and life-cycle management of LFs in the context of EM tasks, similar to how IDEs such as Visual Studio or Eclipse excel in general-purpose programming. Panda's IDE includes many novel features purpose-built for EM, such as smart data sampling, a builtin library of EM utility functions, automatically generated LFs, visual debugging of LFs, and finally, an EM-specific labeling model. We show in this demo that Panda IDE can greatly accelerate the development of high-quality EM solutions using weak supervision.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476332®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/BerroFBBB21•titleŸHAn Extensible and Reusable Pipeline for Automated Utterance Paraphrases.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476358©publisher±Proc. VLDB Endow.ßauthorsï´Auday BerroΩMohammad-ali Yaghub Zade Fard¨Marcos B√°ez≤Boualem Benatallah≤Khalid Benabdeslem®Abstract⁄≈In this demonstration paper we showcase an extensible and reusable pipeline for automatic paraphrase generation, i.e., reformulating sentences using different words. Capturing the nuances of human language is fundamental to the effectiveness of Conversational AI systems, as it allows them to deal with the different ways users can utter their requests in natural language. Traditional approaches to utterance paraphrasing acquisition, such as hiring experts or crowd-sourcing, involve processes that are often costly or time consuming, and with their own trade-offs in terms of quality. Automatic paraphrasing is emerging as an attractive alternative that promises a fast, scalable and cost-effective process. In this paper we showcase how our extensible and reusable pipeline for automated utterance paraphrasing can support the development of Conversational AI systems by integrating and extending existing techniques under an unified and configurable framework.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476311.3476358®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyΩjournals/pvldb/LockhartPWW021•titleŸ8Explaining Inference Queries with Bayesian Optimization.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476304©publisher±Proc. VLDB Endow.ßauthorsï∞Brandon Lockhart¨Jinglin Peng™Weiyuan Wu¨Jiannan WangÆEugene Wu 0002®Abstract⁄UObtaining an explanation for an SQL query result can enrich the analysis experience, reveal data errors, and provide deeper insight into the data. Inference query explanation seeks to explain unexpected aggregate query results on inference data; such queries are challenging to explain because an explanation may need to be derived from the source, training, or inference data in an ML pipeline. In this paper, we model an objective function as a black-box function and propose BOExplain, a novel framework for explaining inference queries using Bayesian optimization (BO). An explanation is a predicate defining the input tuples that should be removed so that the query result of interest is significantly affected. BO --- a technique for finding the global optimum of a black-box function --- is used to find the best predicate. We develop two new techniques (individual contribution encoding and warm start) to handle categorical variables. We perform experiments showing that the predicates found by BOExplain have a higher degree of explanation compared to those found by the state-of-the-art query explanation engines. We also show that BOExplain is effective at deriving explanations for inference queries from source and training data on a variety of real-world datasets. BOExplain is open-sourced as a Python package at https://github.com/sfu-db/BOExplain.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476304®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/XiaoDWZK21•titleŸDOptimizing Fitness-For-Use of Differentially Private Linear Queries.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467864©publisher±Proc. VLDB Endow.ßauthorsï¨Yingtai XiaoÆZeyu Ding 0001™Yuxin Wang≠Danfeng Zhang¨Daniel Kifer®Abstract⁄;In practice, differentially private data releases are designed to support a variety of applications. A data release is fit for use if it meets target accuracy requirements for each application. In this paper, we consider the problem of answering linear queries under differential privacy subject to per-query accuracy constraints. Existing practical frameworks like the matrix mechanism do not provide such fine-grained control (they optimize total error, which allows some query answers to be more accurate than necessary, at the expense of other queries that become no longer useful). Thus, we design a fitness-for-use strategy that adds privacy-preserving Gaussian noise to query answers. The covariance structure of the noise is optimized to meet the fine-grained accuracy requirements while minimizing the cost to privacy.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467864®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/ChiosaPA21•titleŸ8SKT: A One-Pass Multi-Sketch Data Analytics Accelerator.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476287©publisher±Proc. VLDB Endow.ßauthorsì≠Monica ChiosaØThomas Preu√üerÆGustavo Alonso®Abstract⁄hData analysts often need to characterize a data stream as a first step to its further processing. Some of the initial insights to be gained include, e.g., the cardinality of the data set and its frequency distribution. Such information is typically extracted by using sketch algorithms, now widely employed to process very large data sets in manageable space and in a single pass over the data. Often, analysts need more than one parameter to characterize the stream. However, computing multiple sketches becomes expensive even when using high-end CPUs. Exploiting the increasing adoption of hardware accelerators, this paper proposes SKT, an FPGA-based accelerator that can compute several sketches along with basic statistics (average, max, min, etc.) in a single pass over the data. SKT has been designed to characterize a data set by calculating its cardinality, its second frequency moment, and its frequency distribution. The design processes data streams coming either from PCIe or TCP/IP, and it is built to fit emerging cloud service architectures, such as Microsoft's Catapult or Amazon's AQUA. The paper explores the trade-offs of designing sketch algorithms on a spatial architecture and how to combine several sketch algorithms into a single design. The empirical evaluation shows how SKT on an FPGA offers a significant performance gain over high-end, server-class CPUs.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476287®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/AddankiGS21•titleŸ>How to Design Robust Algorithms using Noisy Comparison Oracle.§year§2021£doiŸ(https://doi.org/10.14778/3467861.3467862©publisher±Proc. VLDB Endow.ßauthorsì≥Raghavendra Addanki∞Sainyam Galhotra™Barna Saha®Abstract⁄ZMetric based comparison operations such as finding maximum, nearest and farthest neighbor are fundamental to studying various clustering techniques such as k-center clustering and agglomerative hierarchical clustering. These techniques crucially rely on accurate estimation of pairwise distance between records. However, computing exact features of the records, and their pairwise distances is often challenging, and sometimes not possible. We circumvent this challenge by leveraging weak supervision in the form of a comparison oracle that compares the relative distance between the queried points such as `Is point u closer to v or w closer to x?'.However, it is possible that some queries are easier to answer than others using a comparison oracle. We capture this by introducing two different noise models called adversarial and probabilistic noise. In this paper, we study various problems that include finding maximum, nearest/farthest neighbor search under these noise models. Building upon the techniques we develop for these problems, we give robust algorithms for k-center clustering and agglomerative hierarchical clustering. We prove that our algorithms achieve good approximation guarantees with a high probability and analyze their query complexity. We evaluate the effectiveness and efficiency of our techniques empirically on various real-world datasets.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3467861.3467862®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download
©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/SiddiquiCN21•titleŸVCOMPARE: Accelerating Groupwise Comparison in Relational Databases for Data Analytics.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476291©publisher±Proc. VLDB Endow.ßauthorsì∞Tarique Siddiqui±Surajit Chaudhuri≤Vivek R. Narasayya®Abstract⁄JData analysis often involves comparing subsets of data across many dimensions for finding unusual trends and patterns. While the comparison between subsets of data can be expressed using SQL, they tend to be complex to write, and suffer from poor performance over large and high-dimensional datasets. In this paper, we propose a new logical operator COMPARE for relational databases that concisely captures the enumeration and comparison between subsets of data and greatly simplifies the expressing of a large class of comparative queries. We extend the database engine with optimization techniques that exploit the semantics of COMPARE to significantly improve the performance of such queries. We have implemented these extensions inside Microsoft SQL Server, a commercial DBMS engine. Our extensive evaluation on synthetic and real-world datasets shows that COMPARE results in a significant speedup over existing approaches, including physical plans generated by today's database systems, user-defined functions (UDFs), as well as middleware solutions that compare subsets outside the databases.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476291®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∫journals/pvldb/KuchnikAS21•titleŸHProgressive Compressed Records: Taking a Byte out of Deep Learning Data.§year§2021£doiŸ(https://doi.org/10.14778/3476249.3476308©publisher±Proc. VLDB Endow.ßauthorsìØMichael Kuchnik≤George AmvrosiadisÆVirginia Smith®Abstract⁄∂Deep learning accelerators efficiently train over vast and growing amounts of data, placing a newfound burden on commodity networks and storage devices. A common approach to conserve bandwidth involves resizing or compressing data prior to training. We introduce Progressive Compressed Records (PCRs), a data format that uses compression to reduce the overhead of fetching and transporting data, effectively reducing the training time required to achieve a target accuracy. PCRs deviate from previous storage formats by combining progressive compression with an efficient storage layout to view a single dataset at multiple fidelities---all without adding to the total dataset size. We implement PCRs and evaluate them on a range of datasets, training tasks, and hardware architectures. Our work shows that: (i) the amount of compression a dataset can tolerate exceeds 50% of the original encoding for many DL training tasks; (ii) it is possible to automatically and efficiently select appropriate compression levels for a given task; and (iii) PCRs enable tasks to readily access compressed data at runtime---utilizing as little as half the training bandwidth and thus potentially doubling training speed.©VideoSize¿©VideoLink¿ßPdfLinkŸ3https://dl.acm.org/doi/pdf/10.14778/3476249.3476308®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§typeßarticle£key∑journals/pvldb/Vartak21•titleŸ>From ML Models to Intelligent Applications: The Rise of MLOps.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484240©publisher±Proc. VLDB Endow.ßauthorsë≠Manasi Vartak®Abstract⁄‚The last 5+ years in ML have focused on building the best models, hyperparameter optimization, parallel training, massive neural networks, etc. Now that the building of models has become easy, models are being integrated into every piece of software and device - from smart kitchens to radiology to detecting performance of turbines. This shift from training ML models to building intelligent, ML-driven applications has highlighted a variety of problems going from "a model" to a whole application or business process running on ML. These challenges range from operational challenges (how to package and deploy different types of models using existing SDLC tools and practices), rethinking what existing abstractions mean for ML (e.g., testing, monitoring, warehouses for ML), and collaboration challenges arising from disparate skill sets involved in ML product development (DS vs. SWE), and brand-new problems unique to ML (e.g., explainability, fairness, retraining, etc.) In this talk, I will discuss the slew of challenges that still exist in operationalizing ML to build intelligent applications, some solutions that the community has adopted, and highlight various open problems that would benefit from the research community's contributions.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download ©PaperRefsêã§Infoá§typeßarticle£keyµjournals/pvldb/Shah21•titleŸESummarizing Patients Like Mine via an On-demand Consultation Service.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484242©publisher±Proc. VLDB Endow.ßauthorsë™Nigam Shah®Abstract⁄ΩUsing evidence derived from previously collected medical records to guide patient care has been a long-standing vision of clinicians and informaticians, and one with the potential to transform medical practice. We offered an on-demand consultation service to derive evidence from millions of other patients' data to answer clinician questions and support their bedside decision making. We describe the design and implementation of the service as well as a summary of our experience in responding to the first 100 requests. We will also review a new paradigm for a scalable time-aware clinical data search, and to describe the design, implementation, and use of a search engine realizing this paradigm.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download ©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/Davidson21•titleæIt's not just Cookies and Tea.§year§2021£doiŸ(https://doi.org/10.14778/3476311.3476409©publisher±Proc. VLDB Endow.ßauthorsë±Susan B. Davidson®Abstract⁄bThree of the major research themes over my career have been concurrency, integration and provenance. In this talk, I will explain why these themes are not only important in database research, but how they have played a role in my personal success. I will also discuss how we as a community can use some of these ideas to encourage diversity in our field.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download ©PaperRefsêã§Infoá§typeßarticle£keyªjournals/pvldb/Vanschoren21•titleŸDTowards Scalable Online Machine Learning Collaborations with OpenML.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484239©publisher±Proc. VLDB Endow.ßauthorsë≤Joaquin Vanschoren®Abstract⁄Is massively collaborative machine learning possible? Can we share and organize our collective knowledge of machine learning to solve ever more challenging problems? In a way, yes: as a community, we are already very successful at developing high-quality open-source machine learning libraries, thanks to frictionless collaboration platforms for software development. However, code is only one aspect. The answer is much less clear when we also consider the data that goes into these algorithms and the exact models that are produced. A tremendous amount of work and experience goes into the collection, cleaning, and preprocessing of data and the design, evaluation, and finetuning of models, yet very little of this is shared and organized in a way so that others can easily build on it.Suppose one had a global platform for sharing machine learning datasets, models, and reproducible experiments in a frictionless way so that anybody could chip in at any time to share a good model, add or improve data, or suggest an idea. OpenML is an open-source initiative to create such a platform. It allows anyone to share datasets, machine learning pipelines, and full experiments, organizes all of it online with rich metadata, and enables anyone to reuse and build on them in novel and unexpected ways. All data is open and accessible through APIs, and it is readily integrated into popular machine learning tools to allow easy sharing of models and experiments. This openness also allows a budding ecosystem of automated processes to scale up machine learning further, such as discovering similar datasets, creating systematic benchmarks, or learning from all collected results how to build the best machine learning models and even automatically doing so for any new dataset. We welcome all of you to become a part of it.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download ©PaperRefsêã§Infoá§typeßarticle£keyπjournals/pvldb/Feldmann21•titleŸ#Internet Traffic Analysis at Scale.§year§2021£doiŸ(https://doi.org/10.14778/3484224.3484237©publisher±Proc. VLDB Endow.ßauthorsë≠Anja Feldmann®Abstract⁄±In this talk, I will use multiple internet measurement studies as examples to outline the challenges that we face when performing internet-scale traffic analysis, including implications of the COVID-19 pandemic on internet traffic as well as detecting IoT devices through the lens of an ISP. Using this as motivation, I will discuss the challenges of working with network-wide flow data and correlating such data with other datasets.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download ©PaperRefsê