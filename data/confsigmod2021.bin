‹0ã§Infoá§type≠inproceedings£keyπconf/sigmod/MarcusNMTAK21•titleŸ1Bao: Making Learned Query Optimization Practical.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452838©publisher±SIGMOD Conferenceßauthorsñ´Ryan MarcusØParimarjan Negi™Hongzi Mao≠Nesime Tatbul±Mohammad Alizadeh™Tim Kraska®Abstract⁄÷Recent efforts applying machine learning techniques to query optimization have shown few practical gains due to substantive training overhead, inability to adapt to changes, and poor tail performance. Motivated by these difficulties, we introduce Bao (the \underlineBa ndit \underlineo ptimizer). Bao takes advantage of the wisdom built into existing query optimizers by providing per-query optimization hints. Bao combines modern tree convolutional neural networks with Thompson sampling, a well-studied reinforcement learning algorithm. As a result, Bao automatically learns from its mistakes and adapts to changes in query workloads, data, and schema. Experimentally, we demonstrate that Bao can quickly learn strategies that improve end-to-end query execution performance, including tail latency, for several workloads containing long-running queries. In cloud environments, we show that Bao can offer both reduced costs and better performance compared with a commercial system.©VideoSizeß53.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452838&amp;file=3448016.3452838.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452838®Keywordsì≤query optimization∂reinforcement learning∞machine learning¶Badges¿•Track∞research-article®Citation®DownloadÕr©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/NegiIMAKFJ21•titleŸBSteering Query Optimizers: A Practical Take on Big Data Workloads.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457568©publisher±SIGMOD ConferenceßauthorsóØParimarjan Negi±Matteo Interlandi´Ryan Marcus±Mohammad Alizadeh™Tim Kraska≠Marc Friedman¨Alekh Jindal®Abstract⁄
In recent years, there has been tremendous interest in research that applies machine learning to database systems. Being one of the most complex components of a DBMS, query optimizers could benefit from adaptive policies that are learned systematically from the data and the query workload. Recent research has brought up novel ideas towards a learned query optimizer, however these ideas have not been evaluated on a commercial query processor or on large scale, real-world workloads. In this paper, we take the approach used by Marcus et al. in Bao and adapt it to SCOPE, a big data system used internally at Microsoft. Along the way, we solve multiple new challenges: we define how optimizer rules affect final query plans by introducing the concept of a rule signature, we devise a pipeline computing interesting rule configurations for recurring jobs, and we define a new learning problem allowing us to apply such interesting rule configurations to previously unseen jobs. We evaluate the efficacy of the approach on production workloads that include 150K daily jobs. Our results show that alternative rule configurations can generate plans with lower costs, and this can translate to runtime latency savings of 7-30% on average and up to 90% for a non trivial subset of the workload.©VideoSize•52 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457568&amp;file=3448016.3457568.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457568®Keywordsíølearning for query optimizationædistributed query optimization¶Badges¿•Track∞research-article®Citation®DownloadÕS©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/AmiriAA21•titleŸASharPer: Sharding Permissioned Blockchains Over Network Clusters.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452807©publisher±SIGMOD Conferenceßauthorsì¥Mohammad Javad Amiri±Divyakant Agrawal≠Amr El Abbadi®Abstract⁄YScalability is one of the main roadblocks to business adoption of blockchain systems. Despite recent intensive research on using sharding techniques to enhance the scalability of blockchain systems, existing solutions do not efficiently address cross-shard transactions. In this paper, we introduce SharPer, a scalable permissioned blockchain system. In SharPer, nodes are clustered and each data shard is replicated on the nodes of a cluster. SharPer supports networks consisting of either crash-only or Byzantine nodes. In SharPer, the blockchain ledger is formed as a directed acyclic graph and each cluster maintains only a view of the ledger. SharPer incorporates decentralized flattened protocols to establish cross-shard consensus. The decentralized nature of the cross-shard consensus in SharPer enables parallel processing of transactions with nonoverlapping clusters. Furthermore, SharPer provides deterministic safety guarantees. The experimental results reveal the efficiency of SharPer in terms of performance and scalability especially in workloads with a low percentage of cross-shard transactions.©VideoSize•33 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452807&amp;file=3448016.3452807.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452807®Keywordsï´scalability©consensus®sharding¨permissioned™blockchain¶Badges¿•Track∞research-article®Citation®DownloadÕÃ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/0001ZC21•titleŸ#AI Meets Database: AI4DB and DB4AI.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457542©publisher±SIGMOD Conferenceßauthorsì∞Guoliang Li 0001´Xuanhe Zhou¨Lei Cao 0004®Abstract⁄Database and Artificial Intelligence (AI) can benefit from each other. On one hand, AI can make database more intelligent (AI4DB). For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, learning-based techniques can alleviate this problem. On the other hand, database techniques can optimize AI models (DB4AI). For example, AI is hard to deploy, because it requires developers to write complex codes and train complicated models. Database techniques can be used to reduce the complexity of using AI models, accelerate AI algorithms and provide AI capability inside databases. DB4AI and AI4DB have been extensively studied recently. In this tutorial, we review existing studies on AI4DB and DB4AI. For AI4DB, we review the techniques on learning-based database configuration, optimization, design, monitoring, and security. For DB4AI, we review AI-oriented declarative language, data governance, training acceleration, and inference acceleration. Finally, we provide research challenges and future directions in AI4DB and DB4AI.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457542®Keywordsî¢AI¶models®database∞machine learning¶Badges¿•Track®tutorial®Citation®DownloadÕ«©PaperRefsêã§Infoá§type≠inproceedings£keyªconf/sigmod/JiangGLWAKS0021•titleŸ:Towards Demystifying Serverless Machine Learning Training.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459240©publisher±SIGMOD Conferenceßauthorsô¨Jiawei Jiang´Shaoduo GanßYue Liu´Fanlin WangÆGustavo Alonso¨Ana Klimovic¨Ankit SinglaÆWentao Wu 0001≠Ce Zhang 0001®Abstract⁄tThe appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive applications such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their performance and relative advantage over "serverful" infrastructures (IaaS). In this paper we present a systematic, comparative study of distributed ML training over FaaS and IaaS. We present a design space covering design choices such as optimization algorithms and synchronization protocols, and implement a platform, LambdaML, that enables a fair comparison between FaaS and IaaS. We present experimental results using LambdaML, and further develop an analytic model to capture cost/performance tradeoffs that must be considered when opting for a serverless infrastructure. Our results indicate that ML training pays off in serverless only for models with efficient (i.e., reduced) communication and that quickly converge. In general, FaaS can be much faster but it is never significantly cheaper than IaaS.©VideoSize®804.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459240&amp;file=3448016.3459240.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459240®Keywordsí∞machine learning¥serverless computing¶Badges¿•Track∞research-article®Citation®DownloadÕT©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/XuLZSHL021•titleŸ^Agile and Accurate CTR Prediction Model Training for Massive-Scale Online Advertising Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457236©publisher±SIGMOD Conferenceßauthorsó´Zhiqiang XußDong Li∞Weijie Zhao 0001©Xing Shen¨Tianbo Huang™Xiaoyun Li¨Ping Li 0001®Abstract⁄RDeep neural network has been adopted as the standard model to predict ads click-through rate (CTR) for commercial online advertising systems. Deploying an industrial scale ads system requires to overcome numerous challenges, e.g., hundreds or thousands of billions of input features and also hundreds of billions of training samples, which under the cost budget can cause fundamental issues on storage, communication, or the model training speed. In this work, we present Baidu's industrial-scale practices on how to apply the system and machine learning techniques to address these issues and increase the revenue. In particular, we focus on the strategy for developing GPU-based CTR models combined with quantization techniques to build a compact and agile system which noticeably improves the revenue. With quantization, we are able to effectively increase the model (embedding layer) size without increasing the storage cost. This brings an increase in prediction accuracy and yields a 1% revenue increase and 1.8% higher relative click-through rate in the real sponsored search production environment.©VideoSizeß90.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457236&amp;file=3448016.3457236.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457236®Keywordsî¨quantizationØneural networks∞industrial scale£ctr¶Badges¿•Track∞research-article®Citation®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/CharapkoAD21•titleŸKPigPaxos: Devouring the Communication Bottlenecks in Distributed Consensus.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452834©publisher±SIGMOD Conferenceßauthorsì∞Aleksey Charapko≤Ailidani AilijiangÆMurat Demirbas®Abstract⁄˙Strongly consistent replication helps keep application logic simple and provides significant benefits for correctness and manageability. Unfortunately, the adoption of strongly-consistent replication protocols has been curbed due to their limited scalability and performance. To alleviate the leader bottleneck in strongly-consistent replication protocols, we introduce Pig, an in-protocol communication aggregation and piggybacking technique. Pig employs randomly selected nodes from follower subgroups to relay the leader's message to the rest of the followers in the subgroup, and to perform in-network aggregation of acknowledgments back from these followers. By randomly alternating the relay nodes across replication operations, Pig shields the relay nodes as well as the leader from becoming hotspots and improves throughput scalability. We showcase Pig in the context of classical Paxos protocols employed for strongly consistent replication by many cloud computing services and databases. We implement and evaluate PigPaxos, in comparison to Paxos and EPaxos protocols under various workloads over clusters of size 5 to 25 nodes. We show that the aggregation at the relay has little latency overhead, and PigPaxos can provide more than 3 folds improved throughput over Paxos and EPaxos with little latency deterioration. We support our experimental observations with the analytical modeling of the bottlenecks and show that the communication bottlenecks are minimized when employing only one randomly rotating relay node.©VideoSize®322.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452834&amp;file=3448016.3452834.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452834®Keywordsîµdistributed consensusØlinearizability•paxos´replication¶Badges¿•Track∞research-article®Citation®DownloadÃ¸©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/PengXWHXC21•titleŸ^P<sup>2</sup>B-Trace: Privacy-Preserving Blockchain-based Contact Tracing to Combat Pandemics.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459237©publisher±SIGMOD Conferenceßauthorsñ®Zhe Peng≠Cheng Xu 0004´Haixin Wang¨Jinbin Huang¨Jianliang Xu´Xiaowen Chu®Abstract⁄÷The eruption of a pandemic, such as COVID-19, can cause an unprecedented global crisis. Contact tracing, as a pillar of communicable disease control in public health for decades, has shown its effectiveness on pandemic control. Despite intensive research on contact tracing, existing schemes are vulnerable to attacks and can hardly simultaneously meet the requirements of data integrity and user privacy. The design of a privacy-preserving contact tracing framework to ensure the integrity of the tracing procedure has not been sufficiently studied and remains a challenge. In this paper, we propose P2B-Trace, a privacy-preserving contact tracing initiative based on blockchain. First, we design a decentralized architecture with blockchain to record an authenticated data structure of the user's contact records, which prevents the user from intentionally modifying his local records afterward. Second, we develop a zero-knowledge proximity verification scheme to further verify the user's proximity claim while protecting user privacy. We implement P2B-Trace and conduct experiments to evaluate the cost of privacy-preserving tracing integrity verification. The evaluation results demonstrate the effectiveness of our proposed system.©VideoSize®140.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459237&amp;file=3448016.3459237.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459237®Keywordsî≤privacy-preserving©integrityØcontact tracing™blockchain¶Badges¿•Track∞research-article®Citation®DownloadÃ˘©PaperRefsêã§Infoá§type≠inproceedings£keyªconf/sigmod/TroullinouKLM21•titleŸWSOFOS: Demonstrating the Challenges of Materialized View Selection on Knowledge Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452765©publisher±SIGMOD Conferenceßauthorsî≤Georgia Troullinou¥Haridimos Kondylakis≤Matteo Lissandrini≠Davide Mottin®Abstract⁄xAnalytical queries over RDF data are becoming prominent as a result of the proliferation of knowledge graphs. Yet, RDF databases are not optimized to perform such queries efficiently, leading to long processing times. A well known technique to improve the performance of analytical queries is to exploit materialized views.Although popular in relational databases, view materialization for RDF and SPARQL has not yet transitioned into practice, due to the non-trivial application to the RDF graph model. Motivated by a lack of understanding of the impact of view materialization alternatives for RDF data, we demonstrate Sofos, a system that implements and compares several cost models for view materialization. Sofos is, to the best of our knowledge, the first attempt to adapt cost models, initially studied in relational data, to the generic RDF setting, and to propose new ones, analyzing their pitfalls and merits. Sofos takes an RDF dataset and an analytical query for some facet in the data, and compares and evaluates alternative cost models, displaying statistics and insights about time, memory consumption, and query characteristics.©VideoSize®186.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452765&amp;file=3448016.3452765.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452765®KeywordsìÆview selection∞knowledge graphs£RDF¶Badges¿•Track´short-paper®Citation®Downloada©PaperRefsêã§Infoá§type≠inproceedings£keyºconf/sigmod/SpiegelbergYSK21•titleŸ4Tuplex: Data Science in Python at Native Code Speed.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457244©publisher±SIGMOD Conferenceßauthorsî∑Leonhard F. Spiegelberg±Rahul Yesantharao±Malte Schwarzkopf™Tim Kraska®Abstract⁄lToday's data science pipelines often rely on user-defined functions (UDFs) written in Python. But interpreted Python code is slow, and Python UDFs cannot be compiled to machine code easily.    		 		We present Tuplex, a new data analytics framework that just in-time compiles developers' natural Python UDFs into efficient, end-to-end optimized native code. Tuplex introduces a novel dual-mode execution model that compiles an optimized fast path for the common case, and falls back on slower exception code paths for data that fail to match the fast path's assumptions. Dual-mode execution is crucial to making end-to-end optimizing compilation tractable: by focusing on the common case, Tuplex keeps the code simple enough to apply aggressive optimizations. Thanks to dual-mode execution, Tuplex pipelines always complete even if exceptions occur, and Tuplex's post-facto exception handling simplifies debugging.    		 		We evaluate Tuplex with data science pipelines over real-world datasets. Compared to Spark and Dask, Tuplex improves end-to-end pipeline runtime by 5-91x and comes within 1.1-1.7x of a hand-optimized C++ baseline. Tuplex outperforms other Python compilers by 6x and competes with prior, more limited query compilers. Optimizations enabled by dual-mode processing improve runtime by up to 3x, and Tuplex performs well in a distributed setting on serverless functions.©VideoSize¶234 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457244&amp;file=3448016.3457244.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457244®Keywordsú´native code¨data science≤big data analytics∂user defined functions∫high performance computing∞query processing¶python¨jit compiler´compilationØquery execution£udf©dataframe¶Badges¿•Track∞research-article®Citation®DownloadÕY©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/RuanDLZCLO21•titleŸ<Blockchains vs. Distributed Databases: Dichotomy and Fusion.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452789©publisher±SIGMOD ConferenceßauthorsóÆPingcheng Ruan≤Tien Tuan Anh DinhØDumitrel Loghin¨Meihui ZhangÆGang Chen 0001®Qian Lin≠Beng Chin Ooi®Abstract⁄6Blockchain has come a long way - a system that was initially proposed specifically for cryptocurrencies is now being adapted and adopted as a general-purpose transactional system. As blockchain evolves into another data management system, the natural question is how it compares against distributed database systems. Existing works on this comparison focus on high-level properties, such as security and throughput. They stop short of showing how the underlying design choices contribute to the overall differences. Our work fills this important gap. We perform a twin study of blockchains and distributed database systems as two types of transactional systems. We propose a taxonomy that illustrates the dichotomy across four dimensions, namely replication, concurrency, storage, and sharding. Within each dimension, we discuss how the design choices are driven by two goals: security for blockchains, and performance for distributed databases. We conduct an extensive and in-depth performance analysis of two blockchains, namely Quorum and Hyperledger Fabric, and three distributed databases, namely CockroachDB, TiDB, and etcd. Our analysis exposes the impact of different design choices on the overall performance. Concisely, our work provides a principled framework for analyzing the emerging trend of blockchain-database fusion.©VideoSizeß36.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452789&amp;file=3448016.3452789.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452789®Keywordsï©benchmark´transaction™blockchain®database®taxonomy¶Badges¿•Track∞research-article®Citation®DownloadÕ¯©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/ThostrupSJ0B21•titleŸ5DFI: The Data Flow Interface for High-Speed Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452816©publisher±SIGMOD ConferenceßauthorsïÆLasse ThostrupÆJan SkrzypczakÆMatthias Jasny≥Tobias Ziegler 0001ÆCarsten Binnig®Abstract⁄dIn this paper, we propose the Data Flow Interface (DFI) as a way to make it easier for data processing systems to exploit high-speed networks without the need to deal with the complexity of RDMA. By lifting the level of abstraction, DFI factors out much of the complexity of network communication and makes it easier for developers to declaratively express how data should be efficiently routed to accomplish a given distributed data processing task. As we show in our experiments, DFI is able to support a wide variety of data-centric applications with high performance at a low complexity for the applications.©VideoSize¶196 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452816&amp;file=3448016.3452816.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452816®Keywordsï≤network interfacesØdata managementØmodern hardware≥high-speed networks§rdma¶Badges¿•Track∞research-article®Citation®DownloadÕÌ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/WangY0S21•titleŸaFast Parallel Algorithms for Euclidean Minimum Spanning Tree and Hierarchical Spatial Clustering.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457296©publisher±SIGMOD Conferenceßauthorsî™Yiqiu Wang™Shangdi Yu´Yan Gu 0001´Julian Shun®Abstract⁄This paper presents new parallel algorithms for generating Euclidean minimum spanning trees and spatial clustering hierarchies (known as HDBSCAN*). Our approach is based on generating a well-separated pair decomposition followed by using Kruskal's minimum spanning tree algorithm and bichromatic closest pair computations. We introduce a new notion of well-separation to reduce the work and space of our algorithm for HDBSCAN*. We also give a new parallel divide-and-conquer algorithm for computing the dendrogram and reachability plots, which are used in visualizing clusters of different scale that arise for both EMST and HDBSCAN*. We show that our algorithms are theoretically efficient: they have work (number of operations) matching their sequential counterparts, and polylogarithmic depth (parallel time). We implement our algorithms and propose a memory optimization that requires only a subset of well-separated pairs to be computed and materialized, leading to savings in both space (up to 10x) and time (up to 8x). Our experiments on large real-world and synthetic data sets using a 48-core machine show that our fastest algorithms outperform the best serial algorithms for the problems by 11.13--55.89x, and existing parallel algorithms by at least an order of magnitude.©VideoSize®221.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457296&amp;file=3448016.3457296.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457296®Keywordsì∏shared memory algorithms≥parallel algorithms™clustering¶Badges¿•Track∞research-article®Citation®DownloadÕ‘©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/GaoLCSMFS21•titleŸPA System for Automated Open-Source Threat Intelligence Gathering and Management.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452745©publisher±SIGMOD Conferenceßauthorsó≠Peng Gao 0008¨Xiaoyuan Liu´Edward Choi¨Bhavna SomanØChinmaya Mishra´Kate Farris©Dawn Song®Abstract⁄ITo remain aware of the fast-evolving cyber threat landscape, open-source Cyber Threat Intelligence (OSCTI) has received growing attention from the community. Commonly, knowledge about threats is presented in a vast number of OSCTI reports. Despite the pressing need for high-quality OSCTI, existing OSCTI gathering and management platforms, however, have primarily focused on isolated, low-level Indicators of Compromise. On the other hand, higher-level concepts (e.g., adversary tactics, techniques, and procedures) and their relationships have been overlooked, which contain essential knowledge about threat behaviors that is critical to uncovering the complete threat scenario. To bridge the gap, we propose SecurityKG, a system for automated OSCTI gathering and management. SecurityKG collects OSCTI reports from various sources, uses a combination of AI and NLP techniques to extract high-fidelity knowledge about threat behaviors, and constructs a security knowledge graph. SecurityKG also provides a UI that supports various types of interactivity to facilitate knowledge graph exploration.©VideoSizeß46.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452745&amp;file=3448016.3452745.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452745®Keywordsí≥threat intelligence∏security knowledge graph¶Badges¿•Track´short-paper®Citation®DownloadÕ¥©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/0006ZJWBLMP21•titleŸOMB2: Decomposed Behavior Modeling for Self-Driving Database Management Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457276©publisher±SIGMOD Conferenceßauthorsò´Lin Ma 0006≠William Zhang®Jie Jiao™Wuwen Wang±Matthew Butrovich¨Wan Shen LimØPrashanth Menon¨Andrew Pavlo®Abstract⁄™Database management systems (DBMSs) are notoriously difficult to deploy and administer. The goal of a self-driving DBMS is to remove these impediments by managing itself automatically. However, a critical problem in achieving full autonomy is how to predict the DBMS's runtime behavior and resource consumption. These predictions guide a self-driving DBMS's decision-making components to tune and optimize all aspects of the system. We present the ModelBot2 end-to-end framework for constructing and maintaining prediction models using machine learning (ML) in self-driving DBMSs. Our approach decomposes a DBMS's architecture into fine-grained operating units that make it easier to estimate the system's behavior for configurations that it has never seen before. ModelBot2 then provides an offline execution environment to exercise the system to produce the training data used to train its models. We integrated ModelBot2 in an in-memory DBMS and measured its ability to predict its performance for OLTP and OLAP workloads running in dynamic environments. We also compare ModelBot2 against state-of-the-art ML models and show that our models are up to 25x more accurate in multiple scenarios.©VideoSize®155.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457276&amp;file=3448016.3457276.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457276®Keywordsìµself-driving database≥autonomous database∑database administration¶Badges¿•Track∞research-article®Citation®DownloadÕß©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/ZhouXSNMTABSLRD21•titleŸDFoundationDB: A Distributed Unbundled Transactional Key Value Store.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457559©publisher±SIGMOD Conferenceßauthors‹ ´Jingyu ZhoußMeng Xu∞Alexander Shraer∞Bala Namasivayam´Alex MillerÆEvan TschannenÆSteve Atherton∞Andrew J. Beamon´Rusty Sears™John LeachÆDave Rosenthal®Xin Dong´Will Wilson´Ben Collins≠David Scherer¨Alec Grieser©Young Liu´Alvin MooreØBhaskar Muppana©Xiaoge Su≠Vishesh Yadav®Abstract⁄9FoundationDB is an open source transactional key value store created more than ten years ago. It is one of the first systems to combine the flexibility and scalability of NoSQL architectures with the power of ACID transactions (a.k.a. NewSQL). FoundationDB adopts an unbundled architecture that decouples an in-memory transaction management system, a distributed storage system, and a built-in distributed configuration system. Each sub-system can be independently provisioned and configured to achieve the desired scalability, high-availability and fault tolerance properties. FoundationDB uniquely integrates a deterministic simulation framework, used to test every new feature of the system under a myriad of possible faults. This rigorous testing makes FoundationDB extremely stable and allows developers to introduce and release new features in a rapid cadence. FoundationDB offers a minimal and carefully chosen feature set, which has enabled a range of disparate systems (from semi-relational databases, document and object stores, to graph databases and more) to be built as layers on top. FoundationDB is the underpinning of cloud infrastructure at Apple, Snowflake and other companies, due to its consistency, robustness and availability for storing user data, system metadata and configuration, and other critical information.©VideoSizeß27.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457559&amp;file=3448016.3457559.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457559®Keywordsñ∂strict serializability≤unbundled databaseæoptimistic concurrency control≤simulation testingŸ multiversion concurrency control§oltp¶Badges¿•Track∞research-article®Citation®DownloadÕq©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/DingMCWLLKGK21•titleŸ>Instance-Optimized Data Layouts for Cloud Analytics Workloads.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457270©publisher±SIGMOD Conferenceßauthorsô´Jialin Ding≤Umar Farooq Minhas¥Badrish Chandramouli®Chi Wang®Yinan LißYing LiØDonald KossmannØJohannes Gehrke™Tim Kraska®Abstract⁄Today, businesses rely on efficiently running analytics on large amounts of operational and historical data to gain business insights and competitive advantage. Increasingly, such analytics are run using cloud-based data analytics services, such as Google BigQuery, Microsoft Azure Synapse, Amazon Redshift, and Snowflake. These services persist and process data in compressed, columnar formats, stored in large blocks, each of which contains thousands or millions of records. For these services, disk I/O from (remote) cloud storage is often one of the dominant costs for query processing. To reduce the amount of I/O, services often maintain per-block metadata, such as zone maps, which are used to skip blocks that are irrelevant to the query, leading to lower query execution times. However, the effectiveness of block skipping via zone maps is dependent on how the records are assigned to blocks. Recent work on instance-optimized data layouts aims to maximize block skipping by specializing the block assignment strategy to a specific dataset and workload. However, these existing approaches only optimize the layout for a single table. In this paper, we propose MTO, an instance-optimized data layout framework that determines the blocking strategy for all tables in a multi-table database in the presence of joins, such as in a star or snowflake schema common in real-world workloads. MTO takes advantage of sideways information passing through joins to jointly optimize the layout for all tables, which results in better block skipping and hence reduced query execution times. Experiments on a commercial cloud-based analytics service show that MTO achieves up to 93% reduction in blocks accessed and 75% reduction in end-to-end query times compared to alternative blocking strategies.©VideoSizeß41.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457270&amp;file=3448016.3457270.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457270®Keywordsíºinstance-optimized databasesØcloud analytics¶Badges¿•Track∞research-article®Citation®DownloadÕV©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/Zheng0HNOG21•titleŸVPACE: Learning Effective Task Decomposition for Human-in-the-loop Healthcare Delivery.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457281©publisher±SIGMOD Conferenceßauthorsñ≠Kaiping ZhengÆGang Chen 0001∞Melanie HerschelÆKee Yuan Ngiam≠Beng Chin Ooi´Jinyang Gao®Abstract⁄'Human-in-the-loop data analysis involves both machine learning models and humans in analytic tasks. In healthcare applications, human-in-the-loop data analysis is crucial in that the model can handle "easy" tasks and hand over "hard" ones to medical experts for assistance and medical judgment, where easy tasks are the ones for which the model can provide high accuracy and hard tasks vice versa. In this process, how to decompose tasks in an effective manner is an important stage. To achieve task decomposition, classification with a reject option is a solution. However, existing studies either directly implement a reject option or dive into the theoretical details of the rejection mechanism. Different from such studies, we aim to optimize general classifiers with a reject option and hence, optimize task decomposition for healthcare applications. To this end, we first introduce task decomposition for healthcare applications, which is a crucial stage in human-in-the-loop healthcare delivery. We then devise a framework PACE to learn effective task decomposition concentrating on delivering high performance on the easy tasks. PACE is two-level: on the macro level, PACE employs the Self-Paced Learning method to select easy tasks for each training iteration; on the micro level, PACE adapts the weights of selected tasks through its weighted loss revision strategy. Experimental results in two real-world healthcare datasets show that PACE outperforms baselines in terms of their performance on the easy tasks which are expected to be solved by the learning model.©VideoSize¶1.4 GB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457281&amp;file=3448016.3457281.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457281®Keywordsì≤task decomposition±human-in-the-loop™healthcare¶Badges¿•Track∞research-article®Citation®DownloadÕK©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/SilvestreFSK21•titleŸLClonos: Consistent Causal Recovery for Highly-Available Streaming Dataflows.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457320©publisher±SIGMOD Conferenceßauthorsî≤Pedro F. Silvestre±Marios Fragkoulis≤Diomidis SpinellisµAsterios Katsifodimos®Abstract⁄èStream processing lies in the backbone of modern businesses, being employed for mission critical applications such as real-time fraud detection, car-trip fare calculations, traffic management, and stock trading. Large-scale applications are executed by scale-out stream processing systems on thousands of long-lived operators, which are subject to failures. Recovering from failures fast and consistently are both top priorities, yet they are only partly satisfied by existing fault tolerance methods due to the strong assumptions these make. In particular, prior solutions fail to address consistency in the presence of nondeterminism, such as calls to external services, asynchronous timers and processing-time windows. This paper describes Clonos, a fault tolerance approach that achieves fast, local operator recovery with exactly-once guarantees and high availability by instantly switching to passive standby operators. Clonos enforces causally consistent recovery, including output deduplication, by tracking nondeterminism within the system through causal logging. To implement Clonos we re-engineered many of the internal subsystems of a state of the art stream processor. We evaluate Clonos' overhead and recovery on the Nexmark benchmark against Apache Flink. Clonos achieves instant recovery with negligible overhead and, unlike previous work, does not make assumptions on the deterministic nature of operators.©VideoSizeß42.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457320&amp;file=3448016.3457320.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457320®KeywordsñØfault-tolerance±stream processingØcloud computing±high-availability¨exactly-once´consistency¶Badges¿•Track∞research-article®Citation®DownloadÕA©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/XinMPP21•titleŸYProduction Machine Learning Pipelines: Empirical Analysis and Optimization Opportunities.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457566©publisher±SIGMOD Conferenceßauthorsî©Doris Xin®Hui Miao∂Aditya G. Parameswaran±Neoklis Polyzotis®Abstract⁄œMachine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike the traditional perception of ML in research, ML production pipelines are complex, with many interlocking analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, and complexity of these pipelines to understand how data management research can be used to make them more efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four months, in an effort to understand the complexity and challenges underlying production ML. Our analysis reveals the characteristics, components, and topologies of typical industry-strength ML pipelines at various granularities. Along the way, we introduce a specialized data model for representing and reasoning about repeatedly run components in these ML pipelines, which we call model graphlets. We identify several rich opportunities for optimization, leveraging traditional data management ideas. We show how targeting even one of these opportunities, i.e., identifying and pruning wasted computation that does not translate to model deployment, can reduce wasted computation cost by 50% without compromising the model deployment cadence.©VideoSize®201.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457566&amp;file=3448016.3457566.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457566®KeywordsíØdata management∫machine learning pipelines¶Badges¿•Track∞research-article®Citation®DownloadÕ<©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ChackoMJ21•titleŸGWhy Do My Blockchain Transactions Fail?: A Study of Hyperledger Fabric.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452823©publisher±SIGMOD Conferenceßauthorsì∞Jeeta Ann Chacko´Ruben Mayer≤Hans-Arno Jacobsen®Abstract⁄wPermissioned blockchain systems promise to provide both decentralized trust and privacy. Hyperledger Fabric is currently one of the most wide-spread permissioned blockchain systems and is heavily promoted both in industry and academia. Due to its optimistic concurrency model, the transaction failure rates in Fabric can become a bottleneck. While there is active research to reduce failures, there is a lack of understanding on their root cause and, consequently, a lack of guidelines on how to configure Fabric optimally for different scenarios. To close this gap, in this paper, we first introduce a formal definition of the different types of transaction failures in Fabric. Then, we develop a comprehensive testbed and benchmarking system, HyperLedgerLab, along with four different chaincodes that represent realistic use cases and a chaincode/workload generator. Using HyperLedgerLab, we conduct exhaustive experiments to analyze the impact of different parameters of Fabric such as block size, endorsement policies, and others, on transaction failures. We further analyze three recently proposed optimizations from the literature, Fabric++, Streamchain and FabricSharp, and evaluate under which conditions they reduce the failure rates. Finally, based on our results, we provide recommendations for Fabric practitioners on how to configure the system and also propose new research directions.©VideoSizeß91.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452823&amp;file=3448016.3452823.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452823®Keywordsì¥transaction failures´concurrency´blockchains¶Badges¿•Track∞research-article®Citation®DownloadÕ-©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/DaaseBBR21•titleŸFMaximizing Persistent Memory Bandwidth Utilization for OLAP Workloads.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457292©publisher±SIGMOD Conferenceßauthorsî¨Bj√∂rn Daase¥Lars Jonas BollmeierØLawrence Benson¨Tilmann Rabl®Abstract⁄ˇModern database systems for online analytical processing (OLAP) typically rely on in-memory processing. Keeping all active data in DRAM severely limits the data capacity and makes larger deployments much more expensive than disk-based alternatives. Byte-addressable persistent memory (PMEM) is an emerging storage technology that bridges the gap between slow-but-cheap SSDs and fast-but-expensive DRAM. Thus, research and industry have identified it as a promising alternative to pure in-memory data warehouses. However, recent work shows that PMEM's performance is strongly dependent on access patterns and does not always yield good results when simply treated like DRAM. To characterize PMEM's behavior in OLAP workloads, we systematically evaluate PMEM on a large, multi-socket server commonly used for OLAP workloads. Our evaluation shows that PMEM can be treated like DRAM for most read access but must be used differently when writing. To support our findings, we run the Star Schema Benchmark on PMEM and DRAM. We show that PMEM is suitable for large, read-heavy OLAP workloads with an average query runtime slowdown of 1.66x compared to DRAM. Following our evaluation, we present 7 best practices on how to maximize PMEM's bandwidth utilization in future system designs.©VideoSize§1 GB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457292&amp;file=3448016.3457292.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457292®Keywordsî≥hardware evaluation≥non-volatile memory±persistent memoryºonline analytical processing¶Badges¿•Track∞research-article®Citation®DownloadÕ+©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/QiuD0P021•titleŸELightNE: A Lightweight Graph Processing System for Network Embedding.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457329©publisher±SIGMOD Conferenceßauthorsï¨Jiezhong Qiu∞Laxman Dhulipala≠Jie Tang 0001¨Richard Peng≠Chi Wang 0001®Abstract⁄ÓWe propose LightNE, a cost-effective, scalable, and high quality network embedding system that scales to graphs with hundreds of billions of edges on a single machine. In contrast to the mainstream belief that distributed architecture and GPUs are needed for large-scale network embedding with good quality, we prove that we can achieve higher quality, better scalability, lower cost and faster runtime with shared-memory, CPU-only architecture. LightNE combines two theoretically grounded embedding methods NetSMF and ProNE. We introduce the following techniques to network embedding for the first time: (1) a newly proposed downsampling method to reduce the sample complexity of NetSMF while preserving its theoretical advantages; (2) a high-performance parallel graph processing stack GBBS to achieve high memory efficiency and scalability; (3) sparse parallel hash table to aggregate and maintain the matrix sparsifier in memory; and (4) Intel MKL for efficient randomized SVD and spectral propagation.©VideoSize®250.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457329&amp;file=3448016.3457329.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457329®Keywordsî∑graph processing system∑representation learningÆgraph spectral±network embedding¶Badges¿•Track∞research-article®Citation®DownloadÕ*©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/BandleG021•titleŸNTo Partition, or Not to Partition, That is the Join Question in a Real System.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452831©publisher±SIGMOD Conferenceßauthorsì±Maximilian Bandle´Jana Giceva≥Thomas Neumann 0001®Abstract⁄ˇAn efficient implementation of a hash join has been a highly researched problem for decades. Recently, the radix join has been shown to have superior performance over the alternatives (e.g., the non-partitioned hash join), albeit on synthetic microbenchmarks. Therefore, it is unclear whether one can simply replace the hash join in an RDBMS or use the radix join as a performance booster for selected queries. If the latter, it is still unknown when one should rely on the radix join to improve performance. In this paper, we address these questions, show how to integrate the radix join in Umbra, a code-generating DBMS, and make it competitive for selective queries by introducing a Bloom-filter based semi-join reducer. We have evaluated how well it runs when used in queries from more representative workloads like TPC-H. Surprisingly, the radix join brings a noticeable improvement in only one out of all 59 joins in TPC-H. Thus, with an extensive range of microbenchmarks, we have isolated the effects of the most important workload factors and synthesized the range of values where partitioning the data for the radix join pays off. Our analysis shows that the benefit of data partitioning quickly diminishes as soon as we deviate from the optimal parameters, and even late materialization rarely helps in real workloads. We thus, conclude that integrating the radix join within a code-generating database rarely justifies the increase in code and optimizer complexity and advise against it for processing real-world workloads.©VideoSizeß54.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452831&amp;file=3448016.3452831.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452831®Keywordsï≥in-memory databases¨partitioning∂performance evaluationØjoin processingØmodern hardware¶Badges¿•Track∞research-article®Citation®DownloadÕ)©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ShahLKY021•titleŸATowards Benchmarking Feature Type Inference for AutoML Platforms.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457274©publisher±SIGMOD Conferenceßauthorsï©Vraj Shah≤Jonathan LacanlaleØPremanand Kumar™Kevin YangØArun Kumar 0001®Abstract⁄The paradigm of AutoML has created an opportunity to enable ML for the masses. Emerging industrial-scale cloud AutoML platforms aim to automate the end-to-end ML workflow. While many works have looked into automated feature engineering, model selection, or hyper-parameter search in AutoML, little work has studied a crucial step that serves as an entry point to this workflow: ML feature type inference. The semantic gap between attribute types (e.g., strings, numbers) in databases/files and ML feature types (e.g., Numeric, Categorical) necessitates type inference. In this work, we formalize and standardize this task by creating the first ever benchmark labeled dataset, which we use to objectively evaluate existing AutoML tools. Our dataset has 9921 examples and a 9-class label vocabulary. Our labeled data also offers an alternative approach to automate this task than existing rule-based or syntax-based approaches: use ML itself to predict feature types. We collate a benchmark suite of 30 classification and regression tasks to assess the importance of type inference for downstream models. Empirical comparison on our labeled data shows that an ML-based approach delivers a lift of an average 14% and up to 38% in accuracy for identifying feature types compared to prominent industrial tools. Our downstream benchmark suite reveals that the ML-based approach outperforms existing industrial-strength tools for 47 out of 60 downstream models. We release our labeled dataset, models, and downstream benchmarks in a public repository with a leaderboard.©VideoSizeß29.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457274&amp;file=3448016.3457274.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457274®KeywordsïÆbenchmark data¶autoMLπML feature type inference∞data preparation¨labeled data¶Badges¿•Track∞research-article®Citation®DownloadÕ!©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/AmagataOH21•titleŸTFast and Exact Outlier Detection in Metric Spaces: A Proximity Graph-based Approach.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452782©publisher±SIGMOD ConferenceßauthorsìÆDaichi AmagataÆMakoto Onizuka≠Takahiro Hara®Abstract⁄øDistance-based outlier detection is widely adopted in many fields, e.g., data mining and machine learning, because it is unsupervised, can be employed in a generic metric space, and does not have any assumptions of data distributions. Data mining and machine learning applications face a challenge of dealing with large datasets, which requires efficient distance-based outlier detection algorithms. Due to the popularization of computational environments with large memory, it is possible to build a main-memory index and detect outliers based on it, which is a promising solution for fast distance-based outlier detection. Motivated by this observation, we propose a novel approach that exploits a proximity graph. Our approach can employ an arbitrary proximity graph and obtains a significant speed-up against state-of-the-art. However, designing an effective proximity graph raises a challenge, because existing proximity graphs do not consider efficient traversal for distance-based outlier detection. To overcome this challenge, we propose a novel proximity graph, MRPG. Our empirical study using real datasets demonstrates that MRPG detects outliers significantly faster than the state-of-the-art algorithms.©VideoSizeß87.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452782&amp;file=3448016.3452782.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452782®KeywordsíŸ distance-based outlier detection≠metric spaces¶Badges¿•Track∞research-article®Citation®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Sun0021•titleŸ6Learned Cardinality Estimation for Similarity Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452790©publisher±SIGMOD Conferenceßauthorsì¶Ji Sun∞Guoliang Li 0001≠Nan Tang 0001®Abstract⁄hIn this paper, we study the problem of using deep neural networks (DNNs) for estimating the cardinality of similarity queries. Intuitively, DNNs can capture the distribution of data points, and learn to predict the number of data points that are similar to one data point (a similarity search) or a set of data points (a similarity join). However, DNNs are data hungry; directly training a DNN often results in poor performance. We propose two strategies to improve the accuracy and reduce the size of training data: query segmentation and data segmentation. Query segmentation divides a query into query segments, trains a neural network for each query segment, and combines their outputs with subsequent DNNs to get the query embedding. Data segmentation groups similar data into data segments, train a local-model for each data segment, and learn a global-model to decide which local-models should be used for a given query. The estimates from selected local-models will be summed up as the final estimate.We also extend our model to support similarity joins, which trains a DNN to directly estimate the cumulative sum of objects that are similar to a set of queries. Experiments show that our methods can efficiently (i.e., with small training data) learn to estimate the cardinality of similarity searches/joins, and yield effective estimates (i.e., close to real cardinalities).©VideoSizeß18.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452790&amp;file=3448016.3452790.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452790®Keywordsî±similarity search≠deep learningØsimilarity join∂cardinality estimation¶Badges¿•Track∞research-article®Citation®DownloadÃ˛©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/PhaniR021•titleŸILIMA: Fine-grained Lineage Tracing and Reuse in Machine Learning Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452788©publisher±SIGMOD Conferenceßauthorsì´Arnab Phani≠Benjamin Rath≥Matthias Boehm 0001®Abstract⁄õMachine learning (ML) and data science workflows are inherently exploratory. Data scientists pose hypotheses, integrate the necessary data, and run ML pipelines of data cleaning, feature engineering, model selection and hyper-parameter tuning. The repetitive nature of these workflows, and their hierarchical composition from building blocks exhibits high computational redundancy. Existing work addresses this redundancy with coarse-grained lineage tracing and reuse for ML pipelines. This approach allows using existing ML systems, but views entire algorithms as black boxes, and thus, fails to eliminate fine-grained redundancy and to handle internal non-determinism. In this paper, we introduce LIMA, a practical framework for efficient, fine-grained lineage tracing and reuse inside ML systems. Lineage tracing of individual operations creates new challenges and opportunities. We address the large size of lineage traces with multi-level lineage tracing and reuse, as well as lineage deduplication for loops and functions; exploit full and partial reuse opportunities across the program hierarchy; and integrate this framework with task parallelism and operator fusion. The resulting framework performs fine-grained lineage tracing with low overhead, provides versioning and reproducibility, and is able to eliminate fine-grained redundancy. Our experiments on a variety of ML pipelines show performance improvements up to 12.4x.©VideoSizeß45.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452788&amp;file=3448016.3452788.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452788®Keywordsî≥lineage-based reuseØlineage tracing∂reuse of intermediates™ml systems¶Badges¿•Track∞research-article®Citation®DownloadÃ˚©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/HuangTCLX21•titleŸDDo the Rich Get Richer? Fairness Analysis for Blockchain Incentives.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457285©publisher±SIGMOD Conferenceßauthorsï¨Yuming HuangÆJing Tang 0004¨Qianhao CongØAndrew Lim 0001¨Jianliang Xu®Abstract⁄ÎProof-of-Work (PoW) is the most widely adopted incentive model in current blockchain systems, which unfortunately is energy inefficient. Proof-of-Stake (PoS) is then proposed to tackle the energy issue. The rich-get-richer concern of PoS has been heavily debated in the blockchain community. The debate is centered around the argument that whether rich miners possessing more stakes will obtain higher staking rewards and further increase their potential income in the future. In this paper, we define two types of fairness, i.e., expectational fairness and robust fairness, that are useful for answering this question. In particular, expectational fairness illustrates that the expected income of a miner is proportional to her initial investment, indicating that the expected return on investment is a constant. To better capture the uncertainty of mining outcomes, robust fairness is proposed to characterize whether the return on investment concentrates to a constant with high probability as time evolves. Our analysis shows that the classical PoW mechanism can always preserve both types of fairness as long as the mining game runs for a sufficiently long time. Furthermore, we observe that current PoS blockchains implement various incentive models and discuss three representatives, namely ML-PoS, SL-PoS and C-PoS. We find that (i) ML-PoS (e.g., Qtum and Blackcoin) preserves expectational fairness but may not achieve robust fairness, (ii) SL-PoS (e.g., NXT) does not protect any type of fairness, and (iii) C-PoS (e.g., Ethereum 2.0) outperforms ML-PoS in terms of robust fairness while still maintaining expectational fairness. Finally, massive experiments on real blockchain systems and extensive numerical simulations are performed to validate our analysis.©VideoSizeß36.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457285&amp;file=3448016.3457285.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457285®Keywordsï£PoW™blockchain£PoS®fairness©incentive¶Badges¿•Track∞research-article®Citation®DownloadÃÙ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/GalhotraPS21•titleŸPExplaining Black-Box Algorithms Using Probabilistic Contrastive Counterfactuals.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3458455©publisher±SIGMOD Conferenceßauthorsì∞Sainyam GalhotraÆRomila Pradhan¨Babak Salimi®Abstract⁄ŸThere has been a recent resurgence of interest in explainable artificial intelligence (XAI) that aims to reduce the opaqueness of AI-based decision-making systems, allowing humans to scrutinize and trust them. Prior work in this context has focused on the attribution of responsibility for an algorithm's decisions to its inputs wherein responsibility is typically approached as a purely associational concept. In this paper, we propose a principled causality-based approach for explaining black-box decision-making systems that addresses limitations of existing methods in XAI. At the core of our framework lies probabilistic contrastive counterfactuals, a concept that can be traced back to philosophical, cognitive, and social foundations of theories on how humans generate and select explanations. We show how such counterfactuals can quantify the direct and indirect influences of a variable on decisions made by an algorithm, and provide actionable recourse for individuals negatively affected by the algorithm's decision. Unlike prior work, our system, LEWIS: (1)~can compute provably effective explanations and recourse at local, global and contextual levels; (2)~is designed to work with users with varying levels of background knowledge of the underlying causal model; and (3)~makes no assumptions about the internals of an algorithmic system except for the availability of its input-output data. We empirically evaluate LEWIS on four real-world datasets and show that it generates human-understandable explanations that improve upon state-of-the-art approaches in XAI, including the popular LIME and SHAP. Experiments on synthetic data further demonstrate the correctness of LEWIS's explanations and the scalability of its recourse algorithm.©VideoSizeß35.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3458455&amp;file=3448016.3458455.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3458455®Keywordsî∞machine learningÆexplainable AI©causality®recourse¶Badges¿•Track∞research-article®Citation®DownloadÃÛ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ChenL00021•titleŸTEfficient Exact Algorithms for Maximum Balanced Biclique Search in Bipartite Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459241©publisher±SIGMOD ConferenceßauthorsïßLu Chen¨Chengfei Liu≠Rui Zhou 0001ÆJiajie Xu 0001ØJianxin Li 0001®Abstract⁄Given a bipartite graph, the maximum balanced biclique (MBB) problem, discovering a mutually connected while disjoint sets of equal size with the maximum cardinality, plays a significant role for mining the bipartite graph and has numerous applications. Despite the NP-hardness of the MBB problem, in this paper, we show that an exact MBB can be discovered extremely fast in bipartite graphs for real applications. We propose two exact algorithms dedicated for small dense and large sparse bipartite graphs respectively. For dense bipartite graphs, an O*(1.3803n) algorithm is proposed. This algorithm in fact can find an MBB very fast for small dense bipartite graphs that are common for applications such as VLSI design. This is because, using our proposed novel techniques, the search can fast converge to sufficiently dense bipartite graphs which we prove to be polynomial-time solvable. For large sparse bipartite graphs typical for applications such as biological data analysis, an O*(1.3803 Œ¥) algorithm is proposed, where Œ¥ is only a few hundred for large sparse bipartite graphs with millions of vertices. The indispensible optimization that leads to this time complexity is: we transform a large sparse bipartite graph into a limited number of dense subgraphs such that each of the dense subgraphs has up to Œ¥ vertices and then apply our proposed algorithm for dense bipartite graphs on each of the subgraphs. To further speed up this algorithm, tighter upper bounds, faster heuristics and more effective reductions are proposed, allowing an MBB to be discovered within a few seconds for bipartite graphs with millions of vertices. Extensive experiments are conducted on synthetic and real large bipartite graphs to demonstrate the efficiency and effectiveness of our proposed algorithms and techniques.©VideoSizeß82.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459241&amp;file=3448016.3459241.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459241®KeywordsíØbipartite graph®biclique¶Badges¿•Track∞research-article®Citation®DownloadÃ©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/0001MHGZHMM21•titleŸEParallelizing Intra-Window Join on Multicores: An Experimental Study.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452793©publisher±SIGMOD Conferenceßauthorsò±Shuhao Zhang 0001™Yancan Mao®Jiong He≤Philipp M. Grulich≠Steffen Zeuch¨Bingsheng He∞Richard T. B. Ma¨Volker Markl®Abstract⁄JThe intra-window join (IaWJ), i.e., joining two input streams over a single window, is a core operation in modern stream processing applications. This paper presents the first comprehensive study on parallelizing the IaWJ on modern multicore architectures. In particular, we classify IaWJ algorithms into lazy and eager execution approaches. For each approach, there are further design aspects to consider, including different join methods and partitioning schemes, leading to a large design space. Our results show that none of the algorithms always performs the best, and the choice of the most performant algorithm depends on: (i) workload characteristics, (ii) application requirements, and (iii) hardware architectures. Based on the evaluation results, we propose a decision tree that can guide the selection of an appropriate algorithm.©VideoSizeß21.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452793&amp;file=3448016.3452793.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452793®Keywordsì´window join™multicores±stream processing¶Badges¿•Track∞research-article®Citation®DownloadÃ„©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/GuoWYY21•titleŸdReleasing Locks As Early As You Can: Reducing Contention of Hotspots by Violating Two-Phase Locking.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457294©publisher±SIGMOD Conferenceßauthorsî™Zhihan Guo¶Kan Wu®Cong Yan´Xiangyao Yu®Abstract⁄ÿHotspots, a small set of tuples frequently read/written by a large number of transactions, cause contention in a concurrency control protocol. While a hotspot may comprise only a small fraction of a transaction's execution time, conventional strict two-phase locking allows a transaction to release lock only after the transaction completes, which leaves significant parallelism unexploited. Ideally, a concurrency control protocol serializes transactions only for the duration of the hotspots, rather than the duration of transactions. We observe that exploiting such parallelism requires violating two-phase locking. In this paper, we propose Bamboo, a new concurrency control protocol that can enable such parallelism by modifying the conventional two-phase locking, while maintaining the same guarantees in correctness. We thoroughly analyzed the effect of cascading aborts involved in reading uncommitted data and discussed optimizations that can be applied to further improve the performance. Our evaluation on TPC-C shows a performance improvement up to 4x compared to the best of pessimistic and optimistic baseline protocols. On synthetic workloads that contain a single hotspot, Bamboo achieves a speedup up to 19x over baselines.©VideoSizeß22.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457294&amp;file=3448016.3457294.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457294®Keywordsî±two-phase locking≥concurrency controlØcascading abortßhotspot¶Badges¿•Track∞research-article®Citation®DownloadÃŸ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/SmagulovaD21•titleŸ3Vertex-centric Parallel Computation of SQL Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457314©publisher±SIGMOD ConferenceßauthorsíØAinur Smagulova¨Alin Deutsch®Abstract⁄≠We present a scheme for parallel execution of SQL queries on top of any vertex-centric BSP graph processing engine. The scheme comprises a graph encoding of relational instances and a vertex program specification of our algorithm called TAG-join, which matches the theoretical communication and computation complexity of state-of-the-art join algorithms. When run on top of the vertex-centric TigerGraph database engine on a single multi-core server, TAG-join exploits thread parallelism and is competitive with (and often outperforms) reference RDBMSs on the TPC benchmarks they are traditionally tuned for. In a distributed cluster, TAG-join outperforms the popular Spark SQL engine.©VideoSize®107.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457314&amp;file=3448016.3457314.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457314®KeywordsíªBSP parallel SQL evaluationøvertex-centric graph processing¶Badges¿•Track∞research-article®Citation®DownloadÃŒ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/KohnL021•titleŸ>Building Advanced SQL Analytics From Low-Level Plan Operators.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457288©publisher±SIGMOD Conferenceßauthorsì´Andr√© Kohn´Viktor Leis≥Thomas Neumann 0001®Abstract⁄àAnalytical queries virtually always involve aggregation and statistics. SQL offers a wide range of functionalities to summarize data such as associative aggregates, distinct aggregates, ordered-set aggregates, grouping sets, and window functions. In this work, we propose a unified framework for advanced statistics that composes all flavors of complex SQL aggregates from low-level plan operators. These operators can reuse materialized intermediate results, which decouples monolithic aggregation logic and speeds up complex multi-expression queries. The contribution is therefore twofold: our framework modularizes aggregate implementations, and outperforms traditional systems whenever multiple aggregates are combined. We integrated our approach into the high-performance database system Umbra and experimentally show that we compute complex aggregates faster than the state-of-the-art HyPer system.©VideoSize®274.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457288&amp;file=3448016.3457288.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457288®Keywordsì≤query optimization∞query processing∞database systems¶Badges¿•Track∞research-article®Citation®DownloadÃ«©PaperRefsêã§Infoá§type≠inproceedings£keyºconf/sigmod/Vretinaris0EQO21•titleŸ:Medical Entity Disambiguation Using Graph Neural Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457328©publisher±SIGMOD Conferenceßauthorsï∞Alina Vretinaris©Chuan Lei±Vasilis Efthymiou≠Xiao Qin 0003¨Fatma √ñzcan®Abstract⁄Medical knowledge bases (KBs), distilled from biomedical literature and regulatory actions, are expected to provide high-quality information to facilitate clinical decision making. Entity disambiguation (also referred to as entity linking) is considered as an essential task in unlocking the wealth of such medical KBs. However, existing medical entity disambiguation methods are not adequate due to word discrepancies between the entities in the KB and the text snippets in the source documents. Recently, graph neural networks (GNNs) have proven to be very effective and provide state-of-the-art results for many real-world applications with graph-structured data. In this paper, we introduce ED-GNN based on three representative GNNs (GraphSAGE, R-GCN, and MAGNN) for medical entity disambiguation. We develop two optimization techniques to fine-tune and improve ED-GNN. First, we introduce a novel strategy to represent entities that are mentioned in text snippets as a query graph. Second, we design an effective negative sampling strategy that identifies hard negative samples to improve the model's disambiguation capability. Compared to the best performing state-of-the-art solutions, our ED-GNN offers an average improvement of 7.3% in terms of F1 score on five real-world datasets.©VideoSizeß39.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457328&amp;file=3448016.3457328.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457328®Keywordsì¥graph neural networkµentity disambiguation∞medical ontology¶Badges¿•Track∞research-article®Citation®DownloadÃ∆©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/AmagataH21•titleŸHFast Density-Peaks Clustering: Multicore-based Parallelization Approach.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452781©publisher±SIGMOD ConferenceßauthorsíÆDaichi Amagata≠Takahiro Hara®Abstract⁄=Clustering multi-dimensional points is a fundamental task in many fields, and density-based clustering supports many applications as it can discover clusters of arbitrary shapes. This paper addresses the problem of Density-Peaks Clustering (DPC), a recently proposed density-based clustering framework. Although DPC already has many applications, its straightforward implementation incurs a quadratic time computation to the number of points in a given dataset, thereby does not scale to large datasets. To enable DPC on large datasets, we propose efficient algorithms for DPC. Specifically, we propose an exact algorithm, Ex-DPC, and two approximation algorithms, Approx-DPC and S-Approx-DPC. Under a reasonable assumption about a DPC parameter, our algorithms are sub-quadratic, i.e., break the quadratic barrier. Besides, Approx-DPC does not require any additional parameters and can return the same cluster centers as those of Ex-DPC, rendering an accurate clustering result. S-Approx-DPC requires an approximation parameter but can speed up its computational efficiency. We further present that their efficiencies can be accelerated by leveraging multicore processing. We conduct extensive experiments using synthetic and real datasets, and our experimental results demonstrate that our algorithms are efficient, scalable, and accurate.©VideoSize®117.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452781&amp;file=3448016.3452781.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452781®Keywordsì∏density-peaks clustering∏multi-dimensional points≥parallel algorithms¶Badges¿•Track∞research-article®Citation®DownloadÃ≈©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/Fariha0RGM21•titleŸIConformance Constraint Discovery: Measuring Trust in Data-Driven Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452795©publisher±SIGMOD Conferenceßauthorsï´Anna Fariha≤Ashish Tiwari 0001≤Arjun Radhakrishna≠Sumit Gulwani∞Alexandra Meliou®Abstract⁄VThe reliability of inferences made by data-driven systems hinges on the data's continued conformance to the systems' initial settings and assumptions. When serving data (on which we want to apply inference) deviates from the profile of the initial training data, the outcome of inference becomes unreliable. We introduce conformance constraints, a new data profiling primitive tailored towards quantifying the degree of non-conformance, which can effectively characterize if inference over that tuple is untrustworthy. Conformance constraints are constraints over certain arithmetic expressions (called projections) involving the numerical attributes of a dataset, which existing data profiling primitives such as functional dependencies and denial constraints cannot model. Our key finding is that projections that incur low variance on a dataset construct effective conformance constraints. This principle yields the surprising result that low-variance components of a principal component analysis, which are usually discarded for dimensionality reduction, generate stronger conformance constraints than the high-variance components. Based on this result, we provide a highly scalable and efficient technique--linear in data size and cubic in the number of attributes--for discovering conformance constraints for a dataset. To measure the degree of a tuple's non-conformance with respect to a dataset, we propose a quantitative semantics that captures how much a tuple violates the conformance constraints of that dataset. We demonstrate the value of conformance constraints on two applications: trusted machine learning and data drift. We empirically show that conformance constraints offer mechanisms to (1) reliably detect tuples on which the inference of a machine-learned model should not be trusted, and (2) quantify data drift more accurately than the state of the art.©VideoSize®310.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452795&amp;file=3448016.3452795.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452795®Keywordsì™data drift∏trusted machine learning∑conformance constraints¶Badges¿•Track∞research-article®Citation®DownloadÃ∑©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/DiestelkamperLH21•titleŸkTo Not Miss the Forest for the Trees - A Holistic Approach for Explaining Missing Answers over Nested Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457249©publisher±SIGMOD Conferenceßauthorsî≥Ralf Diestelk√§mper™Seokki Lee∞Melanie Herschel¨Boris Glavic®Abstract⁄ÚQuery-based explanations for missing answers identify which operators of a query are responsible for the failure to return a missing answer of interest. This type of explanations has proven useful, e.g., to debug complex analytical queries. Such queries are frequent in big data systems such as Apache Spark. We present a novel approach to produce query-based explanations. It is the first to support nested data and to consider operators that modify the schema and structure of the data (e.g., nesting, projections) as potential causes of missing answers. To efficiently compute explanations, we propose a heuristic algorithm that applies two novel techniques: (i) reasoning about multiple schema alternatives for a query and (ii) re-validating at each step whether an intermediate result can contribute to the missing answer. Using an implementation on Spark, we demonstrate that our approach is the first to scale to large datasets while often finding explanations that existing techniques fail to identify.©VideoSize®257.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457249&amp;file=3448016.3457249.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457249®Keywordsì≤why-not provenance´nested data∏query-based explanations¶Badges¿•Track∞research-article®Citation®DownloadÃ¥©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/QiuWYLWZ21•titleŸCWeighted Distinct Sampling: Cardinality Estimation for SPJ Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452821©publisher±SIGMOD Conferenceßauthorsñ≠Yuan Qiu 0002™Yilei Wang™Ke Yi 0001ÆFeifei Li 0001´Bin Wu 0003¨Chaoqun Zhan®Abstract⁄)SPJ (select-project-join) queries form the backbone of many SQL queries used in practice. Accurate cardinality estimation of these queries is thus an important problem, with applications in query optimization, approximate query processing, and data analytics. However, this problem has not been rigorously addressed in the literature, despite the fact that cardinality estimation techniques of the three relational operators, selection, projection, and join, have each been extensively studied (but not when used in combination) in the past 30+ years. The major technical difficulty is that (distinct) projection seems to be difficult to combine with the other two operators when it comes to cardinality estimation. In this paper, we give the first formal study of cardinality estimation for SP queries. While it was studied in a prior work in 2001, there is no guarantee on its optimality. We define a class of algorithms, which we call weighted distinct sampling, for estimating SP query sizes, and show how to find a near-optimal sampling strategy that is away from the optimum only by a lower order term. We then extend it to handling SPJ queries, giving the first non-trivial solution for SPJ cardinality estimation. We have also performed an extensive experimental evaluation to complement our theoretical findings.©VideoSizeß46.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452821&amp;file=3448016.3452821.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452821®Keywordsì©spj query∂cardinality estimation±distinct sampling¶Badges¿•Track∞research-article®Citation®DownloadÃ´©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/AsudehSJJ21•titleŸPIdentifying Insufficient Data Coverage for Ordinal Continuous-Valued Attributes.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457315©publisher±SIGMOD ConferenceßauthorsîØAbolfazl Asudeh≠Nima Shahbazi¨Zhongjun JinÆH. V. Jagadish®Abstract⁄†Appropriate training data is a requirement for building good machine-learned models. In this paper, we study the notion of coverage for ordinal and continuous-valued attributes, by formalizing the intuition that the learned model can accurately predict only at data points for which there are "enough" similar data points in the training data set. We develop an efficient algorithm to identify uncovered regions in low-dimensional attribute feature space, by making a connection to Voronoi diagrams. We also develop a randomized approximation algorithm for use in high-dimensional attribute space. We evaluate our algorithms through extensive experiments on real datasets.©VideoSize®160.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457315&amp;file=3448016.3457315.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457315®Keywordsî∏responsible data scienceºfairness in machine learningÆtrustworthy AIÆbias detection¶Badges¿•Track∞research-article®Citation®DownloadÃ©©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/YangL0H021•titleŸ<HUGE: An Efficient and Scalable Subgraph Enumeration System.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457237©publisher±SIGMOD Conferenceßauthorsï¨Zhengyi Yang´Longbin LaiØXuemin Lin 0001≠Kongzhang Hao±Wenjie Zhang 0001®Abstract⁄<Subgraph enumeration is a fundamental problem in graph analytics, which aims to find all instances of a given query graph on a large data graph. In this paper, we propose a system called HUGE to efficiently process subgraph enumeration at scale in the distributed context. HUGE features 1) an optimiser to compute an advanced execution plan without the constraints of existing works; 2) a hybrid communication layer that supports both pushing and pulling communication; 3) a novel two-stage execution mode with a lock-free and zero-copy cache design; 4) a BFS/DFS-adaptive scheduler to bound memory consumption; and 5) two-layer intra- and inter-machine load balancing. HUGE is generic such that all existing distributed subgraph enumeration algorithms can be plugged in to enjoy automatic speed up and bounded-memory execution.©VideoSize®197.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457237&amp;file=3448016.3457237.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457237®Keywordsñºdistributed graph processing•cacheØjoin processing¥subgraph enumeration≤dynamic schedulingÆload balancing¶Badges¿•Track∞research-article®Citation®DownloadÃ®©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/CenKMK21•titleŸ2LEA: A Learned Encoding Advisor for Column Stores.§year§2021£doiŸ'https://doi.org/10.1145/3464509.3464885©publisher´aiDM@SIGMODßauthorsî™Lujing Cen¨Andreas Kipf´Ryan Marcus™Tim Kraska®Abstract⁄°Data warehouses organize data in a columnar format to enable faster scans and better compression. Modern systems offer a variety of column encodings that can reduce storage footprint and improve query performance. Selecting a good encoding scheme for a particular column is an optimization problem that depends on the data, the query workload, and the underlying hardware.  We introduce Learned Encoding Advisor (LEA), a learned approach to column encoding selection. LEA is trained on synthetic datasets with various distributions on the target system. Once trained, LEA uses sample data and statistics (such as cardinality) from the user‚Äôs database to predict the optimal column encodings. LEA can optimize for encoded size, query performance, or a combination of the two. Compared to the heuristic-based encoding advisor of a commercial column store on TPC-H, LEA achieves 19% lower query latency while using 26% less space.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3464509.3464885®Keywordsê¶Badges¿•Track´short-paper®Citation®DownloadÃ§©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/TsengDS21•titleŸGParallel Index-Based Structural Graph Clustering and Its Approximation.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457278©publisher±SIGMOD Conferenceßauthorsì©Tom Tseng∞Laxman Dhulipala´Julian Shun®Abstract⁄qSCAN (Structural Clustering Algorithm for Networks) is a well-studied, widely used graph clustering algorithm. For large graphs, however, sequential SCAN variants are prohibitively slow, and parallel SCAN variants do not effectively share work among queries with different SCAN parameter settings. Since users of SCAN often explore many parameter settings to find good clusterings, it is worthwhile to precompute an index that speeds up queries. This paper presents a practical and provably efficient parallel index-based SCAN algorithm based on GS*-Index, a recent sequential algorithm. Our parallel algorithm improves upon the asymptotic work of the sequential algorithm by using integer sorting. It is also highly parallel, achieving logarithmic span (parallel time) for both index construction and clustering queries. Furthermore, we apply locality-sensitive hashing (LSH) to design a novel approximate SCAN algorithm and prove guarantees for its clustering behavior. We present an experimental evaluation of our algorithms on large real-world graphs. On a 48-core machine with two-way hyper-threading, our parallel index construction achieves 50--151√ó speedup over the construction of GS*-Index. In fact, even on a single thread, our index construction algorithm is faster than GS*-Index. Our parallel index query implementation achieves 5--32√ó speedup over GS*-Index queries across a range of SCAN parameter values, and our implementation is always faster than ppSCAN, a state-of-the-art parallel SCAN algorithm. Moreover, our experiments show that applying LSH results in faster index construction while maintaining good clustering quality.©VideoSize®117.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457278&amp;file=3448016.3457278.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457278®Keywordsì∫locality-sensitive hashing∞graph clustering¥multicore algorithms¶Badges¿•Track∞research-article®Citation®DownloadÃ†©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/FengGHK21•titleŸOEfficient Uncertainty Tracking for Complex Queries with Attribute-level Bounds.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452791©publisher±SIGMOD ConferenceßauthorsîßSu Feng¨Boris Glavic´Aaron Huber±Oliver A. Kennedy®Abstract⁄ Incomplete and probabilistic database techniques are principled methods for coping with uncertainty in data. Unfortunately, the class of queries that can be answered efficiently over such databases is severely limited, even when advanced approximation techniques are employed.We introduce attribute-annotated uncertain databases (AU-DBs), an uncertain data model that annotates tuples and attribute values with bounds to compactly approximate an incomplete database. AU-DBs are closed under relational algebra with aggregation using an efficient evaluation semantics. Using optimizations that trade accuracy for performance, our approach scales to complex queries and large datasets, and produces accurate results.©VideoSize®295.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452791&amp;file=3448016.3452791.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452791®Keywordsî´annotations´aggregation¥incomplete databases´uncertainty¶Badges¿•Track∞research-article®Citation®DownloadÃü©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/SongH21•titleŸ`Auto-Validate: Unsupervised Data Validation Using Data-Domain Patterns Inferred from Data Lakes.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457250©publisher±SIGMOD Conferenceßauthorsí®Jie SongßYeye He®Abstract⁄Complex data pipelines are increasingly common in diverse applications such as BI reporting and ML modeling. These pipelines often recur regularly (e.g., daily or weekly), as BI reports need to be refreshed, and ML models need to be retrained. However, it is widely reported that in complex production pipelines, upstream data feeds can change in unexpected ways, causing downstream applications to break silently that are expensive to resolve. Data validation has thus become an important topic, as evidenced by notable recent efforts from Google and Amazon, where the objective is to catch data quality issues early as they arise in the pipelines. Our experience on production data suggests, however, that on string-valued data, these existing approaches yield high false-positive rates and frequently require human intervention. In this work, we develop a corpus-driven approach to auto-validate machine-generated data by inferring suitable data-validation "patterns'' that accurately describe the underlying data-domain, which minimizes false-positives while maximizing data quality issues caught. Evaluations using production data from real data lakes suggest that \sj is substantially more effective than existing methods. Part of this technology ships as an Auto-Tag feature in Microsoft Azure Purview.©VideoSize®582.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457250&amp;file=3448016.3457250.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457250®Keywordsî¨data quality©data lakeØdata validationÆdata pipelines¶Badges¿•Track∞research-article®Citation®DownloadÃõ©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/TaeW21•titleŸbSlice Tuner: A Selective Data Acquisition Framework for Accurate and Fair Machine Learning Models.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452792©publisher±SIGMOD Conferenceßauthorsí´Ki Hyun Tae¥Steven Euijong Whang®Abstract⁄ºAs machine learning becomes democratized in the era of Software 2.0, a serious bottleneck is acquiring enough data to ensure accurate and fair models. Recent techniques including crowdsourcing provide cost-effective ways to gather such data. However, simply acquiring data as much as possible is not necessarily an effective strategy for optimizing accuracy and fairness. For example, if an online app store has enough training data for certain slices of data (say American customers), but not for others, obtaining more American customer data will only bias the model training. Instead, we contend that one needs to selectively acquire data and propose Slice Tuner, which acquires possibly-different amounts of data per slice such that the model accuracy and fairness on all slices are optimized. This problem is different than labeling existing data (as in active learning or weak supervision) because the goal is obtaining the right amounts of new data. At its core, Slice Tuner maintains learning curves of slices that estimate the model accuracies given more data and uses convex optimization to find the best data acquisition strategy. The key challenges of estimating learning curves are that they may be inaccurate if there is not enough data, and there may be dependencies among slices where acquiring data for one slice influences the learning curves of others. We solve these issues by iteratively and efficiently updating the learning curves as more data is acquired. We evaluate Slice Tuner on real datasets using crowdsourcing for data acquisition and show that Slice Tuner significantly outperforms baselines in terms of model accuracy and fairness, even when the learning curves cannot be reliably estimated.©VideoSizeß50.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452792&amp;file=3448016.3452792.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452792®Keywordsí∞machine learning∫selective data acquisition¶Badges¿•Track∞research-article®Citation®DownloadÃô©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/DiaoGMM21•titleŸ>Efficient Exploration of Interesting Aggregates in RDF Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457307©publisher±SIGMOD Conferenceßauthorsî´Yanlei DiaoÆPawel GuzewiczØIoana ManolescuØMirjana Mazuran®Abstract⁄gAs large Open Data are increasingly shared as RDF graphs today, there is a growing demand to help users discover the most interesting facets of a graph, which are often hard to grasp without automatic tools. We consider the problem of automatically identifying the k most interesting aggregate queries that can be evaluated on an RDF graph, given an integer k and a user-specified interestingness function. Our problem departs from analytics in relational data warehouses in that (i) in an RDF graph we are not given but we must identify the facts, dimensions, and measures of candidate aggregates; (ii) the classical approach to efficiently evaluating multiple aggregates breaks in the face of multi-valued dimensions in RDF data. In this work, we propose an extensible end-to-end framework that enables the identification and evaluation of interesting aggregates based on a new RDF-compatible one-pass algorithm for efficiently evaluating a lattice of aggregates and a novel early-stop technique (with probabilistic guarantees) that can prune uninteresting aggregates. Experiments using both real and synthetic graphs demonstrate the ability of our framework to find interesting aggregates in a large search space, the efficiency of our algorithms (with up to 2.9x speedup over a similar pipeline based on existing algorithms), and scalability as the data size and complexity grow.©VideoSizeß35.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457307&amp;file=3448016.3457307.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457307®Keywordsî£RDF¶graphs∞data explorationÆdata analytics¶Badges¿•Track∞research-article®Citation®DownloadÃò©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/AnglesBDFHHLLLM21•titleŸ"PG-Keys: Keys for Property Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457561©publisher±SIGMOD Conferenceßauthors‹ ¨Renzo AnglesØAngela Bonifati±Stefania DumbravaØGeorge Fletcher≠Keith W. Hare´Jan Hidders≠Victor E. Lee¶Bei Li≠Leonid Libkin´Wim Martens¨Filip Murlak≠Josh PerrymanØOgnjen SavkovicØMichael SchmidtØJuan F. SequedaØSlawek Staworko∞Dominik Tomaszuk®Abstract⁄5We report on a community effort between industry and academia to shape the future of property graph constraints. The standardization for a property graph query language is currently underway through the ISO Graph Query Language (GQL) project. Our position is that this project should pay close attention to schemas and constraints, and should focus next on key constraints. The main purposes of keys are enforcing data integrity and allowing the referencing and identifying of objects. Motivated by use cases from our industry partners, we argue that key constraints should be able to have different modes, which are combinations of basic restriction that require the key to be exclusive, mandatory, and singleton. Moreover, keys should be applicable to nodes, edges, and properties since these all can represent valid real-life entities. Our result is PG-Keys, a flexible and powerful framework for defining key constraints, which fulfills the above goals. PG-Keys is a design by the Linked Data Benchmark Council's Property Graph Schema Working Group, consisting of members from industry, academia, and ISO GQL standards group, intending to bring the best of all worlds to property graph practitioners. PG-Keys aims to guide the evolution of the standardization efforts towards making systems more useful, powerful, and expressive.©VideoSize®162.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457561&amp;file=3448016.3457561.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457561®KeywordsíØkey constraintsØproperty graphs¶Badges¿•Track∞research-article®Citation®DownloadÃó©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/WangSMB21•titleŸTPublic Transport Planning: When Transit Network Connectivity Meets Commuting Demand.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457247©publisher±SIGMOD ConferenceßauthorsîØSheng Wang 0007®Yuan Sun±Christopher Musco´Zhifeng Bao®Abstract⁄7In this paper, we make a first attempt to incorporate both commuting demand and transit network connectivity in bus route planning (CT-Bus), and formulate it as a constrained optimization problem: planning a new bus route with k edges over an existing transit network without building new bus stops to maximize a linear aggregation of commuting demand and connectivity of the transit network. We prove the NP-hardness of CT-Bus and propose an expansion-based greedy algorithm that iteratively scans potential candidate paths in the network. To boost the efficiency of computing the connectivity of new networks with candidate paths, we convert it to a matrix trace estimation problem and employ a Lanczos method to estimate the natural connectivity of the transit network with a guaranteed error bound. Furthermore, we derive upper bounds on the objective values and use them to greedily select candidates for expansion. Our experiments conducted on real-world transit networks in New York City and Chicago verify the efficiency, effectiveness, and scalability of our algorithms.©VideoSize®725.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457247&amp;file=3448016.3457247.movßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457247®Keywordsî™trajectory¥transit connectivity≤bus route planning∞commuting demand¶Badges¿•Track∞research-article®Citation®Downloady©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/PastorAB21•titleŸJLooking for Trouble: Analyzing Classifier Behavior via Pattern Divergence.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457284©publisher±SIGMOD Conferenceßauthorsì≠Eliana PastorÆLuca de Alfaro≠Elena Baralis®Abstract⁄•Machine learning models may perform differently on different data subgroups, which we represent as itemsets (i.e., conjunctions of simple predicates). The identification of these critical data subgroups plays an important role in many applications, for example model validation and testing, or evaluation of model fairness. Typically, domain expert help is required to identify relevant (or sensitive) subgroups. We propose the notion of divergence over itemsets as a measure of different classification behavior on data subgroups, and the use of frequent pattern mining techniques for their identification. A quantification of the contribution of different attribute values to divergence, based on the mathematical foundations provided by Shapley values, allows us to identify both critical and peculiar behaviors of attributes. Extended experiments show the effectiveness of the approach in identifying critical subgroup behaviors.©VideoSize¶203 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457284&amp;file=3448016.3457284.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457284®Keywordsï≠shapley valueÆbias detectionŸ machine-learning model debuggingºfairness in machine learningµclassifier validation¶Badges¿•Track∞research-article®Citation®Downloady©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ZhangCAN21•titleŸUOmniFair: A Declarative System for Model-Agnostic Group Fairness in Machine Learning.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452787©publisher±SIGMOD Conferenceßauthorsî≠Hantian Zhang¶Xu ChuØAbolfazl Asudeh≥Shamkant B. Navathe®Abstract⁄ËMachine learning (ML) is increasingly being used to make decisions in our society. ML models, however, can be unfair to certain demographic groups (e.g., African Americans or females) according to various fairness metrics. Existing techniques for producing fair ML models either are limited to the type of fairness constraints they can handle (e.g., preprocessing) or require nontrivial modifications to downstream ML training algorithms (e.g., in-processing). We propose a declarative system OmniFair for supporting group fairness in ML. OmniFair features a declarative interface for users to specify desired group fairness constraints and supports all commonly used group fairness notions, including statistical parity, equalized odds, and predictive parity. OmniFair is also model-agnostic in the sense that it does not require modifications to a chosen ML algorithm. OmniFair also supports enforcing multiple user declared fairness constraints simultaneously while most previous techniques cannot. The algorithms in OmniFair maximize model accuracy while meeting the specified fairness constraints, and their efficiency is optimized based on the theoretically provable monotonicity property regarding the trade-off between accuracy and fairness that is unique to our system. We conduct experiments on commonly used datasets that exhibit bias against minority groups in the fairness literature. We show that OmniFair is more versatile than existing algorithmic fairness approaches in terms of both supported fairness constraints and downstream ML models. OmniFair reduces the accuracy loss by up to 94.8% compared with the second best method. OmniFair also achieves similar running time to preprocessing methods, and is up to 270x faster than in-processing methods.©VideoSize®143.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452787&amp;file=3448016.3452787.m4vßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452787®Keywordsì∞algorithmic biasÆgroup fairness≥declarative systems¶Badges¿•Track∞research-article®Citation®Downloadv©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/ChatziantoniouK21•titleŸ5DataMingler: A Novel Approach to Data Virtualization.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452752©publisher±SIGMOD Conferenceßauthorsí∑Damianos ChatziantoniouÆVerena Kantere®Abstract⁄“A Data Virtual Machine (DVM) is a novel graph-based conceptual model, similar to the entity-relationship model, representing existing data (persistent, transient, derived) of an organization. A DVM can be built quickly, agilely, offering schematic flexibility to data engineers. Data scientists can visually define complex dataframe queries in an intuitive and simple manner, which are evaluated within an algebraic framework. A DVM can be easily materialized in any logical data model and can be "reoriented'' around any node, offering a "single view of any entity''. In this paper we demonstrate DataMingler, a tool implementing DVMs. We argue that DVMs can have a significant practical impact in analytics environments.©VideoSizeß65.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452752&amp;file=3448016.3452752.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452752®Keywordsí≥data virtualizationµdata virtual machines¶Badges¿•Track´short-paper®Citation®Downloads©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ChenCZL021•titleŸ?Out of Many We are One: Measuring Item Batch with Clock-Sketch.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452784©publisher±SIGMOD Conferenceßauthorsï¨Peiqing Chen©Dong ChenÆLingxiao Zheng©Jizhou LiÆTong Yang 0003®Abstract⁄ﬂItem batch denotes a consecutive sequence of identical items that are close in time in a data stream. It is a useful data stream pattern in cache, burst detection, APT detection, \etc Basic item batch measurement tasks include membership, cardinality, time span and size. Currently, there is no algorithm tailored for item batch measurement. The greatest challenge lies in accurately estimating the time gap between two consecutive identical items. In this paper, we propose Clock-sketch, a framework that introduces the well-known CLOCK algorithm into item batch measurement. The methodology of Clock-sketch is to clean outdated information as much as possible, while guaranteeing that the information of all items visited within the time window $\mathcalT $ is preserved. We conduct experiments on three real-world datasets that feature in item batch pattern. We compare the accuracy and throughput performance of our Clock-sketch against the state-of-the-art and two naive approaches without using Clock-sketch technique. Results of item batch activeness show that Clock-sketch outperforms the state-of-the-art SWAMP in generating 50 times less false positive rate when memory is small. All source codes are open-sourced and released at Github.©VideoSize®134.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452784&amp;file=3448016.3452784.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452784®Keywordsî™item batch¶sketch•clock≤data stream mining¶Badges¿•Track∞research-article®Citation®Downloado©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/BessaCRSSDF21•titleŸ=An Ecosystem of Applications for Modeling Political Violence.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457235©publisher±SIGMOD Conferenceßauthorsó´Aline Bessa≠Sonia Castelo¨R√©mi Rampin≥A√©cio S. R. Santos∞Michael Shoemate≠Vito D'OrazioÆJuliana Freire®Abstract⁄ØConflict researchers face many challenges, including (1) how to model conflicts, (2) how to measure them, (3) how to manage their spatio-temporal character, and (4) how to handle a potential abundance of information and explanation. In this paper, we describe an ecosystem of tools designed for use by subject matter experts that addresses these challenges. Three case studies show workflows that are facilitated by this ecosystem.©VideoSizeß43.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457235&amp;file=3448016.3457235.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457235®Keywordsì≤political violence∏applied machine learning±conflict modeling¶Badges¿•Track∞research-article®Citation®Downloadn©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/WangTB21•titleŸâDemonstrating UDO: A Unified Approach for Optimizing Transaction Code, Physical Design, and System Parameters via Reinforcement Learning.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452754©publisher±SIGMOD Conferenceßauthorsì≠Junxiong Wang∞Immanuel TrummerÆDebabrota Basu®Abstract⁄UDO is a versatile tool for offline tuning of database systems for specific workloads. UDO can consider a variety of tuning choices, reaching from picking transaction code variants over index selections up to database system parameter tuning. UDO uses reinforcement learning to converge to near-optimal configurations, creating and evaluating different configurations via actual query executions (instead of relying on simplifying cost models). To cater to different parameter types, UDO distinguishes heavy parameters (which are expensive to change, e.g. physical design parameters) from light parameters. Specifically for optimizing heavy parameters, UDO uses reinforcement learning algorithms that allow delaying the point at which reward feedback becomes available. This gives us the freedom to optimize the point in time and the order in which different configurations are created and evaluated (by benchmarking a workload sample). UDO uses a cost-based planner to minimize configuration switching overheads. For instance, it aims to amortize the creation of expensive data structures by consecutively evaluating configurations using them. We demonstrate UDO on Postgres as well as MySQL and on TPC-H as well as TPC-C, optimizing a variety of light and heavy parameters concurrently.©VideoSizeß12.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452754&amp;file=3448016.3452754.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452754®KeywordsíŸ$machine learning for data management≤query optimization¶Badges¿•Track´short-paper®Citation®Downloadh©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/LinkW21•titleŸNLogical Schema Design that Quantifies Update Inefficiency and Join Efficiency.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459238©publisher±SIGMOD ConferenceßauthorsíÆSebastian Link™Ziheng Wei®Abstract⁄|The goal of classical normalization is to maintain data consistency under updates, with a minimum level of effort. Given functional dependencies (FDs) alone, this goal is only achievable in the special case an FD-preserving Boyce-Codd Normal Form (BCNF) decomposition exists. As we show, in all other cases the level of effort can be neither controlled nor quantified. In response, we establish the l-Bounded Cardinality Normal Form, parameterized by a positive integer l. For every l, the normal form condition requires from every instance that every value combination over the left-hand side of every non-trivial FD does not occur in more than l tuples. BCNF is captured when l=1. We demonstrate that schemata in this normal form characterize the instances that are i) free from level l data redundancy and update inefficiency, and ii) permit level l join efficiency. We establish algorithms that compute schemata in l-Bounded Cardinality Normal Form for the smallest level l attainable across all FD-preserving decompositions. Additional algorithms i) attain even smaller levels of effort based on the loss of some FDs, and ii) decompose schemata based on prioritized FDs that cause high levels of effort. Our framework informs de-normalization already during logical design. In particular, level l quantifies both the incremental maintenance and join support of materialized views. Experiments with synthetic and real-world data illustrate which properties the schemata have that result from our algorithms, and how these properties predict the performance of update and query operations on instances over the schemata, without and with materialized views.©VideoSizeß39.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459238&amp;file=3448016.3459238.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459238®KeywordsóØdata redundancy≠normalization´normal form∂cardinality constraintµfunctional dependency§join¶update¶Badges¿•Track∞research-article®Citation®DownloadZ©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/MaroulisBPVV21•titleŸ8RawVis: A System for Efficient In-situ Visual Analytics.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452764©publisher±SIGMOD Conferenceßauthorsï∞Stavros Maroulis≠Nikos BikakisµGeorge Papastefanatos±Panos Vassiliadis∞Yannis Vassiliou®Abstract⁄⁄In-situ processing has received a great deal of attention in recent years. In in-situ scenarios, big raw data files which do not fit in main memory, must be efficiently handled on-the-fly using commodity hardware, without the overhead of a preprocessing phase or the loading of data into a database system. This paper presents RawVis, an open source data visualization system for in-situ visual exploration and analytics over big raw data. RawVis implements novel indexing schemes and adaptive processing techniques allowing users to perform efficient visual and analytics operations directly over the data files. RawVis provides real-time interaction, reporting low response time, over large data files, using commodity hardware.©VideoSize®411.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452764&amp;file=3448016.3452764.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452764®Keywordsó¶rawvis∞visual analytics¨big raw data∞data exploration∂big data visualizationŸ!adaptive and progressive indexing≤visualization tool¶Badges¿•Track´short-paper®Citation®DownloadN©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/EppertF021•titleŸHA Tailored Regression for Learned Indexes: Logarithmic Error Regression.§year§2021£doiŸ'https://doi.org/10.1145/3464509.3464891©publisher´aiDM@SIGMODßauthorsì≠Martin Eppert¨Philipp Fent≥Thomas Neumann 0001®Abstract⁄hAlthough linear regressions are essential for learned index structures, most implementations use Simple Linear Regression, which optimizes the squared error. Since learned indexes use exponential search, regressions that optimize the logarithmic error are much better tailored for the use-case. By using this fitting optimization target, we can significantly improve learned index‚Äôs lookup performance with no architectural changes.  While the log-error is harder to optimize, our novel algorithms and optimization heuristics can bring a practical performance improvement of the lookup latency. Even in cases where fast build times are paramount, log-error regressions still provide a robust fallback for degenerated leaf models. The resulting regressions are much better suited for learned indexes, and speed up lookups on data sets with outliers by over a factor of 2.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3464509.3464891®Keywordsê¶Badges¿•Track∞research-article®Citation®DownloadF©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/CubukcuEPSS21•titleŸ>Citus: Distributed PostgreSQL for Data-Intensive Applications.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457551©publisher±SIGMOD Conferenceßauthorsï¨Umur Cubukcu≠Ozgun Erdogan≠Sumedh PathakµSudhakar Sannakkayala™Marco Slot®Abstract⁄
Citus is an open source distributed database engine for PostgreSQL that is implemented as an extension. Citus gives users the ability to distribute data, queries, and transactions in PostgreSQL across a cluster of PostgreSQL servers to handle the needs of data-intensive applications. The development of Citus has largely been driven by conversations with companies looking to scale PostgreSQL beyond a single server and their workload requirements. This paper describes the requirements of four common workload patterns and how Citus addresses those requirements. It also shares benchmark results demonstrating the performance and scalability of Citus in each of the workload patterns and describes how Microsoft uses Citus to address one of its most challenging data problems.©VideoSizeß65.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457551&amp;file=3448016.3457551.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457551®Keywordsî¥distributed database≤database extension≥relational database™postgresql¶Badges¿•Track∞research-article®Citation ®DownloadÕ‚©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/CaoZYLWHCCLFWWS21•titleŸKPolarDB Serverless: A Cloud Native Database for Disaggregated Data Centers.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457560©publisher±SIGMOD Conferenceßauthors‹ ßWei CaoØYingqiang Zhang´Xinjun YangÆFeifei Li 0001™Sheng Wang©Qingda Hu¨Xuntao Cheng¨Zongzhi Chen´Zhenjun Liu©Jing FangßBo Wang™Yuhui Wang´Haiqing SunßZe Yang¨Zhushi Cheng®Sen ChenßJian Wu¶Wei Hu¨Jianwei Zhao™Yusong Gao™Songlu Cai≠Yunyang Zhang¨Jiawang Tong®Abstract⁄6\beginabstract The trend in the DBMS market is to migrate to the cloud for elasticity, high availability, and lower costs. The traditional, monolithic database architecture is difficult to meet these requirements. With the development of high-speed network and new memory technologies, disaggregated data center has become a reality: it decouples various components from monolithic servers into separated resource pools (e.g., compute, memory, and storage) and connects them through a high-speed network. The next generation cloud native databases should be designed for disaggregated data centers. In this paper, we describe the novel architecture of \name, which follows thedisaggregation design paradigm: the CPU resource on compute nodes is decoupled from remote memory pool and storage pool. Each resource pool grows or shrinks independently, providing \revon-demand provisoning at multiple dimensions while improving reliability. We also design our system to mitigate the inherent penalty brought by resource disaggregation, and introduce optimizations such as optimistic locking and index awared prefetching. Compared to the architecture that uses local resources, \name achieves better dynamic resource provisioning capabilities and 5.3 times faster failure recovery speed, while achieving comparable performance. \endabstract©VideoSize¶130 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457560&amp;file=3448016.3457560.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457560®Keywordsî¥shared remote memoryÆcloud databaseπdisaggregated data centerÆshared storage¶Badges¿•Track∞research-article®Citation ®DownloadÕ_©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/WangCDGCSRBCMMR21•titleŸWConsistency and Completeness: Rethinking Distributed Stream Processing in Apache Kafka.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457556©publisher±SIGMOD Conferenceßauthorsú≠Guozhang Wang®Lei ChenØAyusman DikshitØJason Gustafson´Boyang ChenØMatthias J. Sax¨John Roesler≥Sophie Blee-Goldman≠Bruno Cadonna¨Apurva Mehta´Varun MadanßJun Rao®Abstract⁄ÁAn increasingly important system requirement for distributed stream processing applications is to provide strong correctness guarantees under unexpected failures and out-of-order data so that its results can be authoritative (not needing complementary batch results). Although existing systems have put a lot of effort into addressing some specific issues, such as consistency and completeness, how to enable users to make flexible and transparent trade-off decisions among correctness, performance, and cost still remains a practical challenge. Specifically, similar mechanisms are usually applied to tackle both consistency and completeness, which can result in unnecessary performance penalties. We present Apache Kafka's core design for stream processing, which relies on its persistent log architecture as the storage and inter-processor communication layers to achieve correctness guarantees. Kafka Streams, a scalable stream processing client library in Apache Kafka, defines the processing logic as read-process-write cycles in which all processing state updates and result outputs are captured as log appends. Idempotent and transactional write protocols are utilized to guarantee exactly-once semantics. Furthermore, revision-based speculative processing is employed to emit results as soon as possible while handling out-of-order data. We also demonstrate how Kafka Streams behaves in practice with large-scale deployments and performance insights exhibiting its flexible and low-overhead trade-offs.©VideoSizeß79.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457556&amp;file=3448016.3457556.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457556®Keywordsí±stream processing©semantics¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/CaoFLZGZ021•titleŸ7LogStore: A Cloud-Native and Multi-Tenant Log Database.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457565©publisher±SIGMOD ConferenceßauthorsóßWei Cao¨Xiaojie Feng¨Boyuan Liang¨Tianyu Zhang™Yusong Gao≠Yunyang ZhangÆFeifei Li 0001®Abstract⁄8With the prevalence of cloud computing, more and more enterprises are migrating applications to cloud infrastructures. Logs are the key to helping customers understand the status of their applications running on the cloud. They are vital for various scenarios, such as service stability assessment, root cause analysis and user activity profiling. Therefore, it is essential to manage the massive amount of logs collected on the cloud and tap their value. Although various log storages have been widely used in the past few decades, it is still a non-trivial problem to design a cost-effective log storage for cloud applications. It faces challenges of heavy write throughput of tens of millions of log records per second, retrieval on PB-level logs and massive hundreds of thousands of tenants. Traditional log processing systems cannot satisfy all these requirements. To address these challenges, we propose the cloud-native log database LogStore. It combines shared-nothing and shared-data architecture, and utilizes highly scalable and low-cost cloud object storage, while overcoming the bandwidth limitations and high latency of using remote storage when writing a large number of logs. We also propose a multi-tenant management method that physically isolates tenant data to ensure compliance and flexible data expiration policies, and uses a novel traffic scheduling algorithm to mitigate the impact of traffic skew and hotspots among tenants. In addition, we design an efficient column index structure LogBlock to support queries with full-text search, and combined several query optimization techniques to reduce query latency on cloud object storage. LogStore has been deployed in Alibaba Cloud on a large scale (more than 500 machines), processing logs of more than 100 GB per second, and has been running stably for more than two years.©VideoSizeß19.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457565&amp;file=3448016.3457565.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457565®Keywordsó∏shared-data architectureªshared-nothing architecture¨cloud-native¨column store≥full text retrieval´log storage¨multi-tenant¶Badges¿•Track∞research-article®Citation ®DownloadÕ∆©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/PangLCWXW21•titleŸ>ArkDB: A Key-Value Engine for Scalable Cloud Storage Services.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457553©publisher±SIGMOD Conferenceßauthorsñ®Zhu Pang©Qingda Lu©Shuo Chen®Rui Wang©Yikang Xu´Jiesheng Wu®Abstract⁄TPersistent key-value stores play a crucial role in enabling internet-scale services. At Alibaba Cloud, scale-out cloud storage services including Object Storage Service, File Storage Service and Tablestore are built on distributed key-value stores. Key challenges in the design of the underlying key-value engine for these services lie in utilization of disaggregated storage, supporting write and range query-heavy workloads, and balancing of scalability, availability and resource usage. This paper presents ArkDB, a key-value engine designed to address these challenges by combining advantages of both LSM tree and Bw-tree, and leveraging advances in hardware technologies. Built on top of Pangu, an append-only distributed file system, ArkDB's innovations include shrinkable page mapping table, clear separation of system and user states for fast recovery, write amplification reduction, efficient garbage collection and lightweight partition split and merge. Experimental results demonstrate ArkDB's improvements over existing designs. Compared with Bw-tree, ArkDB efficiently stabilizes the mapping table size despite continuous write working set growth. Compared with RocksDB, an LSM tree-based key-value engine, ArkDB increases ingestion throughput by 2.16x, while reducing write amplification by 3.1x. It outperforms RocksDB by 52% and 37% respectively on a write-heavy workload and a range query-intensive workload of the Yahoo! Cloud Serving Benchmark. Experiments running in Tablestore in a cluster environment further demonstrate ArkDB's performance on Pangu and its efficient partition split/merge support.©VideoSizeß32.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457553&amp;file=3448016.3457553.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457553®KeywordsïØkey-value store®recovery≤garbage collection≥transaction logging∫cloud storage architecture¶Badges¿•Track∞research-article®Citation ®DownloadÕí©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/HuangG21•titleŸBNova-LSM: A Distributed, Component-based LSM-tree Key-value Store.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457297©publisher±SIGMOD Conferenceßauthorsí´Haoyu Huang∑Shahram Ghandeharizadeh®Abstract⁄©The cloud infrastructure motivates disaggregation of monolithic data stores into components that are assembled together based on an application's workload. This study investigates disaggregation of an LSM-tree key-value store into components that communicate using RDMA. These components separate storage from processing, enabling processing components to share storage bandwidth and space. The processing components scatter blocks of a file (SSTable) across an arbitrary number of storage components and balance load across them using power-of-d. They construct ranges dynamically at runtime to parallelize compaction and enhance performance. Each component has configuration knobs that control its scalability. The resulting component-based system, Nova-LSM, is elastic. It outperforms its monolithic counterparts, both LevelDB and RocksDB, by several orders of magnitude with workloads that exhibit a skewed pattern of access to data.©VideoSizeß38.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457297&amp;file=3448016.3457297.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457297®Keywordsì§RDMA®LSM-treeøcomponent-based key-value store¶Badges¿•Track∞research-article®Citation ®DownloadÕ\©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/CaiZ0JOZ21•titleŸ@ARM-Net: Adaptive Relation Modeling Network for Structured Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457321©publisher±SIGMOD Conferenceßauthorsñ¨Shaofeng Cai≠Kaiping ZhengÆGang Chen 0001ÆH. V. Jagadish≠Beng Chin Ooi¨Meihui Zhang®Abstract⁄sRelational databases are the de facto standard for storing and querying structured data, and extracting insights from structured data requires advanced analytics. Deep neural networks (DNNs) have achieved super-human prediction performance in particular data types, e.g., images. However, existing DNNs may not produce meaningful results when applied to structured data. The reason is that there are correlations and dependencies across combinations of attribute values in a table, and these do not follow simple additive patterns that can be easily mimicked by a DNN. The number of possible such cross features is combinatorial, making them computationally prohibitive to model. Furthermore, the deployment of learning models in real-world applications has also highlighted the need for interpretability, especially for high-stakes applications, which remains another issue of concern to DNNs. In this paper, we present ARM-Net, an adaptive relation modeling network tailored for structured data, and a lightweight framework ARMOR based on ARM-Net for relational data analytics. The key idea is to model feature interactions with cross features selectively and dynamically, by first transforming the input features into exponential space, and then determining the interaction order and interaction weights adaptively for each cross feature. We propose a novel sparse attention mechanism to dynamically generate the interaction weights given the input tuple, so that we can explicitly model cross features of arbitrary orders with noisy features filtered selectively. Then during model inference, ARM-Net can specify the cross features being used for each prediction for higher accuracy and better interpretability. Our extensive experiments on real-world datasets demonstrate that ARM-Net consistently outperforms existing models and provides more interpretable predictions for data-driven decision making.©VideoSizeß43.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457321&amp;file=3448016.3457321.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457321®Keywordsñ≤feature importance≥feature interaction∞interpretability∫multi-head gated attentionØneural networksØstructured data¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/DayanT21•titleŸ.Chucky: A Succinct Cuckoo Filter for LSM-Tree.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457273©publisher±SIGMOD Conferenceßauthorsí©Niv Dayan¨Moshe Twitto®Abstract⁄
Modern key-value stores typically rely on an LSM-tree in storage (SSD) to handle writes and Bloom filters in memory (DRAM) to optimize reads. With ongoing advances in SSD technology shrinking the performance gap between storage and memory devices, the Bloom filters are now emerging as a performance bottleneck. We propose Chucky, a new design that replaces the multiple Bloom filters by a single Cuckoo filter that maps each data entry to an auxiliary address of its location within the LSM-tree. We show that while such a design entails fewer memory accesses than with Bloom filters, its false positive rate off the bat is higher. The reason is that the auxiliary addresses occupy bits that would otherwise be used as parts of the Cuckoo filter's fingerprints. To address this, we harness techniques from information theory to succinctly encode the auxiliary addresses so that the fingerprints can stay large. As a result, Chucky achieves the best of both worlds: a modest access cost and a low false positive rate at the same time.©VideoSize®209.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457273&amp;file=3448016.3457273.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457273®Keywordsñ¨bloom filter≠cuckoo filterÆHuffman coding∞entropy encoding∏succinct data structures®LSM-tree¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/KimKCYKJ21•titleŸ#Rethink the Scan in MVCC Databases.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452783©publisher±SIGMOD Conferenceßauthorsñ¨Jong-Bin Kim´Kihwang Kim´Hyunsoo Cho™Jaeseon Yu¨Sooyong Kang≠Hyungsoo Jung®Abstract⁄JA scan is one of the fundamental operations in databases for retrieving tuples from tables, and research on access methods has been of importance to query optimization. However, our community is aware of the inconvenient truth that its performance may plummet amid steep increases in search costs when acting on MVCC databases since multi-versioning may forfeit all the benefits of using database indexes. An execution plan for a query on multi-versioned data often comprises a series of point lookup operations, of which each internally executes a linear traversal of record versions. Therefore, the generated plan is surprisingly worse than a full table (or version store) scan, mainly due to redundant access to database pages. To address such an all-or-nothing approach, we propose version weaver (vWeaver), a light-weight access method for record versions, that expedites a scan on record versions with each being augmented by just a few pointer fields. vWeaver incrementally constructs a version search structure over even an append-only version store (e.g., undo space) and allows a scan to traverse new version search structures for fast lookup. We applied vWeaver to in-memory and disk-based MVCC databases and demonstrated that the systems with vWeaver generally improved scan performance under various workloads with negligible space overhead.©VideoSize®142.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452783&amp;file=3448016.3452783.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452783®Keywordsî∞multi-versioningÆdatabase index≠version store§scan¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/ZhouAPC21•titleŸKSpitfire: A Three-Tier Buffer Manager for Volatile and Non-Volatile Memory.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452819©publisher±SIGMOD Conferenceßauthorsî¨Xinjing Zhou´Joy Arulraj¨Andrew Pavlo´David Cohen®Abstract⁄‘The design of the buffer manager in database management systems (DBMSs) is influenced by the performance characteristics of volatile memory (i.e., DRAM) and non-volatile storage (e.g., SSD). The key design assumptions have been that the data must be migrated to DRAM for the DBMS to operate on it and that storage is orders of magnitude slower than DRAM. But the arrival of new non-volatile memory (NVM) technologies that are nearly as fast as DRAM invalidates these previous assumptions. Researchers have recently designed Hymem, a novel buffer manager for a three-tier storage hierarchy comprising of DRAM, NVM, and SSD. Hymem supports cache-line-grained loading and an NVM-aware data migration policy. While these optimizations improve its throughput, Hymem suffers from two limitations. First, it is a single-threaded buffer manager. Second, it is evaluated on an NVM emulation platform. These limitations constrain the utility of the insights obtained using Hymem. In this paper, we present Spitfire, a multi-threaded, three-tier buffer manager that is evaluated on Optane Persistent Memory Modules, an NVM technology that is now being shipped by Intel. We introduce a general framework for reasoning about data migration in a multi-tier storage hierarchy. We illustrate the limitations of the optimizations used in Hymem on Optane and then discuss how Spitfire circumvents them. We demonstrate that the data migration policy has to be tailored based on the characteristics of the devices and the workload. Given this, we present a machine learning technique for automatically adapting the policy for an arbitrary workload and storage hierarchy. Our experiments show that Spitfire works well across different workloads and storage hierarchies.©VideoSize®132.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452819&amp;file=3448016.3452819.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452819®KeywordsîØstorage engines∞database systems±buffer management≥non-volatile memory¶Badges¿•Track∞research-article®Citation ®DownloadÕ¸©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/GuptaYCKEMTS21•titleŸfFast Processing and Querying of 170TB of Genomics Data via a Repeated And Merged BloOm Filter (RAMBO).§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457333©publisher±SIGMOD Conferenceßauthorsò¨Gaurav Gupta´Minghao Yan∞Benjamin Coleman´Bryce Kille≥Ryan A. Leo Elworth≠Tharun Medini∞Todd J. TreangenµAnshumali Shrivastava®Abstract⁄ﬁDNA sequencing, especially of microbial genomes and metagenomes, has been at the core of recent research advances in large-scale comparative genomics. The data deluge has resulted in exponential growth in genomic datasets over the past years and has shown no sign of slowing down. Several recent attempts have been made to tame the computational burden of sequence search on these terabyte and petabyte-scale datasets, including raw reads and assembled genomes. However, no known implementation provides both fast query and construction time, keeps the low false-positive requirement, and offers cheap storage of the data structure. We propose a data structure for search called RAMBO (Repeated And Merged BloOm Filter) which is significantly faster in query time than state-of-the-art genome indexing methods- COBS (Compact bit-sliced signature index), Sequence Bloom Trees, HowDeSBT, and SSBT. Furthermore, it supports insertion and query process parallelism, cheap updates for streaming inputs, has a zero false-negative rate, a low false-positive rate, and a small index size. RAMBO converts the search problem into set membership testing among K documents. Interestingly, it is a count-min sketch type arrangement of a membership testing utility (Bloom Filter in our case). The simplicity of the algorithm and embarrassingly parallel architecture allows us to stream and index a 170TB whole-genome sequence dataset in a mere 9 hours on a cluster of 100 nodes while competing methods require weeks.©VideoSize®161.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457333&amp;file=3448016.3457333.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457333®Keywordsì∑genomic sequence searchµinformation retrieval¨bloom filter¶Badges¿•Track∞research-article®Citation ®DownloadÕ≈©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/ZhangWCJT0Z021•titleŸOResTune: Resource Oriented Tuning Boosted by Meta-Learning for Cloud Databases.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457291©publisher±SIGMOD Conferenceßauthorsò´Xinyi ZhangßHong Wu™Zhuo Chang´Shuowei Jin®Jian TanÆFeifei Li 0001≠Tieying Zhang¨Bin Cui 0001®Abstract⁄éModern database management systems (DBMS) contain tens to hundreds of critical performance tuning knobs that determine the system runtime behaviors. To reduce the total cost of ownership, cloud database providers put in drastic effort to automatically optimize the resource utilization by tuning these knobs. There are two challenges. First, the tuning system should always abide by the service level agreement (SLA) while optimizing the resource utilization, which imposes strict constrains on the tuning process. Second, the tuning time should be reasonably acceptable since time-consuming tuning is not practical for production and online troubleshooting. In this paper, we design ResTune to automatically optimize the resource utilization without violating SLA constraints on the throughput and latency requirements. ResTune leverages the tuning experience from the history tasks and transfers the accumulated knowledge to accelerate the tuning process of the new tasks. The prior knowledge is represented from historical tuning tasks through an ensemble model. The model learns the similarity between the historical workloads and the target, which significantly reduces the tuning time by a meta-learning based approach. ResTune can efficiently handle different workloads and various hardware environments. We perform evaluations using benchmarks and real world workloads on different types of resources. The results show that, compared with the manually tuned configurations, ResTune reduces 65%, 87%, 39% of CPU utilization, I/O and memory on average, respectively. Compared with the state-of-the-art methods, ResTune finds better configurations with up to ~18x speedups.©VideoSize®195.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457291&amp;file=3448016.3457291.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457291®Keywordsî®resourceÆcloud database∑service level agreement¶tuning¶Badges¿•Track∞research-article®Citation ®DownloadÕ∫©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Cui0ZCLO21•titleŸVAlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457324©publisher±SIGMOD ConferenceßauthorsñßCan Cui≠Wei Wang 0059¨Meihui ZhangÆGang Chen 0001¨Zhaojing Luo≠Beng Chin Ooi®Abstract⁄Alphas are stock prediction models capturing trading signals in a stock market. A set of effective alphas can generate weakly correlated high returns to diversify the risk. Existing alphas can be categorized into two classes: Formulaic alphas are simple algebraic expressions of scalar features, and thus can generalize well and be mined into a weakly correlated set. Machine learning alphas are data-driven models over vector and matrix features. They are more predictive than formulaic alphas, but are too complex to mine into a weakly correlated set. In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes. The new alphas predict returns with high accuracy and can be mined into a weakly correlated set. In addition, we propose a novel alpha mining framework based on AutoML, called AlphaEvolve, to generate the new alphas. To this end, we first propose operators for generating the new alphas and selectively injecting relational domain knowledge to model the relations between stocks. We then accelerate the alpha mining by proposing a pruning technique for redundant alphas. Experiments show that AlphaEvolve can evolve initial alphas into the new alphas with high returns and weak correlations.©VideoSizeß27.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457324&amp;file=3448016.3457324.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457324®Keywordsí∞search algorithm∞stock prediction¶Badges¿•Track∞research-article®Citation ®DownloadÕØ©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/WangYGJXLWGLXYY21•titleŸ6Milvus: A Purpose-Built Vector Data Management System.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457550©publisher±SIGMOD Conferenceßauthors‹ ±Jianguo Wang 0001´Xiaomeng Yi´Rentong Guo¨Hai Jin 0001ßPeng Xu´Shengjun Li¨Xiangyu Wang≠Xiangzhou Guo¨Chengming Li™Xiaohai Xu¶Kun Yu´Yuxing Yuan´Yinghao Zou´Jiquan LongØYudong Cai 0002¨Zhenxiang Li≠Zhifeng Zhang®Yihua Mo¶Jun Gu´Ruiyi Jiang¶Yi Wei´Charles Xie®Abstract⁄€Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI &amp; Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.©VideoSize®369.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457550&amp;file=3448016.3457550.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457550®Keywordsï∑heterogeneous computingØvector database∞machine learning¨data scienceŸ"high-dimensional similarity search¶Badges¿•Track∞research-article®Citation ®DownloadÕî©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ZhaoYZLR21•titleŸ'A Learned Sketch for Subgraph Counting.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457289©publisher±SIGMOD Conferenceßauthorsï¨Kangfei Zhao≠Jeffrey Xu Yu©Hao Zhang®Qiyan LißYu Rong®Abstract⁄∞Subgraph counting, as a fundamental problem in network analysis, is to count the number of subgraphs in a data graph that match a given query graph by either homomorphism or subgraph isomorphism. The importance of subgraph counting derives from the fact that it provides insights of a large graph, in particular a labeled graph, when a collection of query graphs with different sizes and labels are issued. The problem of counting is challenging. On one hand, exact counting by enumerating subgraphs is NP-hard. % On the other hand, approximate counting by subgraph isomorphism can only support 3/5-node query graphs over unlabeled graphs. % Another way for subgraph counting is to specify it as an \SQL query and estimate the cardinality of the query in \rdbm. Existing approaches for cardinality estimation can only support subgraph counting by homomorphism up to some extent, as it is difficult to deal with sampling failure when a query graph becomes large. A question that arises is if subgraph counting can be supported by machine learning (ML) and deep learning (DL). The existing DL approach for subgraph isomorphism can only support small data graphs. The ML/DL approaches proposed in \rdbm context for approximate query processing and cardinality estimation cannot be used, as subgraph counting is to do complex self-joins over one relation, whereas existing approaches focus on multiple relations. In this paper, we propose an Active Learned Sketch for Subgraph Counting (\ALSS) with two main components: a sketch learned (≈ÅSS) and an active learner (\AL). The sketch is learned by a neural network regression model, and the active learner is to perform model updates based on new arrival test query graphs. % We conduct extensive experimental studies to confirm the effectiveness and efficiency of \ALSS using large real labeled graphs. Moreover, we show that \ALSS can assist query optimizers to find a better query plan for complex multi-way self-joins.©VideoSize®240.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457289&amp;file=3448016.3457289.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457289®Keywordsí±subgraph counting≠deep learning¶Badges¿•Track∞research-article®Citation ®DownloadÕç©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Fan0XYYZ21•titleŸ"Incrementalizing Graph Algorithms.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452796©publisher±SIGMOD Conferenceßauthorsñ™Wenfei FanÆChao Tian 0001®Ruiqi Xu©Qiang Yin™Wenyuan Yu¨Jingren Zhou®Abstract⁄RIncremental algorithms are important to dynamic graph analyses, but are hard to write and analyze. Few incremental graph algorithms are in place, and even fewer offer performance guarantees. This paper approaches this by proposing to incrementalize existing batch algorithms. We identify a class of incrementalizable algorithms abstracted in a fixpoint model. We show how to deduce an incremental algorithm AŒî from such an algorithm A. Moreover, AŒî can be made bounded relative to A, i.e., its cost is determined by the sizes of changes to graphs and changes to the affected area that is necessarily checked by batch algorithm A. We provide generic conditions under which a deduced algorithm AŒî warrants to be correct and relatively bounded, by adopting the same logic and data structures of A, at most using timestamps as an additional auxiliary structure. Based on these, we show that a variety of graph-centric algorithms can be incrementalized with relative boundedness. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the incrementalized algorithms.©VideoSizeß25.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452796&amp;file=3448016.3452796.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452796®Keywordsì≤incrementalization≤fixpoint algorithm´boundedness¶Badges¿•Track∞research-article®Citation ®DownloadÕ~©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/0001RBMW021•titleŸ4Practical Security and Privacy for Database Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457544©publisher±SIGMOD Conferenceßauthorsñ™Xi He 0001≠Jennie Rogers´Johes Bater∂Ashwin MachanavajjhalaÆChenghong WangÆXiao Wang 0012®Abstract⁄«Computing technology has enabled massive digital traces of our personal lives to be collected and stored. These datasets play an important role in numerous real-life applications and research analysis, such as contact tracing for COVID 19, but they contain sensitive information about individuals. When managing these datasets, privacy is usually addressed as an afterthought, engineered on top of a database system optimized for performance and usability. This has led to a plethora of unexpected privacy attacks in the news. Specialized privacy-preserving solutions usually require a group of privacy experts and they are not directly transferable to other domains. There is an urgent need for a generally trustworthy database system that offers end-to-end security and privacy guarantees. In this tutorial, we will first describe the security and privacy requirements for database systems in different settings and cover the state-of-the-art tools that achieve these requirements. We will also show challenges in integrating these techniques together and demonstrate the design principles and optimization opportunities for these security and privacy-aware database systems. This is designed to be a three hour tutorial.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457544®KeywordsïΩtrusted execution environment®securityßprivacy≤secure computation¥differential privacy¶Badges¿•Track®tutorial®Citation ®DownloadÕ}©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/TingC21•titleªConditional Cuckoo Filters.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452811©publisher±SIGMOD Conferenceßauthorsí´Daniel Ting©Rick Cole®Abstract⁄ÆBloom filters, cuckoo filters, and other approximate set membership sketches have a wide range of applications. Oftentimes, expensive operations can be skipped if an item is not in a data set. These filters provide an inexpensive, memory efficient way to test if an item is in a set and avoid unnecessary operations. Existing sketches only allow membership testing for a single set. However, in some applications such as join processing, the relevant set is not fixed and is determined by a set of predicates. We propose the Conditional Cuckoo Filter, a simple modification of the cuckoo filter that allows for set membership testing given predicates on a pre-computed sketch. This filter also introduces a novel chaining technique that enables cuckoo filters to handle insertion of duplicate keys. We evaluate our methods on a join processing application and show that they significantly reduce the number of tuples that a join must process.©VideoSize®166.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452811&amp;file=3448016.3452811.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452811®Keywordsî©semi-join∫approximate set membershipÆcuckoo filters¨bloom filter¶Badges¿•Track∞research-article®Citation ®DownloadÕv©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/PandeyWXB21•titleŸBTerrace: A Hierarchical Graph Container for Skewed Dynamic Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457313©publisher±SIGMOD ConferenceßauthorsîØPrashant PandeyÆBrian Wheatman®Helen Xu¨Aydin Bulu√ß®Abstract⁄…Various applications model problems as streaming graphs, which need to quickly apply a stream of updates and run algorithms on the updated graph. Furthermore, many dynamic real-world graphs, such as social networks, follow a skewed distribution of vertex degrees, where there are a few high-degree vertices and many low-degree vertices. Existing static graph-processing systems optimized for graph skewness achieve high performance and low space usage by preprocessing a cache-efficient graph partitioning based on vertex degree. In the streaming setting, the whole graph is not available upfront, however, so finding an optimal partitioning is not feasible in the presence of updates. As a result, existing streaming graph-processing systems take a "one-size-fits-all" approach, leaving performance on the table. We present Terrace, a system for streaming graphs that uses a hierarchical data structure design to store a vertex's neighbors in different data structures depending on the degree of the vertex. This multi-level structure enables Terrace to dynamically partition vertices based on their degrees and adapt to skewness in the underlying graph. Our experiments show that Terrace supports faster batch insertions for batch sizes up to 1M when compared to Aspen, a state-of-the-art graph streaming system. On graph query algorithms, Terrace is between 1.7X--2.6X faster than Aspen and between 0.5X--1.3X as fast as Ligra, a state-of-the-art static graph-processing system.©VideoSizeß63.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457313&amp;file=3448016.3457313.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457313®Keywordsì©streaming®indexingµgraph data structures¶Badges¿•Track∞research-article®Citation ®DownloadÕt©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/PandeyCDBFJ21•titleŸNVector Quotient Filters: Overcoming the Time/Space Trade-Off in Filter Design.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452841©publisher±SIGMOD ConferenceßauthorsñØPrashant Pandey´Alex Conway©Joe Durie±Michael A. Bender¥Martin Farach-Colton´Rob Johnson®Abstract⁄›Today's filters, such as quotient, cuckoo, and Morton, have a trade-off between space and speed; even when moderately full (e.g., 50%-75% full), their performance degrades nontrivially. The result is that today's systems designers are forced to choose between speed and space usage. In this paper, we present the vector quotient filter (VQF). Locally, the VQF is based on Robin Hood hashing, like the quotient filter, but uses power-of-two-choices hashing to reduce the variance of runs, and thus offers consistent, high throughput across load factors. Power-of-two-choices hashing also makes it more amenable to concurrent updates, compared to the cuckoo filter and variants. Finally, the vector quotient filter is designed to exploit SIMD instructions so that all operations have O (1) cost, independent of the size of the filter or its load factor. We show that the vector quotient filter is 2√ó faster for inserts compared to the Morton filter (a cuckoo filter variant and state-of-the-art for inserts) and has similar lookup and deletion performance as the cuckoo filter (which is fastest for queries and deletes), despite having a simpler design and implementation. The vector quotient filter has minimal performance decline at high load factors, a problem that has plagued modern filters, including quotient, cuckoo, and Morton. Furthermore, we give a thread-safe version of the vector quotient filter and show that insertion throughput scales 3√ó with four threads compared to a single thread.©VideoSizeß68.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452841&amp;file=3448016.3452841.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452841®Keywordsì∞membership queryßfiltersπdictionary data structure¶Badges¿•Track∞research-article®Citation ®DownloadÕr©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/ZhangSLCY021•titleŸRALG: Fast and Accurate Active Learning Framework for Graph Convolutional Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457325©publisher±SIGMOD Conferenceßauthorsñ¨Wentao ZhangßYu ShenßYang Li≠Lei Chen 0002≠Zhi Yang 0001¨Bin Cui 0001®Abstract⁄æGraph Convolutional Networks (GCNs) have become state-of-the-art methods in many supervised and semi-supervised graph representation learning scenarios. In order to achieve satisfactory performance, GCNs require a sufficient amount of labeled data. However, in real-world scenarios, labeled data is often expensive to obtain. Therefore, we propose ALG, a novel Active Learning framework for GCNs, which employs domain-specific intelligence to achieve much higher performance and efficiency compared to the generic AL frameworks. First, by decoupling GCN models, ALG serves as an effective and efficient AL framework for measuring and combining node representativeness and informativeness. Second, by exploiting the characteristic of the reception field in GCNs, ALG considers both the importance and correlation of nodes by proposing a new node selection metric that maximizes the effective reception field (ERF). We prove that this ERF maximization problem is NP-hard and provide an efficient algorithm accompanied with a provable approximation guarantee. The empirical studies on four public datasets demonstrate that ALG can significantly improve both the performance and efficiency of active learning for GCNs.©VideoSize®163.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457325&amp;file=3448016.3457325.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457325®KeywordsìØreception fieldØactive learningºgraph convolutional networks¶Badges¿•Track∞research-article®Citation ®DownloadÕp©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/Baunsgaard0CDGG21•titleŸ6ExDRa: Exploratory Data Science on Federated Raw Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457549©publisher±SIGMOD Conferenceßauthors‹ ¥Sebastian Baunsgaard≥Matthias Boehm 0001ØAnkit Chaudhary≤Behrouz Derakhshan¥Stefan Gei√üels√∂der≤Philipp M. Grulich≤Michael Hildebrand∞Kevin Innerebner¨Volker MarklÆClaus NeubauerØSarah OsterburgØOlga Ovcharenko≠Sergey Redyuk≠Tobias Rieger∏Alireza Rezaei Mahdiraji∏Sebastian Benjamin Wrede≠Steffen Zeuch®Abstract⁄hData science workflows are largely exploratory, dealing with under-specified objectives, open-ended problems, and unknown business value. Therefore, little investment is made in systematic acquisition, integration, and pre-processing of data. This lack of infrastructure results in redundant manual effort and computation. Furthermore, central data consolidation is not always technically or economically desirable or even feasible (e.g., due to privacy, and/or data ownership). The ExDRa system aims to provide system infrastructure for this exploratory data science process on federated and heterogeneous, raw data sources. Technical focus areas include (1) ad-hoc and federated data integration on raw data, (2) data organization and reuse of intermediates, and (3) optimization of the data science lifecycle, under awareness of partially accessible data. In this paper, we describe use cases, the overall system architecture, selected features of SystemDS' new federated backend (for federated linear algebra programs, federated parameter servers, and federated data preparation), as well as promising initial results. Beyond existing work on federated learning, ExDRa focuses on enterprise federated ML and related data pre-processing challenges. In this context, federated ML has the potential to create a more fine-grained spectrum of data ownership and thus, even new markets.©VideoSizeß42.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457549&amp;file=3448016.3457549.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457549®Keywordsî≤federated learning¨ml pipelines®raw data¨data science¶Badges¿•Track∞research-article®Citation ®DownloadÕk©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/SioulasA21•titleŸ<Scalable Multi-Query Execution using Reinforcement Learning.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452799©publisher±SIGMOD Conferenceßauthorsí≤Panagiotis Sioulas≤Anastasia Ailamaki®Abstract⁄#The growing demand for data-intensive decision support and the migration to multi-tenant infrastructures put databases under the stress of high analytical query load. The requirement for high throughput contradicts the traditional design of query-at-a-time databases that optimize queries for efficient serial execution. Sharing work across queries presents an opportunity to reduce the total cost of processing and therefore improve throughput with increasing query load. Systems can share work either by assessing all opportunities and restructuring batches of queries ahead of execution, or by inspecting opportunities in individual incoming queries at runtime: the former strategy scales poorly to large query counts, as it requires expensive sharing-aware optimization, whereas the latter detects only a subset of the opportunities. Both strategies fail to minimize the cost of processing for large and ad-hoc workloads. This paper presents RouLette, a specialized intelligent engine for multi-query execution that addresses, through runtime adaptation, the shortcomings of existing work-sharing strategies. RouLette scales by replacing sharing-aware optimization with adaptive query processing, and it chooses opportunities to explore and exploit by using reinforcement learning. RouLette also includes optimizations that reduce the adaptation overhead. RouLette increases throughput by 1.6-28.3x, compared to a state-of-the-art query-at-a-time engine, and up to 6.5x, compared to sharing-enabled prototypes, for multi-query workloads based on the schema of TPC-DS.©VideoSize®160.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452799&amp;file=3448016.3452799.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452799®Keywordsî∏multi-query optimization∂reinforcement learning§joinßsharing¶Badges¿•Track∞research-article®Citation ®DownloadÕ`©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/Miao0021•titleŸvRotom: A Meta-Learned Data Augmentation Framework for Entity Matching, Data Cleaning, Text Classification, and Beyond.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457258©publisher±SIGMOD Conferenceßauthorsì≠Zhengjie MiaoØYuliang Li 0001±Xiaolan Wang 0001®Abstract⁄QDeep Learning revolutionizes almost all fields of computer science including data management. However, the demand for high-quality training data is slowing down deep neural nets' wider adoption. To this end, data augmentation (DA), which generates more labeled examples from existing ones, becomes a common technique. Meanwhile, the risk of creating noisy examples and the large space of hyper-parameters make DA less attractive in practice. We introduce Rotom, a multi-purpose data augmentation framework for a range of data management and mining tasks including entity matching, data cleaning, and text classification. Rotom features InvDA, a new DA operator that generates natural yet diverse augmented examples by formulating DA as a seq2seq task. The key technical novelty of Rotom is a meta-learning framework that automatically learns a policy for combining examples from different DA operators, whereby combinatorially reduces the hyper-parameters space. Our experimental results show that Rotom effectively improves a model's performance by combining multiple DA operators, even when applying them individually does not yield performance improvement. With this strength, Rotom outperforms the state-of-the-art entity matching and data cleaning systems in the low-resource settings as well as two recently proposed DA techniques for text classification.©VideoSize®134.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457258&amp;file=3448016.3457258.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457258®Keywordsî≠deep learning±data augmentationØentity matchingØerror detection¶Badges¿•Track∞research-article®Citation ®DownloadÕ]©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/AlotaibiCDM21•titleŸNHADAD: A Lightweight Approach for Optimizing Hybrid Complex Analytics Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457311©publisher±SIGMOD Conferenceßauthorsî≠Rana Alotaibi≠Bogdan Cautis¨Alin DeutschØIoana Manolescu®Abstract⁄'Hybrid complex analytics workloads typically include (i) data management tasks (joins, selections, etc. ), easily expressed using relational algebra (RA)-based languages, and (ii) complex analytics tasks (regressions, matrix decompositions, etc.), mostly expressed in linear algebra (LA) expressions. Such workloads are common in many application areas, including scientific computing, web analytics, and business recommendation. Existing solutions for evaluating hybrid analytical tasks - ranging from LA-oriented systems, to relational systems (extended to handle LA operations), to hybrid systems - either optimize data management and complex tasks separately, exploit RA properties only while leaving LA-specific optimization opportunities unexploited, or focus heavily on physical optimization, leaving semantic query optimization opportunities unexplored. Additionally, they are not able to exploit precomputed (materialized) results to avoid recomputing (part of) a given mixed (RA and/or LA) computation. In this paper, we take a major step towards filling this gap by proposing HADAD, an extensible lightweight approach for optimizing hybrid complex analytics queries, based on a common abstraction that facilitates unified reasoning: a relational model endowed with integrity constraints. Our solution can be naturally and portably applied on top of pure LA and hybrid RA-LA platforms without modifying their internals. An extensive empirical evaluation shows that HADAD yields significant performance gains on diverse workloads, ranging from LA-centered to hybrid.©VideoSize®147.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457311&amp;file=3448016.3457311.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457311®KeywordsîÆlinear algebra•chaseØquery rewritingµintegrity constraints¶Badges¿•Track∞research-article®Citation ®DownloadÕP©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/0010ANHW21•titleŸ1SIA: Optimizing Queries using Learned Predicates.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457262©publisher±SIGMOD Conferenceßauthorsï¨Qi Zhou 0010´Joy Arulraj≥Shamkant B. NavatheÆWilliam Harris™Jinpeng Wu®Abstract⁄
Predicate-centric rules for rewriting queries is a key technique in optimizing queries. These include pushing down the predicate below the join and aggregation operators, or optimizing the order of evaluating predicates. However, many of these rules are only applicable when the predicate uses a certain set of columns. For example, to move the predicate below the join operator, the predicate must only use columns from one of the joined tables. By generating a predicate that satisfies these column constraints and preserves the semantics of the original query, the optimizer may leverage additional predicate-centric rules that were not applicable before. Researchers have proposed syntax-driven rewrite rules and machine learning algorithms for inferring such predicates. However, these techniques suffer from two limitations. First, they do not let the optimizer constrain the set of columns that may be used in the learned predicate. Second, machine learning algorithms do not guarantee that the learned predicate preserves semantics. In this paper, we present SIA, a system for learning predicates while being guided by counter-examples and a verification technique, that addresses these limitations. The key idea is to leverage satisfiability modulo theories to generate counter-examples and use them to iteratively learn a valid, optimal predicate. We formalize this problem by proving the key properties of synthesized predicates. We implement our approach in SIA and evaluate its efficacy and efficiency. We demonstrate that it synthesizes a larger set of valid predicates compared to prior approaches. On a collection of 200 queries derived from the TPC-H benchmark, SIA successfully rewrites 114 queries with learned predicates. 66 of these rewritten queries exhibit more than 2X speed up.©VideoSize®255.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457262&amp;file=3448016.3457262.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457262®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÕH©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/KimCP0HH21•titleŸTVersatile Equivalences: Speeding up Subgraph Query Processing and Subgraph Matching.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457265©publisher±SIGMOD Conferenceßauthorsñ¨Hyunjoon Kim≠Yunyoung Choi´Kunsoo ParkØXuemin Lin 0001≠Seok-Hee Hong≠Wook-Shin Han®Abstract⁄ÍSubgraph query processing (also known as subgraph search) and subgraph matching are fundamental graph problems in many application domains. A lot of efforts have been made to develop practical solutions for these problems. Despite the efforts, existing algorithms showed limited running time and scalability in dealing with large and/or many graphs. In this paper, we propose a new subgraph search algorithm using equivalences of vertices in order to reduce search space: (1) static equivalence of vertices in a query graph that leads to an efficient matching order of the vertices, and (2) dynamic equivalence of candidate vertices in a data graph, which enables us to capture and remove redundancies in search space. These techniques for subgraph search also lead to an improved algorithm for subgraph matching. Experiments show that our approach outperforms state-of-the-art subgraph search and subgraph matching algorithms by up to several orders of magnitude with respect to query processing time.©VideoSize®110.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457265&amp;file=3448016.3457265.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457265®KeywordsïØsubgraph search±subgraph matching≤vertex equivalenceπsubgraph query processingØneighbor-safety¶Badges¿•Track∞research-article®Citation ®DownloadÕB©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/IzenovDRS21•titleŸHCOMPASS: Online Sketch-based Query Optimization for In-Memory Databases.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452840©publisher±SIGMOD Conferenceßauthorsî∞Yesdaulet Izenov´Asoke Datta´Florin RusuÆJun Hyung Shin®Abstract⁄GCost-based query optimization remains a critical task in relational databases even after decades of research and industrial development. Query optimizers rely on a large range of statistical synopses for accurate cardinality estimation. As the complexity of selections and the number of join predicates increase, two problems arise. First, statistics cannot be incrementally composed to effectively estimate the cost of the sub-plans generated in plan enumeration. Second, small errors are propagated exponentially through joins, which can lead to severely sub-optimal plans. In this paper, we introduce COMPASS, a novel query optimization paradigm for in-memory databases based on a single type of statistics---Fast-AGMS sketches. In COMPASS, query optimization and execution are intertwined. Selection predicates and sketch updates are pushed-down and evaluated online during query optimization. This allows Fast-AGMS sketches to be computed only over the relevant tuples---which enhances cardinality estimation accuracy. Plan enumeration is performed over the query join graph by incrementally composing attribute-level sketches---not by building a separate sketch for every sub-plan. We prototype COMPASS in MapD -- an open-source parallel database -- and perform extensive experiments over the complete JOB benchmark. The results prove that COMPASS generates better execution plans -- both in terms of cardinality and runtime -- compared to four other database systems. Overall, COMPASS achieves a speedup ranging from 1.35X to 11.28X in cumulative query execution time over the considered competitors.©VideoSize®118.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452840&amp;file=3448016.3452840.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452840®Keywordsì®sketches¥permutation distanceªjoin cardinality estimation¶Badges¿•Track∞research-article®Citation ®DownloadÕ>©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/FangW0021•titleŸnCohesive Subgraph Search over Big Heterogeneous Information Networks: Applications, Challenges, and Solutions.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457538©publisher±SIGMOD Conferenceßauthorsî¨Yixiang Fang≠Kai Wang 0037ØXuemin Lin 0001±Wenjie Zhang 0001®Abstract⁄ôWith the advent of a wide spectrum of recent applications, querying heterogeneous information networks (HINs) has received a great deal of attention from both academic and industrial societies. HINs involve objects (vertices) and links (edges) that are classified into multiple types; examples include bibliography networks, knowledge networks, and user-item networks in E-business. An important component of these HINs is the cohesive subgraph, or a subgraph containing vertices that are densely connected internally. Searching cohesive subgraphs over HINs has found many real applications, such as community search, product recommendation, fraud detection, and so on. Consequently, how to design effective cohesive subgraph models and how to efficiently search cohesive subgraphs on large HINs become important research topics in the era of big data. In this tutorial, we first highlight the importance of cohesive subgraph search over HINs in various applications and the unique challenges that need to be addressed. Subsequently, we conduct a thorough review of existing works of cohesive subgraph search over HINs. Then, we analyze and compare the models and solutions in these works. Finally, we point out new research directions. We believe that this tutorial not only helps researchers to have a better understanding of existing cohesive subgraph search models and solutions, but also provides them insights for future study.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457538®KeywordsîØdense subgraphs¥heterogeneous graphsŸ"heterogeneous information networks≤cohesive subgraphs¶Badges¿•Track®tutorial®Citation ®DownloadÕ4©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/FuSYJXT021•titleŸbVF<sup>2</sup>Boost: Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457241©publisher±SIGMOD Conferenceßauthorsó¨Fangcheng Fu¨Yingxia ShaoßLele Yu¨Jiawei Jiang´Huanran Xue™Yangyu Tao¨Bin Cui 0001®Abstract⁄ÕWith the ever-evolving concerns on privacy protection, vertical federated learning (FL), where participants own non-overlapping features for the same set of instances, is becoming a heated topic since it enables multiple enterprises to strengthen the machine learning models collaboratively with privacy guarantees. Nevertheless, to achieve privacy preservation, vertical FL algorithms involve complicated training routines and time-consuming cryptography operations, leading to slow training speed. This paper explores the efficiency of the gradient boosting decision tree (GBDT) algorithm under the vertical FL setting. Specifically, we introduce VF^2Boost, a novel and efficient vertical federated GBDT system. Significant solutions are developed to tackle the major bottlenecks. First, to handle the deficiency caused by frequent mutual-waiting in federated training, we propose a concurrent training protocol to reduce the idle periods. Second, to speed up the cryptography operations, we analyze the characteristics of the algorithm and propose customized operations. Empirical results show that our system can be 12.8-18.9 times faster than the existing vertical federated implementations and support much larger datasets.©VideoSizeß40.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457241&amp;file=3448016.3457241.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457241®Keywordsíªvertical federated learningøgradient boosting decision tree¶Badges¿•Track∞research-article®Citation ®DownloadÕ-©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/AbouzourABDMRSS21•titleŸ(Bringing Cloud-Native Storage to SAP IQ.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457563©publisher±SIGMOD Conferenceßauthorsò±Mohammed Abouzour¨G√ºnes Alu√ßÆIvan T. BowmanßXi DengÆNandan MaratheÆSagar Ranadive±Muhammed Sharique∞John C. Smirnios®Abstract⁄°In this paper, we describe our journey of transforming SAP IQ into a relational database management system (RDBMS) that utilizes cheap, elastically scalable object stores on the cloud. SAP IQ is a three-decade old, disk-based, columnar RDBMS that is optimized for complex online analytical processing (OLAP) workloads. Traditionally, SAP IQ has been designed to operate on shared storage devices with strong consistency guarantees (e.g., high-caliber storage area network devices). Therefore, deploying SAP IQ on the cloud, as is, would have meant utilizing storage solutions such as NetApp or AWS EFS that provide a POSIX compliant file interface and strong consistency guarantees, but at a much higher monetary cost. These costs can accumulate easily to diminish the economies of scale that one would expect on the cloud, which can be undesirable. Instead, we have enhanced the design of SAP IQ to operate on cloud object stores such as AWS S3 and Azure Blob Storage. Object stores rely on a weaker consistency model, and potentially have higher latency; however, because of these design trade-offs, they are able to offer (i) better pricing, (ii) enhanced durability, (iii) improved elasticity, and (iv) higher throughput. By enhancing SAP IQ to operate under these design trade-offs, we have unlocked many of the opportunities offered by object stores. More specifically, we have extended SAP IQ's buffer manager and transaction manager, and have introduced a new caching layer that utilizes instance storage on AWS EC2. Experiments using the TPC-H benchmark demonstrate that we can gain an order of magnitude reduction in data-at rest storage costs while improving query and load performance.©VideoSizeß52.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457563&amp;file=3448016.3457563.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457563®Keywordsî≤garbage collection¥cloud-native storage©snapshotsßcaching¶Badges¿•Track∞research-article®Citation ®DownloadÕ,©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/KangJB21•titleŸTJigsaw: A Data Storage and Query Processing Engine for Irregular Table Partitioning.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457547©publisher±SIGMOD Conferenceßauthorsì´Donghe Kang≠Ruochen Jiang≠Spyros Blanas®Abstract⁄#The physical data layout significantly impacts performance when database systems access cold data. In addition to the traditional row store and column store designs, recent research proposes to partition tables hierarchically, starting from either horizontal or vertical partitions and then determining the best partitioning strategy on the other dimension independently for each partition. All these partitioning strategies naturally produce rectangular partitions. Coarse-grained rectangular partitioning reads unnecessary data when a table cannot be partitioned along one dimension for all queries. Fine-grained rectangular partitioning produces many small partitions which negatively impacts I/O performance and possibly introduces a high tuple reconstruction overhead. This paper introduces Jigsaw, a system that employs a novel partitioning strategy that creates partitions with arbitrary shapes, which we refer to as irregular partitions. The traditional tuple-at-a-time or operator-at-a-time query processing models cannot fully leverage the advantages of irregular partitioning, because they may repeatedly read a partition due to its irregular shape. Jigsaw introduces a partition-at-a-time evaluation strategy to avoid repeated accesses to an irregular partition. We implement and evaluate Jigsaw on the HAP and TPC-H benchmarks and find that irregular partitioning is up to 4.2√ófaster than a columnar layout for moderately selective queries. Compared with the columnar layout, irregular partitioning only transfers 21% of the data to complete the same query.©VideoSizeß37.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457547&amp;file=3448016.3457547.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457547®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÕ+©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/WuC21•titleŸWA Unified Deep Model of Learning from both Data and Queries for Cardinality Estimation.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452830©publisher±SIGMOD Conferenceßauthorsí©Peizhi Wu®Gao Cong®Abstract⁄YCardinality estimation is a fundamental problem in database systems. To capture the rich joint data distributions of a relational table, most of the existing work either uses data as unsupervised information or uses query workload as supervised information. Very little work has been done to use both types of information, and cannot fully make use of both types of information to learn the joint data distribution. In this work, we aim to close the gap between data-driven and query-driven methods by proposing a new unified deep autoregressive model, UAE, that learns the joint data distribution from both the data and query workload. First, to enable using the supervised query information in the deep autoregressive model, we develop differentiable progressive sampling using the Gumbel-Softmax trick. Second, UAE is able to utilize both types of information to learn the joint data distribution in a single model. Comprehensive experimental results demonstrate that UAE achieves single-digit multiplicative error at tail, better accuracies over state-of-the-art methods, and is both space and time efficient.©VideoSizeß68.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452830&amp;file=3448016.3452830.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452830®Keywordsî∂cardinality estimation∫deep autoregressive modelsælearning from data and queries∏the gumbel-softmax trick¶Badges¿•Track∞research-article®Citation ®DownloadÕ&©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/Kang0TCSH21•titleŸ`Efficient Deep Learning Pipelines for Accurate Cost Estimations Over Large Scale Query Workload.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457546©publisher±SIGMOD Conferenceßauthorsñ≤Johan Kok Zhi Kang´Gaurav 0004´Sien Yi Tan™Feng Cheng´Shixuan Sun¨Bingsheng He®Abstract⁄¨The use of deep learning models for forecasting the resource consumption patterns of SQL queries have recently been a popular area of study. While these models have demonstrated promising accuracy, training them over large scale industry workloads are expensive. Space inefficiencies of encoding techniques over large numbers of queries and excessive padding used to enforce shape consistency across diverse query plans implies 1) longer model training time and 2) the need for expensive, scaled up infrastructure to support batched training. In turn, we developed Prestroid, a tree convolution based data science pipeline that accurately predicts resource consumption patterns of query traces, but at a much lower cost. We evaluated our pipeline over 19K Presto OLAP queries, on a data lake of more than 20PB of data from Grab. Experimental results imply that our pipeline outperforms benchmarks on predictive accuracy, contributing to more precise resource prediction for large-scale workloads, yet also reduces per-batch memory footprint by 13.5x and per-epoch training time by 3.45x. We demonstrate direct cost savings of up to 13.2x for large batched model training over Microsoft Azure VMs.©VideoSize®186.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457546&amp;file=3448016.3457546.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457546®Keywordsì∞tree convolutionπquery resource allocation≠deep learning¶Badges¿•Track∞research-article®Citation ®DownloadÕ%©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/KoLHLSSH21•titleŸ@iTurboGraph: Scaling and Automating Incremental Graph Analytics.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457243©publisher±SIGMOD Conferenceßauthorsó´Seongyun Ko´Taesung Lee™Kijae Hong´Wonseok Lee¶In Seo©Jiwon Seo≠Wook-Shin Han®Abstract⁄&With the rise of streaming data for dynamic graphs, large-scale graph analytics meets a new requirement of Incremental Computation because the larger the graph, the higher the cost for updating the analytics results by re-execution. A dynamic graph consists of an initial graph G and graph mutation updates Œî G$ of edge insertions or deletions. Given a query Q, its results $Q(G)$, and updates for Œî G$ to G, incremental graph analytics computes updates Œî Q$ such that Q($G \cup Œî G)$ = $Q(G)$ $\cup$ Œî Q$ where $\cup$ is a union operator. In this paper, we consider the problem of large-scale incremental neighbor-centric graph analytics (\NGA ). We solve the limitations of previous systems: lack of usability due to the difficulties in programming incremental algorithms for \NGA and limited scalability and efficiency due to the overheads in maintaining intermediate results for graph traversals in \NGA. First, we propose a domain-specific language, ≈ÅNGA, and develop its compiler for intuitive programming of \NGA, automatic query incrementalization, and query optimizations. Second, we define Graph Streaming Algebra as a theoretical foundation for scalable processing of incremental \NGA. We introduce a concept of Nested Graph Windows and model graph traversals as the generation of walk streams. Lastly, we present a system \SystemName, which efficiently processes incremental \NGA for large graphs. Comprehensive experiments show that it effectively avoids costly re-executions and efficiently updates the analytics results with reduced IO and computations.©VideoSize®245.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457243&amp;file=3448016.3457243.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457243®KeywordsîØgraph analyticsªincremental graph analytics≥distributed systems≠dynamic graph¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/TaranovGH21•titleŸ*CoRM: Compactable Remote Memory over RDMA.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452817©publisher±SIGMOD Conferenceßauthorsì≤Konstantin TaranovµSalvatore Di GirolamoØTorsten Hoefler®Abstract⁄ÌDistributed memory systems are becoming increasingly important since they provide a system-scale abstraction where physically separated memories can be addressed as a single logical one. This abstraction enables memory disaggregation, allowing systems as in-memory databases, caching services, and ephemeral storage to be naturally deployed at large scales. While this abstraction effectively increases the memory capacity of these systems, it faces additional overheads for remote memory accesses. To narrow the difference between local and remote accesses, low latency RDMA networks are a key element for efficient memory disaggregation. However, RDMA acceleration poses new obstacles to efficient memory management and particularly to memory compaction: network controllers and CPUs can concurrently access memory, potentially leading to inconsistencies if memory management operations are not synchronized. To ensure consistency, most distributed memory systems do not provide memory compaction and are exposed to memory fragmentation. We introduce CoRM, an RDMA-accelerated shared memory system that supports memory compaction and ensures strict consistency while providing one-sided RDMA accesses. We show that CoRM sustains high read throughput during normal operations, comparable to similar systems not providing memory compaction while experiencing minimal overheads during compaction. CoRM never disrupts RDMA connections and can reduce applications' active memory up to 6x by performing memory compaction.©VideoSize®336.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452817&amp;file=3448016.3452817.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452817®Keywordsì±memory compaction±memory allocation§rdma¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/TangSMEK21•titleŸHResource-efficient Shared Query Execution via Exploiting Time Slackness.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457282©publisher±SIGMOD Conferenceßauthorsï™Dixin Tang¨Zechao Shang≠William W. MaØAaron J. ElmoreØSanjay Krishnan®Abstract⁄ˇShared query execution can reduce resource consumption by sharing common sub-expressions across concurrent queries. We show that this is not always the case when regularly querying a dataset under change. Depending on latency goals, how eagerly to incrementally process the new data differs. Naively sharing the execution of queries with different latency goals will push the whole shared plan to meet the lowest latency goal and execute more eagerly than each participating query. The overhead introduced by the eager execution can even offset the benefit of shared query execution. We propose an optimization framework iShare to exploit the benefit of shared execution and avoid the overhead of eager execution. iShare judiciously shares queries with different latency goals and selectively executes parts of the share plan lazily. iShare can significantly reduce resource consumption compared to eagerly executing share plans from the state-of-the-art multi-query optimizer or approaches that execute queries separately.©VideoSize®112.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457282&amp;file=3448016.3457282.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457282®KeywordsïÆcloud databaseºincremental view maintenance±scheduled queries∂shared query execution∏multi-query optimization¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/LiMZGR21•titleŸSPutting Things into Context: Rich Explanations for Query Answers using Join Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459246©publisher±SIGMOD Conferenceßauthorsï™Chenjie Li≠Zhengjie Miao´Qitian Zeng¨Boris Glavic´Sudeepa Roy®Abstract⁄…In many data analysis applications there is a need to explain why a surprising or interesting result was produced by a query. Previous approaches to explaining results have directly or indirectly relied on data provenance, i.e., input tuples contributing to the result(s) of interest. However, some information that is relevant for explaining an answer may not be contained in the provenance. We propose a new approach for explaining query results by augmenting provenance with information from other related tables in the database. Using a suite of optimization techniques, we demonstrate experimentally using real datasets and through a user study that our approach produces meaningful results and is efficient.©VideoSizeß29.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459246&amp;file=3448016.3459246.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459246®Keywordsî≤explaining queries¨context-rich∑using relational graphs∏provenance summarization¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£keyŸ%conf/sigmod/Katsogiannis-Meimarakis21•titleŸBA Deep Dive into Deep Learning Approaches for Text-to-SQL Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457543©publisher±SIGMOD ConferenceßauthorsíæGeorge Katsogiannis-Meimarakis∞Georgia Koutrika®Abstract⁄2Data is a prevalent part of every business and scientific domain,but its explosive volume and increasing complexity make data querying challenging even for experts. For this reason, numerous text-to-SQL systems have been developed that enable querying relational databases using natural language. The recent advances on deep neural networks along with the creation of two large datasets specifically made for training text-to-SQL systems, have paved the path for a novel and very promising research area. The purpose of this tutorial is a deep dive into this area, covering state-of-the-art techniques for natural language representation in neural networks,benchmarks that sparked research and competition, recent text-to-SQL systems using deep learning techniques, as well as open problems and research opportunities.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457543®Keywordsí≠deep learning´text-to-SQL¶Badges¿•Track®tutorial®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/MiaoNSYJM021•titleŸMHeterogeneity-Aware Distributed Machine Learning Training via Partial Reduce.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452773©publisher±SIGMOD Conferenceßauthorsó´Xupeng Miao´Xiaonan Nie¨Yingxia Shao≠Zhi Yang 0001¨Jiawei Jiang´Lingxiao Ma¨Bin Cui 0001®Abstract⁄ïAll-reduce is the key communication primitive used in distributed data-parallel training due to the high performance in the homogeneous environment. However, All-reduce is sensitive to stragglers and communication delays as deep learning has been increasingly deployed on the heterogeneous environment like cloud. In this paper, we propose and analyze a novel variant of all-reduce, called partial-reduce, which provides high heterogeneity tolerance and performance by decomposing the synchronous all-reduce primitive into parallel-asynchronous partial-reduce operations. We provide theoretical guarantees, proving that partial-reduce converges to a stationary point at the similar sub-linear rate as distributed SGD. To enforce the convergence of the partial-reduce primitive, we further propose a dynamic staleness-aware distributed averaging algorithm and implement a novel group generation mechanism to prevent possible update isolation in heterogeneous environments. We build a prototype system in the real production cluster and validate its performance under different workloads. The experiments show that it is 1.21x-2x faster than other state-of-the-art baselines.©VideoSizeß18.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452773&amp;file=3448016.3452773.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452773®Keywordsì™all-reduce≠heterogeneityºdistributed machine learning¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/QiCZJZZX21•titleŸ?A Byzantine Fault Tolerant Storage for Permissioned Blockchain.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452744©publisher±SIGMOD Conferenceßauthorsó´Xiaodong Qi´Zhihao ChenØZhao Zhang 0009´Cheqing Jin´Aoying Zhou¨Haizhen Zhuo¨Quangqing Xu®Abstract⁄‰The full-replication data storage mechanism, as commonly utilized in existing blockchains, suffers from poor scalability, since it requires every node to preserve a complete copy of the whole block data locally to tolerant potential Byzantine failures. In a hostile environment, the malicious node may discard or tamper data deliberately. Thus, existing distributed storage method, which partitions data into multiple parts and distributes them over all nodes, cannot suit for blockchains. This demonstration showcases BFT-Store, a novel distributed storage engine for blockchains to break full-replication by integrating erasure coding with Byzantine Fault Tolerance (BFT) consensus protocol. This demonstration will (\romannumeral1) allow audience members to see how BFT-Store partitions block data over all nodes to reduce the storage occupation of system, and (\romannumeral2) allow audience members to see how BFT-Store recovers blocks under distributed scenario even with Byzantine failure.©VideoSize¶107 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452744&amp;file=3448016.3452744.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452744®Keywordsî§pbftÆerasure coding±storage partition™blockchain¶Badges¿•Track´short-paper®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/AntonopoulosKKA21•titleŸDSQL Ledger: Cryptographically Verifiable Data in Azure SQL Database.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457558©publisher±SIGMOD Conferenceßauthorsó∑Panagiotis AntonopoulosÆRaghav Kaushik∞Hanuma KodavallaµSergio Rosales Aceves´Reilly WongÆJason AndersonØJakub Szymaszek®Abstract⁄bSQL Ledger is a new technology that allows cryptographically verifying the integrity of relational data stored in Azure SQL Database and SQL Server. This is achieved by maintaining all historical data in the database and persisting its cryptographic (SHA-256) digests in an immutable, tamper-evident ledger. Digests representing the overall state of the ledger can then be extracted and stored outside of the RDBMS to protect the data from any attacker or high privileged user, including DBAs, system and cloud administrators. The ledger and the historical data are managed transparently, offering protection without any application changes. Historical data is maintained in a relational form to support SQL queries for auditing, forensics and other purposes. SQL Ledger provides cryptographic data integrity guarantees while maintaining the power, flexibility and performance of a commercial RDBMS. In contrast to Blockchain solutions that aim for full integrity, SQL Ledger offers a form of integrity protection known as Forward Integrity. The proposed technology is significantly cheaper and more secure than traditional solutions that establish trust based on audits or mediators, but also has substantial advantages over Blockchain solutions that are complex to deploy, lack data management capabilities and suffer in terms of performance due to their decentralized nature.©VideoSize®193.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457558&amp;file=3448016.3457558.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457558®Keywordsñ¶ledgerªcryptographic verifiability¥integrity protection™blockchain≤data verifiability±database security¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/FuS21•titleŸ&Real-time Data Infrastructure at Uber.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457552©publisher±SIGMOD Conferenceßauthorsí©Yupeng Fu≠Chinmay Soman®Abstract⁄Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.©VideoSizeß35.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457552&amp;file=3448016.3457552.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457552®Keywordsí¥streaming processing∏real-time infrastructure¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/DurnerL021•titleŸ3JSON Tiles: Fast Analytics on Semi-Structured Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452809©publisher±SIGMOD ConferenceßauthorsìÆDominik Durner´Viktor Leis≥Thomas Neumann 0001®Abstract⁄—Developers often prefer flexibility over upfront schema design, making semi-structured data formats such as JSON increasingly popular. Large amounts of JSON data are therefore stored and analyzed by relational database systems. In existing systems, however, JSON's lack of a fixed schema results in slow analytics. In this paper, we present JSON tiles, which, without losing the flexibility of JSON, enables relational systems to perform analytics on JSON data at native speed. JSON tiles automatically detects the most important keys and extracts them transparently - often achieving scan performance similar to columnar storage. At the same time, JSON tiles is capable of handling heterogeneous and changing data. Furthermore, we automatically collect statistics that enable the query optimizer to find good execution plans. Our experimental evaluation compares against state-of-the-art systems and research proposals and shows that our approach is both robust and efficient.©VideoSize®222.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452809&amp;file=3448016.3452809.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452809®Keywordsó¥semi-structured data§olap•jsonb©analytics§jsonßstorage§scan¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/00020T21•titleŸ&Self-adaptive Graph Traversal on GPUs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457279©publisher±SIGMOD Conferenceßauthorsì´Mo Sha 0002ÆYuchen Li 0001¨Kian-Lee Tan®Abstract⁄&GPU's massive computing power offers unprecedented opportunities to enable large graph analysis. Existing studies proposed various preprocessing approaches that convert the input graphs into dedicated structures for GPU-based optimizations. However, these dedicated approaches incur significant preprocessing costs as well as weak programmability to build general graph applications. In this paper, we introduce SAGE, a self-adaptive graph traversal on GPUs, which is free from preprocessing and operates on ubiquitous graph representations directly. We propose Tiled Partitioning and Resident Tile Stealing to fully exploit the computing power of GPUs in a runtime and self-adaptive manner. We also propose Sampling-based Reordering to further optimize the memory efficiency of SAGE through a lightweight and effective node reordering technique on the fly. Extensive experiments demonstrate that SAGE can achieve superior graph traversal performance over existing approaches under different architectural scenarios, i.e., single-GPU, out-of-core, and multi-GPU.©VideoSize®981.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457279&amp;file=3448016.3457279.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457279®Keywordsì∏parallel task scheduling•GPGPU∞graph processing¶Badges¿•Track∞research-article®Citation ®DownloadÕ
©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/PaulLHL21•titleŸHMG-Join: A Scalable Join for Massively Parallel Multi-GPU Architectures.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457254©publisher±SIGMOD Conferenceßauthorsî™Johns Paul≠Shengliang Lu¨Bingsheng HeÆChiew Tong Lau®Abstract⁄√The recent scale-up of GPU hardware through the integration of multiple GPUs into a single machine and the introduction of higher bandwidth interconnects like NVLink 2.0 has enabled new opportunities of relational query processing on multiple GPUs. However, due to the unique characteristics of GPUs and the interconnects, existing hash join implementations spend up to 66% of their execution time moving the data between the GPUs and achieve lower than 50% utilization of the newer high bandwidth interconnects. This leads to extremely poor scalablity of hash join performance on multiple GPUs, which can be slower than the performance on a single GPU. In this paper, we propose MG-Join, a scalable partitioned hash join implementation on multiple GPUs of a single machine. In order to effectively improve the bandwidth utilization, we develop a novel multi-hop routing for cross-GPU communication that adaptively chooses the efficient route for each data flow to minimize congestion. Our experiments on the DGX-1 machine show that MG-Join helps significantly reduce the communication overhead and achieves up to 97% utilization of the bisection bandwidth of the interconnects, resulting in significantly better scalability. Overall, MG-Join outperforms the state-of-the-art hash join implementations by up to 2.5x. MG-Join further helps improve the overall performance of TPC-H queries by up to 4.5x over multi-GPU version of an open-source commercial GPU database Omnisci.©VideoSizeß32.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457254&amp;file=3448016.3457254.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457254®Keywordsî∑multi-gpu architectures©hash join≥network utilization∞distributed join¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/AmiriAA21a•titleŸBPermissioned Blockchains: Properties, Techniques and Applications.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457539©publisher±SIGMOD Conferenceßauthorsì¥Mohammad Javad Amiri±Divyakant Agrawal≠Amr El Abbadi®Abstract⁄6The unique features of blockchains such as immutability, transparency, provenance, and authenticity have been used by many large-scale data management systems to deploy a wide range of distributed applications including supply chain management, healthcare, and crowdworking in permissioned settings. Unlike permissionless settings, e.g., Bitcoin, where the network is public, and anyone can participate without a specific identity, a permissioned blockchain system consists of a set of known, identified nodes that might not fully trust each other. While the characteristics of permissioned blockchains are appealing to a wide range of largescale data management systems, these systems, have to satisfy four main requirements: confidentiality, verifiability, performance, and scalability. Various approaches have been developed in industry and academia to satisfy these requirements with varying assumptions and costs. The focus of this tutorial is on presenting many of these techniques while highlighting the trade-offs among them. We demonstrate the practicality of such techniques in real-life by presenting three different applications, i.e., supply chain management, large-scale databases, and multi-platform crowdworking environments, and show how those techniques can be utilized to meet the requirements of such applications.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457539®Keywordsó´performance´scalability∑permissioned blockchainØconfidentiality≠verifiabilityªlarge-scale data management©consensus¶Badges¿•Track®tutorial®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/FanLLL21•titleŸ.Making Graphs Compact by Lossless Contraction.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452797©publisher±SIGMOD Conferenceßauthorsî™Wenfei Fan™Yuanhao Li™Muyang Liu¶Can Lu®Abstract⁄ùThis paper proposes a scheme to reduce big graphs to small graphs. It contracts obsolete parts, stars, cliques and paths into supernodes. The supernodes carry a synopsis S_Q for each query class Q to abstract key features of the contracted parts for answering queries of Q. The contraction scheme provides a compact graph representation and prioritizes up-to-date data. Better still, it is generic and lossless. We show that the same contracted graph is able to support multiple query classes at the same time, no matter whether their queries are label-based or not, local or non-local. Moreover, existing algorithms for these queries can be readily adapted to compute exact answers by using the synopses when possible, and decontracting the supernodes only when necessary. As a proof of concept, we show how to adapt existing algorithms for subgraph isomorphism, triangle counting and shortest distance to contracted graphs. We also provide an incremental contraction algorithm in response to updates. We experimentally verify that on average, the contraction scheme reduces graphs by 71.2%, and improves the evaluation of these queries by 1.53, 1.42 and 2.14 times, respectively.©VideoSize®399.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452797&amp;file=3448016.3452797.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452797®Keywordsì∞graph algorithms±graph contractionµgraph data management¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/FengML0ZHC21•titleŸ|RisGraph: A Real-Time Streaming System for Evolving Graphs to Support Sub-millisecond Per-update Analysis at Millions Ops/s.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457263©publisher±SIGMOD Conferenceßauthorsó´Guanyu Feng©Zixuan Ma™Daixuan Li±Shengqi Chen 0001´Xiaowei Zhu™Wentao Han≠Wenguang Chen®Abstract⁄÷Evolving graphs in the real world are large-scale and constantly changing, as hundreds of thousands of updates may come every second. Monotonic algorithms such as Reachability and Shortest Path are widely used in real-time analytics to gain both static and temporal insights and can be accelerated by incremental computing. Existing streaming systems adopt the incremental computing model and achieve either low latency or high throughput, but not both. However, both high throughput and low latency are required in real scenarios such as financial fraud detection. This paper presents RisGraph, a real-time streaming system that provides low-latency analysis for each update with high throughput. RisGraph addresses the challenge with localized data access and inter-update parallelism. We propose a data structure named Indexed Adjacency Lists and use sparse arrays and Hybrid Parallel Mode to enable localized data access. To achieve inter-update parallelism, we propose a domain-specific concurrency control mechanism based on the classification of safe and unsafe updates. Experiments show that RisGraph can ingest millions of updates per second for graphs with several hundred million vertices and billions of edges, and the P999 processing time latency is within 20 milliseconds. RisGraph achieves orders-of-magnitude improvement on throughput when analyses are executed for each update without batching and performs better than existing systems with batches of up to 20 million updates.©VideoSize®349.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457263&amp;file=3448016.3457263.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457263®Keywordsì≥monotonic algorithmµincremental computingØstreaming graph¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/YongH0T21•titleŸBEfficient Graph Summarization using Weighted LSH at Billion-Scale.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457331©publisher±SIGMOD Conferenceßauthorsî¨Quinton YongØMahdi HajiabadiπVenkatesh Srinivasan 0001™Alex Thomo®Abstract⁄õSummarizing graphs is of paramount importance due to diverse applications of large-scale graph analysis. A popular family of summarization methods is the group-based approach. The general idea consists of merging nodes of the original graph into supernodes of the summary graph, encoding original edges into superedges/correction set edges, and dropping certain superedges or correction set edges (for lossy summarization). The current state of the art has several steps in its computation that are serious bottlenecks in terms of running time and scalability. In this work, we propose algorithm LDME, a correction set based graph summarization algorithm that produces compact output representations in a fast and scalable manner. To achieve this, we introduce (1) weighted locality sensitive hashing to drastically reduce the number comparisons required to find good node merges, (2) an efficient way to compute the best quality merges that produces more compact outputs, and (3) a new sort-based encoding algorithm that is faster and more robust. More interestingly, our algorithm provides performance tuning settings to allow the option of trading compression for running time. On high compression settings, LDME achieves compression equal to or better than the state of the art with up to 53x speedup in running time. On high speed settings, LDME achieves up to two orders of magnitude speedup with only slightly lower compression.©VideoSizeß44.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457331&amp;file=3448016.3457331.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457331®Keywordsì≥graph summarization¨weighted lsh≤jaccard similarity¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/HuangLT21•titleŸHPoint-to-Hyperplane Nearest Neighbor Search Beyond the Unit Hypersphere.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457240©publisher±SIGMOD Conferenceßauthorsì´Qiang Huang©Yifan Lei≤Anthony K. H. Tung®Abstract⁄ŸPoint-to-Hyperplane Nearest Neighbor Search (P2HNNS) is a fundamental yet challenging problem, and it has plenty of applications in various fields. Existing hyperplane hashing schemes enjoy sub-linear query time and achieve excellent performance on applications such as large-scale active learning with Support Vector Machines (SVMs). However, they only conditionally deal with this problem with a strong assumption that all of the data objects are normalized, located at the unit hypersphere. Those hyperplane hashing schemes may be arbitrarily bad without this assumption. In this paper, we introduce a new asymmetric transformation and develop the first two provable hyperplane hashing schemes, Nearest Hyperplane hashing (NH) and Furthest Hyperplane hashing (FH), for high-dimensional P2HNNS beyond the unit hypersphere. With this asymmetric transformation, we demonstrate that the hash functions of NH and FH are locality-sensitive to the hyperplane queries, and both of them enjoy quality guarantee on query results. Moreover, we propose a data-dependent multi-partition strategy to boost the search performance of FH. NH can perform the hyperplane queries in sub-linear time, while FH enjoys a better practical performance. We evaluate NH and FH over five real-life datasets and show that we are around $3 \sim 100 \times$ faster than the best competitor in four out of five datasets, especially for the recall in $[20%, 80%]$. Code is available at \urlhttps://github.com/HuangQiang/P2HNNS.©VideoSize•40 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457240&amp;file=3448016.3457240.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457240®Keywordsï∏furthest neighbor searchØactive learning∑nearest neighbor searchπasymmetric transformation∫locality-sensitive hashing¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/GkiniBKI21•titleŸ0An In-Depth Benchmarking of Text-to-SQL Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452836©publisher±SIGMOD Conferenceßauthorsî´Orest Gkini±Theofilos Belmpas∞Georgia Koutrika≥Yannis E. Ioannidis®Abstract⁄∂Text-to-SQL systems allow users to explore relational databases by posing free-form queries, alleviating the need for using structured languages, such as SQL. Although numerous systems have been developed so far, existing system evaluations lack in rigour. In this work, we build a text-to-SQL benchmark that covers different classes of queries, and we evaluate the effectiveness of several systems in the field. To evaluate system efficiency, we measure execution time and resource consumption for the different query classes. Our comprehensive evaluation aims at filling in a big gap in understanding the capabilities and boundaries of existing systems and it reveals several open challenges.©VideoSize•17 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452836&amp;file=3448016.3452836.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452836®Keywordsî™evaluation´text-to-sqlªnatural language interfacesØquery benchmark¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/FoufoulasSSI21•titleŸ6Adaptive Compression for Fast Scans on String Columns.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452798©publisher±SIGMOD Conferenceßauthorsî∞Yannis Foufoulas¥Lefteris SidirourgosªElefterios Stamatogiannakis≥Yannis E. Ioannidis®Abstract⁄[State-of-the-art OLAP systems tend to use columnar data representations, as these are both suitable for analytics and amenable to compression. Local dictionary value encoding has been shown to achieve high compression rates for string columns while still allowing fast filtered scans. In this paper, we argue that the effectiveness and efficiency of local dictionary compression is limited by data repetition across file blocks and by dictionary look-ups inside each block during filtered scan execution. To address this problem, we introduce an adaptive compression technique that is based on differential dictionaries and targets both storage efficiency and query performance. The proposed scheme reduces dramatically the need to store repeated values across different file blocks and significantly accelerates read operations by reducing the time needed for dictionary look-ups. A preliminary set of experiments has given very promising results, showing that, in many cases, the proposed new dictionary compression scheme is much more efficient than existing techniques, occasionally up to an order of magnitude.©VideoSizeß28.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452798&amp;file=3448016.3452798.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452798®Keywordsîπdifferential dictionaries§olap∂dictionary compression≠column stores¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ChenFJ0Z21•titleŸQP2H: Efficient Distance Querying on Road Networks by Projected Vertex Separators.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459245©publisher±SIGMOD Conferenceßauthorsï´Zitong ChenØAda Wai-Chee Fu¨Minhao Jiang¨Eric Lo 0001≠Pengfei Zhang®Abstract⁄çThe most efficient known approach for shortest distance querying on road networks is via a tree decomposition based 2-hop labeling index. A major challenge here is how to reduce the query time by reducing the label size. To this end, we propose P2H with the novel ideas of projected vertex separators and optimized selection of vertex separators. We also introduce mechanisms for index maintenance for edge weight updating. Our experiments on multiple real road networks show that P2H can greatly reduce the effective label sizes and query time over existing algorithms. For larger datasets, P2H is around twice as efficient as the best known algorithm.©VideoSizeß27.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459245&amp;file=3448016.3459245.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459245®Keywordsï¶Zitong£Ada§EricßPengfei¶Minhao¶Badges¿•Track∞research-article®Citation ®DownloadÕ©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/LyuZXGWCPYGWLAY21•titleŸHGreenplum: A Hybrid Database for Transactional and Analytical Workloads.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457562©publisher±SIGMOD Conferenceßauthors‹ ¨Zhenghua Lyu±Huan Hubert Zhang™Gang Xiong®Gang Guo¨Haozhou Wang´Jinbao Chen¨Asim PraveenßYu Yang¨Xiaoming GaoÆAlexandra WangßWen LinÆAshwin Agrawal¨Junfeng Yang¶Hao Wu¨Xiaoliang Li®Feng Guo®Jiang Wu´Jesse Zhang≤Venkatesh Raghavan®Abstract⁄9Demand for enterprise data warehouse solutions to support real-time Online Transaction Processing (OLTP) queries as well as long-running Online Analytical Processing (OLAP) workloads is growing. Greenplum database is traditionally known as an OLAP data warehouse system with limited ability to process OLTP workloads. In this paper, we augment Greenplum into a hybrid system to serve both OLTP and OLAP workloads. The challenge we address here is to achieve this goal while maintaining the ACID properties with minimal performance overhead. In this effort, we identify the engineering and performance bottlenecks such as the under-performing restrictive locking and the two-phase commit protocol. Next we solve the resource contention issues between transactional and analytical queries. We propose a global deadlock detector to increase the concurrency of query processing. When transactions that update data are guaranteed to reside on exactly one segment we introduce one-phase commit to speed up query processing. Our resource group model introduces the capability to separate OLAP and OLTP workloads into more suitable query processing mode. Our experimental evaluation on the TPC-B and CH-benCHmark benchmarks demonstrates the effectiveness of our approach in boosting the OLTP performance without sacrificing the OLAP performance.©VideoSizeß35.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457562&amp;file=3448016.3457562.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457562®KeywordsíŸ+hybrid transactional and analytical process®database¶Badges¿•Track∞research-article®Citation ®DownloadÕ ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/Sagadeeva021•titleŸKSliceLine: Fast, Linear-Algebra-based Slice Finding for ML Model Debugging.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457323©publisher±SIGMOD Conferenceßauthorsí≤Svetlana Sagadeeva≥Matthias Boehm 0001®Abstract⁄zSlice finding---a recent work on debugging machine learning (ML) models---aims to find the top-K data slices (e.g., conjunctions of predicates such as gender female and degree PhD), where a trained model performs significantly worse than on the entire training/test data. These slices may be used to acquire more data for the problematic subset, add rules, or otherwise improve the model. In contrast to decision trees, the general slice finding problem allows for overlapping slices. The resulting search space is huge as it covers all subsets of features and their distinct values. Hence, existing work primarily relies on heuristics and focuses on small datasets that fit in memory of a single node. In this paper, we address these scalability limitations of slice finding in a holistic manner from both algorithmic and system perspectives. We leverage monotonicity properties of slice sizes, errors and resulting scores to facilitate effective pruning. Additionally, we present an elegant linear-algebra-based enumeration algorithm, which allows for fast enumeration and automatic parallelization on top of existing ML systems. Experiments with different real-world regression and classification datasets show that effective pruning and efficient sparse linear algebra renders exact enumeration feasible, even for datasets with many features, correlations, and data sizes beyond single node memory.©VideoSizeß42.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457323&amp;file=3448016.3457323.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457323®KeywordsñÆlinear algebraºlarge-scale machine learning∑frequent itemset mining≠slice finding≠data coverageØmodel debugging¶Badges¿•Track∞research-article®Citation ®DownloadÃˇ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/XiaCKHT021•titleŸHDPGraph: A Benchmark Platform for Differentially Private Graph Analysis.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452756©publisher±SIGMOD Conferenceßauthorsñ™Siyuan Xia≠Beizhen Chang™Karl Knopf®Yihan He™Yuchao Tao™Xi He 0002®Abstract⁄πDifferential privacy has become an appealing choice for analyzing sensitive data while offering strong privacy protection, even for complex data types like graphs. Despite a decade of academic efforts in designing differentially private algorithms for graph analysis, few works have been used in practice. This is due to their complexity in the choice of privacy guarantees and parameter/environmental configurations, or due to their scalability issues for large datasets. To bridge the gap between theory and practice, we present DPGraph, a web-based end-to-end benchmark platform built for researchers and practitioners to evaluate private algorithms on graph data. This platform supports a rich set of tunable algorithms for popular graph statistics, such as degree distribution and subgraph counting, with different differential privacy guarantees. A general framework for these algorithms has also been designed for users to tune the algorithms, by changing the sub-algorithms or re-distributing the privacy budget among the sub-algorithms. This enables users to understand the trade-off between privacy, accuracy, and performance of existing work and discover suitable algorithms for their applications.©VideoSize®736.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452756&amp;file=3448016.3452756.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452756®Keywordsì™graph data¥differential privacy¨network data¶Badges¿•Track´short-paper®Citation ®DownloadÃ¯©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Luo00CLQ21•titleŸZSynthesizing Natural Language to Visualization (NL2VIS) Benchmarks from NL2SQL Benchmarks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457261©publisher±SIGMOD Conferenceßauthorsñ®Yuyu Luo≠Nan Tang 0001∞Guoliang Li 0001ØChengliang Chai®Wenbo Li©Xuedi Qin®Abstract⁄rNatural language (NL) is a promising interaction paradigm for data visualization (VIS). However, there are not any NL to VIS (NL2VIS) benchmarks available. Our goal is to provide the first NL2VIS benchmark to enable and push the field of NL2VIS, especially with deep learning technologies. In this paper, we propose a NL2VIS synthesizer (NL2SQL-to-NL2VIS) that synthesizes NL2VIS benchmarks by piggybacking NL2SQL benchmarks. The intuition is based on the semantic connection between SQL queries and VIS queries: SQL queries specify what data is needed and VIS queries additionally need to specify how to visualize. However, different from SQL that has well-defined syntax, VIS languages (e.g., Vega-Lite, VizQL, ggplot2) are syntactically very different. To provide NL2VIS benchmarks that can support many VIS languages, we use a unified intermediate representation, abstract syntax trees (ASTs), for both SQL and VIS queries. We can synthesize multiple VIS trees through adding/deleting nodes to/from an SQL tree. Each VIS tree can then be converted to (any) VIS language. The NL for VIS will be modified based on the NL for SQL to reflect corresponding tree edits. We produce the first NL2VIS benchmark (nvBench), by applying NL2SQL-to-NL2VIS on a popular NL2SQL benchmark Spider, which covers 105 domains, supports seven common types of visualizations, and contains 25,750 (NL, VIS) pairs. Our method reduces the man-hour to 5.7% of developing a NL2VIS benchmark from scratch (or building a NL2VIS benchmark from scratch takes 17.5√ó man-hours of our method). Extensive human validation, through 23 experts and 312 crowd workers, demonstrates the high-quality of nvBench. In order to verify that nvBench can enable learning-based approaches, we develop a SEQ2VIS model. Our experimental results show that SEQ2VIS works well and significantly outperforms the state-of-the-art methods of the NL2VIS task.©VideoSize®226.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457261&amp;file=3448016.3457261.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457261®KeywordsîŸ!natural language to visualization©benchmark≠visualization∫natural language interface¶Badges¿•Track∞research-article®Citation ®DownloadÃˆ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ArenasGS21•titleŸ<Querying in the Age of Graph Databases and Knowledge Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457545©publisher±SIGMOD ConferenceßauthorsìÆMarcelo Arenas∑Claudio Guti√©rrez 0001ØJuan F. Sequeda®Abstract⁄ Graphs have become the best way we know of representing knowledge. The computing community has investigated and developed the support for managing graphs by means of digital technology. Graph databases and knowledge graphs surface as the most successful solutions to this program. This tutorial will provide a conceptual map of the data management tasks underlying these developments, paying particular attention to data models and query languages for graphs©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457545®Keywordsî®queryingØgraph databases∞knowledge graphs´data models¶Badges¿•Track®tutorial®Citation ®DownloadÃÙ©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/XuC21•titleŸ[DIV: Resolving the Dynamic Issues of Zero-knowledge Set Membership Proof in the Blockchain.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457248©publisher±SIGMOD Conferenceßauthorsí©Zihuan Xu®Lei Chen®Abstract⁄ŸZero-knowledge set membership (ZKSM) proof is widely used in blockchain to enable private membership attestation. However, existing mechanisms do not fully consider dynamic issues in the blockchain scenario. Particularly, frequent addition/removal of set elements, not only brings the significant cost to keep public parameters up to date to provers and verifiers but also affects mechanism efficiency (e.g., generation time of the proof and verification, etc.). In this paper, we propose DIV to shard elements on blockchain into independent subsets with the same cardinality to reduce the effect of dynamic issues. However, due to the diverse proof frequency, an improper element-set assignment can result in frequently used elements being easily inferred and corrupted. Thus, we formalize the assignment problem under both element addition and removal cases as two optimization problems and prove their NP-hardness. For each problem, we consider two cases if each element proof frequency is known in advance by the set maintainer or not, and propose solutions with theoretical guarantees. We implement DIV on both Merkle tree and RSA-based ZKSM mechanisms to evaluate its efficiency and effectiveness and apply DIV on a ZKSMbased application named zkSync to demonstrate its applicability. Results show that DIV can achieve O(1) time/space cost on ZKSM under dynamic situations while protecting the information about frequently used elements. It also notably reduces the system latency of zkSync.©VideoSize¶305 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457248&amp;file=3448016.3457248.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457248®Keywordsí™blockchainŸ#zero-knowledge set membership proof¶Badges¿•Track∞research-article®Citation ®DownloadÃÚ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/LiCFMK21•titleŸ?Asynchronous Prefix Recoverability for Fast Distributed Stores.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3458454©publisher±SIGMOD ConferenceßauthorsïÆTianyu Li 0001¥Badrish ChandramouliØJose M. Faleiro≠Samuel MaddenØDonald Kossmann®Abstract⁄eAccessing and updating data sharded across distributed machines safely and speedily in the face of failures remains a challenging problem. Most prominently, applications that share state across different nodes want their writes to quickly become visible to others, without giving up recoverability guarantees in case a failure occurs. Current solutions of a fast cache backed by storage cannot support this use case easily. In this work, we design a distributed protocol, called Distributed Prefix Recovery (DPR) that builds on top of a sharded cache-store architecture with single-key operations, to provide cross-shard recoverability guarantees. With DPR, many clients can read and update shared state at sub-millisecond latency, while receiving periodic prefix durability guarantees. On failure, DPR quickly restores the system to a prefix-consistent state with a novel non-blocking rollback scheme. We added DPR to a key-value store (FASTER) and cache (Redis) and show that we can get high throughput and low latency similar to in-memory systems, while lazily providing durability guarantees similar to persistent stores.©VideoSize®134.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3458454&amp;file=3448016.3458454.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3458454®KeywordsìØkey-value store≤distributed system∞failure recovery¶Badges¿•Track∞research-article®Citation ®DownloadÃÚ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/LiWZZDYY21•titleŸYBuilding Fast and Compact Sketches for Approximately Multi-Set Multi-Membership Querying.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452829©publisher±SIGMOD Conferenceßauthorsó™Rundong Li¨Pinghui Wang´Jiongli Zhu¨Junzhou Zhao¶Jia Di¨Xiaofei Yang¶Kai Ye®Abstract⁄èGiven a set S, Membership Querying (MQ) answers whether a query element $q\in S$. It is a fundamental task in areas like database systems and computer networks. In this paper, we consider a more general problem, Multi-Set Multi-Membership Querying (MS-MMQ). Given n sets $S_0,≈Çdots,S_n-1 $, MS-MMQ answers which sets contain element q. A direct way to address MS-MMQ is to build an MQ structure (e.g., Bloom Filter) for each set. However, the query and space complexities grow linearly with n and become prohibitive for a large n. To address this challenge, we propose a novel Circular Shift and Coalesce (CSC) framework to efficiently achieve approximate MS-MMQ. Instead of building an MQ data structure for each set, the CSC index encodes all n sets into a compact sketch and retrieves only a few bytes in the sketch for a query, which achieves high memory-efficiency and boosts the query speed by several times. CSC is compatible with mainstream data structures for Approximate MQ. We conduct experiments on real-world datasets and results demonstrate that our framework is up to 91.2 times faster and up to 48.9 times more accurate than state-of-the-art methods.©VideoSizeß29.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452829&amp;file=3448016.3452829.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452829®Keywordsì¶sketchºprobabilistic data structure∞membership query¶Badges¿•Track∞research-article®Citation ®DownloadÃÏ©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/0003W0CAL21•titleŸREfficient and Effective Algorithms for Revenue Maximization in Social Advertising.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459243©publisher±SIGMOD Conferenceßauthorsñ¨Kai Han 0003©Benwei WuÆJing Tang 0004™Shuang Cui≠√áigdem AslayµLaks V. S. Lakshmanan®Abstract⁄éWe consider the revenue maximization problem in social advertising, where a social network platform owner needs to select seed users for a group of advertisers, each with a payment budget, such that the total expected revenue that the owner gains from the advertisers by propagating their ads in the network is maximized. Previous studies on this problem show that it is intractable and present approximation algorithms. We revisit this problem from a fresh perspective and develop novel efficient approximation algorithms, both under the setting where an exact influence oracle is assumed and under one where this assumption is relaxed. Our approximation ratios significantly improve upon the previous ones. Furthermore, we empirically show, using extensive experiments on four datasets, that our algorithms considerably outperform the existing methods on both the solution quality and computation efficiency.©VideoSizeß30.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459243&amp;file=3448016.3459243.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459243®Keywordsì©influence≤social advertising∑approximation algorithm¶Badges¿•Track∞research-article®Citation ®DownloadÃÏ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/LuSP0H21•titleŸ9Cache-Efficient Fork-Processing Patterns on Large Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457253©publisher±SIGMOD Conferenceßauthorsï≠Shengliang Lu´Shixuan Sun™Johns PaulÆYuchen Li 0001¨Bingsheng He®Abstract⁄‰As large graph processing emerges, we observe a costly fork-processing pattern (FPP) that is common in many graph algorithms. The unique feature of the FPP is that it launches many independent queries from different source vertices on the same graph. For example, an algorithm in analyzing the network community profile can execute Personalized PageRanks that start from tens of thousands of source vertices at the same time. We study the efficiency of handling FPPs in state-of-the-art graph processing systems on multi-core architectures, including Ligra, Gemini, and GraphIt. We find that those systems suffer from severe cache miss penalty because of the irregular and uncoordinated memory accesses in processing FPPs. In this paper, we propose ForkGraph, a cache-efficient FPP processing system on multi-core architectures. In order to improve the cache reuse, we divide the graph into partitions each sized of LLC (last-level cache) capacity, and the queries in an FPP are buffered and executed on the partition basis. We further develop efficient intra- and inter-partition execution strategies for efficiency. For intra-partition processing, since the graph partition fits into LLC, we propose to execute each graph query with efficient sequential algorithms (in contrast with parallel algorithms in existing parallel graph processing systems) and present an atomic-free query processing method by consolidating contending operations to cache-resident graph partition. For inter-partition processing, we propose two designs, yielding and priority-based scheduling, to reduce redundant work in processing. Besides, we theoretically prove that ForkGraph performs the same amount of work, to within a constant factor, as the fastest known sequential algorithms in FPP queries processing, which is work efficient. Our evaluations on real-world graphs show that ForkGraph significantly outperforms state-of-the-art graph processing systems (including Ligra, Gemini, and GraphIt) with two orders of magnitude speedups.©VideoSize®194.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457253&amp;file=3448016.3457253.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457253®Keywordsî∑raph processing systems∫concurrent query execution∏buffered execution model∑fork-processing pattern¶Badges¿•Track∞research-article®Citation ®DownloadÃÎ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/WangWKL21•titleŸLQuery-by-Sketch: Scaling Shortest Path Graph Queries on Very Large Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452826©publisher±SIGMOD ConferenceßauthorsîßYe Wang©Qing WangØHenning Koehler¶Yu Lin®Abstract⁄ìComputing shortest paths is a fundamental operation in processing graph data. In many real-world applications, discovering shortest paths between two vertices empowers us to make full use of the underlying structure to understand how vertices are related in a graph, e.g. the strength of social ties between individuals in a social network. In this paper, we study the shortest-path-graph problem that aims to efficiently compute a shortest path graph containing exactly all shortest paths between any arbitrary pair of vertices on complex networks. Our goal is to design an exact solution that can scale to graphs with millions or billions of vertices and edges. To achieve high scalability, we propose a novel method, Query-by-Sketch (QbS), which efficiently leverages offline labelling (i.e., precomputed labels) to guide online searching through a fast sketching process that summarizes the important structural aspects of shortest paths in answering shortest-path-graph queries. We theoretically prove the correctness of this method and analyze its computational complexity. To empirically verify the efficiency of QbS, we conduct experiments on 12 real-world datasets, among which the largest dataset has 1.7 billion vertices and 7.8 billion edges. The experimental results show that QbS can answer shortest-path-graph queries in microseconds for million-scale graphs and less than half a second for billion-scale graphs.©VideoSizeß32.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452826&amp;file=3448016.3452826.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452826®Keywordsò´2-hop coverÆshortest paths™algorithmsπpruned landmark labelling¥breadth-first search¨graph sketch≤distance labelling¶graphs¶Badges¿•Track∞research-article®Citation ®DownloadÃÈ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/AkiliW21•titleŸMMuSE Graphs for Flexible Distribution of Event Stream Processing in Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457318©publisher±SIGMOD Conferenceßauthorsí¨Samira Akili±Matthias Weidlich®Abstract⁄˛Complex event processing (CEP) enables reactive and predictive applications through the continuous evaluation of queries over streams of event data. In a network of event sources, efficient query evaluation is achieved through distribution: Queries are split into operators (query decomposition), which are then assigned to some of the nodes (operator placement). Yet, existing solutions limit the decomposition to the operator hierarchy of a query, ignoring possible rewritings of it, and place each operator at exactly one node in the network. That neglects optimizations based on pattern composition through multiple queries as results are always gathered at a single sink node. In this paper, we propose a new evaluation model for CEP, coined Multi-Sink Evaluation (MuSE) graphs. It incorporates arbitrary projections of queries for distribution and assigns them to potentially many nodes. We prove correctness of query evaluation with MuSE graphs and provide a cost model to assess its efficiency. Since the construction of cost-optimal MuSE graphs is intractable, we present an approximation algorithm and several pruning trategies. Our evaluation results show that MuSE graphs reduce network transmission costs by up to three orders of magnitude over baseline strategies.©VideoSize®174.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457318&amp;file=3448016.3457318.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457318®Keywordsì≤query distribution≤operator placement∏complex event processing¶Badges¿•Track∞research-article®Citation ®DownloadÃË©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/MouratidisL021•titleŸoMarrying Top-k with Skyline Queries: Relaxing the Preference Input while Producing Output of Controllable Size.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457299©publisher±SIGMOD Conferenceßauthorsì≥Kyriakos Mouratidis©Keming Li¨Bo Tang 0016®Abstract⁄The two most common paradigms to identify records of preference in a multi-objective setting rely either on dominance (e.g., the skyline operator) or on a utility function defined over the records' attributes (typically, using a top-k query). Despite their proliferation, each of them has its own palpable drawbacks. Motivated by these drawbacks, we identify three hard requirements for practical decision support, namely, personalization, controllable output size, and flexibility in preference specification. With these requirements as a guide, we combine elements from both paradigms and propose two new operators, ORD and ORU. We perform a qualitative study to demonstrate how they work, and evaluate their performance against adaptations of previous work that mimic their output.©VideoSizeß36.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457299&amp;file=3448016.3457299.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457299®Keywordsì∫multi-dimensional datasetsßskyline´top-k query¶Badges¿•Track∞research-article®Citation ®DownloadÃË©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/RuanGWW21•titleŸ(Dynamic Structural Clustering on Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452828©publisher±SIGMOD Conferenceßauthorsî©Boyu Ruan™Junhao Gan¶Hao Wu≠Anthony Wirth®Abstract⁄F\em Structural Clustering ($\strclu$) is one of the most popular graph clustering paradigms. In this paper, we consider $\strclu$ under Jaccard similarity on a dynamic graph, G = (V, E), subject to edge insertions and deletions (updates). The goal is to maintain certain information under updates, so that the strclu clustering result on~G can be retrieved in O(|V| + |E|)$ time, upon request. The state-of-the-art worst-case cost is~O(|V|) per update; we improve this update-time bound \em significantly with the œÅ-approximate notion. Specifically, for a specified failure probability, Œ¥^*, and \em every sequence of~M updates (no need to know M's value in advance), our algorithm, $\dynelm$, achieves~O(?og^2 |V| + og |V| \cdot ?og \fracM ?^* )$ amortized cost for each update, \em at all times in linear space. Moreover, $\dynelm$ provides a provable "sandwich'' guarantee on the clustering quality at all times after each update with probability at least 1 - ^*. We further develop dynelm into our ultimate algorithm, dynstr, which also supports \em cluster-group-by queries. Given Q \subseteq V, this puts the non-empty intersection of Q and each strclu cluster into a distinct group. dynstr not only achieves all the guarantees of dynelm, but also runs \em cluster-group-by queries in~O(|Q|\cdot og |V|) time. We demonstrate the performance of our algorithms via extensive experiments, on 15 real datasets. Experimental results confirm that our algorithms are up to three orders of magnitude more efficient than state-of-the-art competitors, and still provide quality structural clustering results.©VideoSize®111.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452828&amp;file=3448016.3452828.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452828®Keywordsì™algorithmsÆdynamic graphsµstructural clustering¶Badges¿•Track∞research-article®Citation ®DownloadÃÂ©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/ShahPVDCKSWBGGV21•titleŸ.AutoAI-TS: AutoAI for Time Series Forecasting.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457557©publisher±SIGMOD Conferenceßauthorsù∞Syed Yousaf Shah¨Dhaval PatelßLong VuÆXuan-Hong Dang®Bei ChenÆPeter Kirchner∞Horst Samulowitz™David WoodØGregory Bramble±Wesley M. GiffordµGiridhar GanapavarapuÆRoman Vacul√≠n≠Petros Zerfos®Abstract⁄∏A large number of time series forecasting models including traditional statistical models, machine learning models and more recently deep learning have been proposed in the literature. However, choosing the right model along with good parameter values that performs well on a given data is still challenging. Automatically providing a good set of models to users for a given dataset saves both time and effort from using trial-and-error approaches with a wide variety of available models along with parameter optimization. We present AutoAI for Time Series Forecasting (AutoAI-TS) that provides users with a zero configuration (zero-conf) system to efficiently train, optimize and choose best forecasting model among various classes of models for the given dataset. With its flexible zero-conf design, AutoAI-TS automatically performs all the data preparation, model creation, parameter optimization, training and model selection for users and provides a trained model that is ready to use. For given data, AutoAI-TS utilizes a wide variety of models including classical statistical models, Machine Learning (ML) models, statistical-ML hybrid models and deep learning models along with various transformations to create forecasting pipelines. It then evaluates and ranks pipelines using the proposed T-Daub mechanism to choose the best pipeline. The paper describe in detail all the technical aspects of AutoAI-TS along with extensive benchmarking on a variety of real world data sets for various use-cases. Benchmark results show that AutoAI-TS, with no manual configuration from the user, automatically trains and selects pipelines that on average outperform existing state-of-the-art time series forecasting toolkits.©VideoSizeß18.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457557&amp;file=3448016.3457557.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457557®Keywordsï∞machine learning´time series¨ml pipelines¨optimization¶automl¶Badges¿•Track∞research-article®Citation ®DownloadÃÂ©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/0001H21•titleŸDEquiTensors: Learning Fair Integrations of Heterogeneous Urban Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452777©publisher±SIGMOD Conferenceßauthorsí´An Yan 0001©Bill Howe®Abstract⁄
Neural methods are state-of-the-art for urban prediction problems such as transportation resource demand, accident risk, crowd mobility, and public safety. Model performance can be improved by integrating exogenous features from open data repositories (e.g., weather, housing prices, traffic, etc.), but these uncurated sources are often too noisy, incomplete, and biased to use directly. We propose to learn integrated representations, called EquiTensors, from heterogeneous datasets that can be reused across a variety of tasks. We align datasets to a consistent spatio-temporal domain, then describe an unsupervised model based on convolutional denoising autoencoders to learn shared representations. We extend this core integrative model with adaptive weighting to prevent certain datasets from dominating the signal. To combat discriminatory bias, we use adversarial learning to remove correlations with a sensitive attribute (e.g., race or income). Experiments with 23 input datasets and 4 real applications show that EquiTensors could help mitigate the effects of the sensitive information embodied in the biased data. Meanwhile, applications using EquiTensors outperform models that ignore exogenous features and are competitive with "oracle" models that use hand-selected datasets.©VideoSizeß35.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452777&amp;file=3448016.3452777.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452777®KeywordsïØneural networks∞data integrationºspatial-temporal predictions©open dataºfairness in machine learning¶Badges¿•Track∞research-article®Citation ®DownloadÃ„©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/LiGGMP021•titleŸPPRISM: Private Verifiable Set Computation over Multi-Owner Outsourced Databases.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452839©publisher±SIGMOD Conferenceßauthorsñ¶Yin Li±Dhrubajyoti Ghosh≠Peeyush GuptaØSharad Mehrotra¨Nisha Panwar¥Shantanu Sharma 0001®Abstract⁄LThis paper proposes Prism, a secret sharing based approach to compute private set operations (i.e., intersection and union), as well as aggregates over outsourced databases belonging to multiple owners. Prism enables data owners to pre-load the data onto non-colluding servers and exploits the additive and multiplicative properties of secret-shares to compute the above-listed operations in (at most) two rounds of communication between the servers (storing the secret-shares) and the querier, resulting in a very efficient implementation. Also, Prism does not require communication among the servers and supports result verification techniques for each operation to detect malicious adversaries. Experimental results show that Prism scales both in terms of the number of data owners and database sizes, to which prior approaches do not scale.©VideoSize®119.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452839&amp;file=3448016.3452839.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452839®Keywordsîµaggregation operation≥result verification±private set union∏private set intersection¶Badges¿•Track∞research-article®Citation ®DownloadÃ‚©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/ZhouCPWM021•titleŸ)VeriDB: An SGX-based Verifiable Database.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457308©publisher±SIGMOD Conferenceßauthorsñ¨Wenchao Zhou©Yifan Cai¨Yanqing Peng™Sheng Wang•Ke MaÆFeifei Li 0001®Abstract⁄⁄The emergence of trusted hardwares (such as Intel SGX) provides a new avenue towards verifiable database. Such trust hardwares act as an additional trust anchor, allowing great simplification and, in turn, performance improvement in the design of verifiable databases. In this paper, we introduce the design and implementation of VeriDB, an SGX-based verifiable database that supports relational tables, multiple access methods and general SQL queries. Built on top of write-read consistent memory, VeriDB provides verifiable page-structured storage, where results of storage operations can be efficiently verified with low, constant overhead. VeriDB further provides verifiable query execution that supports general SQL queries. Through a series of evaluation using practical workload, we demonstrate that VeriDB incurs low overhead for achieving verifiability: an overhead of 1-2 microseconds for read/write operations, and a 9% - 39% overhead for representative analytical workloads.©VideoSize®119.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457308&amp;file=3448016.3457308.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457308®Keywordsí≥verifiable database£SGX¶Badges¿•Track∞research-article®Citation ®DownloadÃ‚©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/Hu0L21•titleŸ&Accelerating Triangle Counting on GPU.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452815©publisher±SIGMOD Conferenceßauthorsì¶Lin Hu¨Lei Zou 0001¶Yu Liu®Abstract⁄¶Triangle counting is an important problem in graph mining, which has achieved great performance improvement on GPU in recent years. Instead of proposing a new GPU triangle counting algorithm, in this paper, we propose a novel lightweight graph preprocessing method to boost many state-of-the-art GPU triangle counting algorithms without changing their implementations and data structures. Specifically, we find common computing patterns in existing algorithms, and abstract two analytic models to measure how workload imbalance and diversity in these computing patterns affect performance exactly. Then, due to the NP-hardness of the model optimization, we propose approximate solutions by determining edge directions to balance workloads and reordering vertices to maximize the degree of parallelism within GPU blocks. Finally, extensive experiments confirm the significant performance improvement and high usability of our approach.©VideoSize•44 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452815&amp;file=3448016.3452815.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452815®Keywordsî£gpu±triangle countingÆedge directingØvertex ordering¶Badges¿•Track∞research-article®Citation ®DownloadÃﬂ©PaperRefsêã§Infoá§type≠inproceedings£keyºconf/sigmod/WangLLXYWWCYSG21•titleŸXAPAN: Asynchronous Propagation Attention Network for Real-time Temporal Graph Embedding.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457564©publisher±SIGMOD Conferenceßauthorsõ´Xuhong Wang®Ding Lyu´Mengjian Li®Yang XiaßQi Yang´Xinwen Wang≠Xinguang Wang®Ping Cui©Yupu Yang©Bowen Sun™Zhenyu Guo®Abstract⁄KTo capture higher-order structural features, most GNN-based algorithms learn node representations incorporating k-hop neighbors' information. Due to the high time complexity of querying k-hop neighbors, most graph algorithms cannot be deployed in a giant dense temporal network to execute millisecond-level inference. This problem dramatically limits the potential of applying graph algorithms in certain areas, especially financial fraud detection. Therefore, we propose Asynchronous Propagation Attention Network, an asynchronous continuous time dynamic graph algorithm for real-time temporal graph embedding. Traditional graph models usually execute two serial operations: first graph querying and then model inference. Different from previous graph algorithms, we decouple model inference and graph computation to alleviate the damage of the heavy graph query operation to the speed of model inference. Extensive experiments demonstrate that the proposed method can achieve competitive performance while greatly improving the inference speed. The source code is published at a Github repository.©VideoSizeß30.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457564&amp;file=3448016.3457564.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457564®Keywordsì≠dynamic graph±network embeddingµgraph neural networks¶Badges¿•Track∞research-article®Citation ®DownloadÃﬁ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/SunCHH21•titleŸAPathEnum: Towards Real-Time Hop-Constrained s-t Path Enumeration.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457290©publisher±SIGMOD Conferenceßauthorsî´Shixuan Sun´Yuhang Chen¨Bingsheng He™Bryan Hooi®Abstract⁄}We study the hop-constrained s-t path enumeration (HcPE ) problem, which takes a graph G, two distinct vertices s,t and a hop constraint k as input, and outputs all paths from s to t whose length is at most k. The state-of-the-art algorithms suffer from severe performance issues caused by the costly pruning operations during enumeration for the workloads with the large search space. Consequently, these algorithms hardly meet the real-time constraints of many online applications. In this paper, we propose PathEnum, an efficient index-based algorithm towards real-time HcPE. For an input query, PathEnum first builds a light-weight index aiming to reduce the number of edges involved in the enumeration, and develops efficient index-based approaches for enumeration, one based on depth-first search and the other based on joins. We further develop a query optimizer based on a join-based cost model to optimize the search order. We conduct experiments with 15 real-world graphs. Our experiment results show that PathEnum outperforms the state-of-the-art approaches by orders of magnitude in terms of the query time, throughput and response time.©VideoSize•31 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457290&amp;file=3448016.3457290.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457290®KeywordsìØhop-constrained¥s-t path enumeration≤light-weight index¶Badges¿•Track∞research-article®Citation ®DownloadÃ›©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/ZhongYLT0021•titleŸ,BurstSketch: Finding Bursts in Data Streams.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452775©publisher±SIGMOD Conferenceßauthorsñ´Zheng Zhong®Shen Yan®Zikun Li´Decheng TanÆTong Yang 0003¨Bin Cui 0001®Abstract⁄ëBurst is a common pattern in data streams which is characterized by a sudden increase in terms of arrival rate followed by a sudden decrease. Burst detection has attracted extensive attention from the research community. In this paper, we propose a novel sketch, namely BurstSketch, to detect bursts accurately in real time. BurstSketch first uses the technique Running Track to select potential burst items efficiently, and then monitors the potential burst items and capture the key features of burst pattern by a technique called Snapshotting. Experimental results show that our sketch achieves a 1.75 times higher recall rate than the strawman solution.©VideoSize®571.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452775&amp;file=3448016.3452775.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452775®Keywordsì´data stream•burst¶sketch¶Badges¿•Track∞research-article®Citation ®DownloadÃ⁄©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/TianoBN21•titleŸ-FeatTS: Feature-based Time Series Clustering.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452757©publisher±SIGMOD Conferenceßauthorsì¨Donato TianoØAngela Bonifati™Raymond Ng®Abstract⁄>Clustering time series is a recurrent problem in real-life applications involving data science and data analytics pipelines. Existing time series clustering algorithms are ineffective for feature-rich real-world time series since they only compare the time series based on raw data or use a fixed set of features for determining the similarity. In this paper, we showcase FeatTS, a feature-based semi-supervised clustering framework addressing the above issues for variable-length and heterogeneous time series. Specifically, FeatTS leverages a graph encoding of the time series that is obtained by considering a high number of significant extracted features. It then employs community detection and builds upon a Co-Occurrence matrix in order to unify all the best clustering results. We let the user explore the various steps of FeatTS by visualizing the initial data, its graph encoding and its division into communities along with the obtained clusters. We show how the user can interact with the process for the choice of the features and for varying the percentage of input labels and the various parameters. In view of its characteristics, FeatTS outperforms the state of the art clustering methods and is the first to be able to digest domain-specific time series such as healthcare time series, while still being robust and scalable.©VideoSize®169.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452757&amp;file=3448016.3452757.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452757®Keywordsî≤features selection∫semi-supervised clusteringªclustering for data science≥community detection¶Badges¿•Track´short-paper®Citation ®DownloadÃŸ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/WenH0ZKX21•titleŸNRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452779©publisher±SIGMOD Conferenceßauthorsñ¨Qingsong Wen¶Kai HeÆLiang Sun 0001ÆYingying Zhang¶Min KeßHuan Xu®Abstract⁄ΩPeriodicity detection is a crucial step in time series tasks, including monitoring and forecasting of metrics in many areas, such as IoT applications and self-driving database management system. In many of these applications, multiple periodic components exist and are often interlaced with each other. Such dynamic and complicated periodic patterns make the accurate periodicity detection difficult. In addition, other components in the time series, such as trend, outliers and noises, also pose additional challenges for accurate periodicity detection. In this paper, we propose a robust and general framework for multiple periodicity detection. Our algorithm applies maximal overlap discrete wavelet transform to transform the time series into multiple temporal-frequency scales such that different periodic components can be isolated. We rank them by wavelet variance, and then at each scale detect single periodicity by our proposed Huber-periodogram and Huber-ACF robustly. We rigorously prove the theoretical properties of Huber-periodogram and justify the use of Fisher's test on Huber-periodogram for periodicity detection. To further refine the detected periods, we compute unbiased autocorrelation function based on Wiener-Khinchin theorem from Huber-periodogram for improved robustness and efficiency. Experiments on synthetic and real-world datasets show that our algorithm outperforms other popular ones for both single and multiple periodicity detection.©VideoSize®151.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452779&amp;file=3448016.3452779.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452779®Keywordsñ¥multiple periodicity≥database monitoring´periodogram£acf´time seriesµperiodicity detection¶Badges¿•Track∞research-article®Citation ®DownloadÃÿ©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/ChenH21•titleŸ;TSExplain: Surfacing Evolving Explanations for Time Series.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452769©publisher±SIGMOD Conferenceßauthorsí©Yiru Chen™Silu Huang®Abstract⁄ıUnderstanding the underlying explanations for what has happened is more and more crucial in today's business decision-making processes. Existing explanation engines focus on explaining the difference between two given sets. However, for time-series, the explanations usually evolve as time advances. Thus, only considering two end timestamps would miss all explanations in between. To mitigate this, we demonstrate TSExplain, a system to help users understand the underlying evolving explanations for any aggregated time-series. Internally, TSExplain models the explanation problem as a segmentation problem over the time dimension and uses existing works on two-sets diff as building blocks. In our demonstration, conference attendees will be able to easily and interactively explore the evolving explanations and visualize how these explanations contribute to the overall changes in various datasets: COVID-19, S&amp;P500, Iowa Liquor Sales. Questions-like "which states make COVID-19 total confirmed case number go up dramatically during the past year?", "which stocks drive the dramatic crashes of S&amp;P500 in Mar and the quick rebound later?", and "how does Liquor sales trend look like from January 2020 till now and why"-can all get well-answered by TSExplain.©VideoSize®223.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452769&amp;file=3448016.3452769.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452769®Keywordsì∞data explanation¨segmentation´time series¶Badges¿•Track´short-paper®Citation ®DownloadÃ◊©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/Kim21•titleŸ9Boosting Graph Similarity Search through Pre-Computation.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452780©publisher±SIGMOD Conferenceßauthorsë™Jongik Kim®Abstract⁄?Graph similarity search is to retrieve all graphs from a graph database whose graph edit distance (GED) to a query graph is within a given threshold. As GED computation is NP-hard, existing solutions adopt the filtering-and-verification framework, where the main focus is on the filtering phase to reduce the number of GED verifications. However, existing filtering techniques have inherently limited filtering capabilities, and suffer from a large number of GED verifications. To address the problem, in this paper, we propose a fundamentally different approach that utilizes pre-computed GEDs between data graphs in the filtering phase. Based on the approach, we develop a novel search framework Nass, which substantially reduces the verification workload. Because the efficiency of GED computation is essential in GED pre-computation, not to mention the verification of candidate graphs, we also propose an efficient GED computation algorithm as a part of Nass. We conduct extensive experiments on real datasets, and show Nass significantly outperforms the state-of-the art solutions.©VideoSize®130.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452780&amp;file=3448016.3452780.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452780®Keywordsì≥graph edit distanceØpre-computation∑graph similarity search¶Badges¿•Track∞research-article®Citation ®DownloadÃ◊©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/RavindraG21•titleŸ2De-anonymization Attacks on Neuroimaging Datasets.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457234©publisher±SIGMOD ConferenceßauthorsíØVikram Ravindra¨Ananth Grama®Abstract⁄tAdvances in imaging technologies, combined with inexpensive storage, have led to an explosion in the volume of publicly available neuroimaging datasets. Effective analyses of these images hold the potential for uncovering mechanisms that govern functioning of the human brain, and understanding various neurological diseases and disorders. The potential significance of these studies notwithstanding, a growing concern relates to the protection of privacy and confidentiality of subjects who participate in these studies. In this paper, we present a de-anonymization attack rooted in the innate uniqueness of the structure and function of the human brain. We show that the attack reveals not only the identity of an individual, but also the efficacy with which they performing cognitive tasks. Our attack relies on novel matrix analyses techniques that are used to extract discriminating features in neuroimages. These features correspond to individual-specific signatures that can be matched across datasets for highly accurate identification. We present data preprocessing, signature extraction, and matching techniques that are computationally inexpensive, and can scale to large datasets. We characterize the efficacy of our de-anonymization attacks on publicly available databases. Finally, we discuss implications of the attack and challenges associated with defending against such attacks.©VideoSizeß83.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457234&amp;file=3448016.3457234.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457234®KeywordsíŸ"privacy concerns in medical images∑de-anonymization attack¶Badges¿•Track∞research-article®Citation ®DownloadÃ’©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/BeedkarQM21•titleŸ+Compliant Geo-distributed Query Processing.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3453687©publisher±SIGMOD Conferenceßauthorsì∞Kaustubh Beedkar∫Jorge-Arnulfo Quian√©-Ruiz¨Volker Markl®Abstract⁄ëIn this paper, we address the problem of compliant geo-distributed query processing. In particular, we focus on dataflow policies that impose restrictions on movement of data across geographical or institutional borders. Traditional ways to distributed query processing do not consider such restrictions and therefore in geo-distributed environments may lead to non-compliant query execution plans. For example, an execution plan for a query over data sources from Europe, North America, and Asia, which may otherwise be optimal, may not comply with dataflow policies as a result of shipping some restricted (intermediate) data. We pose this problem of compliance in the setting of geo-distributed query processing. We propose a compliance-based query optimizer that takes into account dataflow policies, which are declaratively specified using our policy expressions, to generate compliant geo-distributed execution plans. Our experimental study using a geo-distributed adaptation of the TPC-H benchmark data indicates that our optimization techniques are effective in generating efficient compliant plans and incur low overhead on top of traditional query optimizers.©VideoSize•38 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3453687&amp;file=3448016.3453687.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3453687®Keywordsñ∞data constraintsŸ declarative policy specification±dataflow policies™compliance≤query optimizationªdistributed data processing¶Badges¿•Track∞research-article®Citation ®DownloadÃ’©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/0002SSK21•titleŸWCombining Aggregation and Sampling (Nearly) Optimally for Approximate Query Processing.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457277©publisher±SIGMOD Conferenceßauthorsî≠Xi Liang 0002ÆStavros Sintos¨Zechao ShangØSanjay Krishnan®Abstract⁄€Sample-based approximate query processing (AQP) suffers from many pitfalls such as the inability to answer very selective queries and unreliable confidence intervals when sample sizes are small. Recent research presented an intriguing solution of combining materialized, pre-computed aggregates with sampling for accurate and more reliable AQP. We explore this solution in detail in this work and propose an AQP physical design called PASS, or Precomputation-Assisted Stratified Sampling. PASS builds a tree of partial aggregates that cover different partitions of the dataset. The leaf nodes of this tree form the strata for stratified samples. Aggregate queries whose predicates align with the partitions (or unions of partitions) are exactly answered with a depth-first search, and any partial overlaps are approximated with the stratified samples. We propose an algorithm for optimally partitioning the data into such a data structure with various practical approximation techniques.©VideoSize®156.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457277&amp;file=3448016.3457277.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457277®Keywordsì¥aggregate estimationºapproximate query processingÆdata sketching¶Badges¿•Track∞research-article®Citation ®DownloadÃ‘©PaperRefsêã§Infoá§type≠inproceedings£keyøconf/sigmod/Thirumuruganathan21•titleŸRTo Intervene or Not To Intervene: Cost based Intervention for Combating Fake News.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452778©publisher±SIGMOD ConferenceßauthorsìªSaravanan ThirumuruganathanØMichael SimpsonµLaks V. S. Lakshmanan®Abstract⁄CSocial media platforms provide valuable and powerful means with which users can share content, comment, and communicate. They also suffer from abuse through the dissemination of fake news and misinformation. While a fair amount of work has been done on detecting fake news, on the complementary problem of limiting its propagation, progress has been modest. Once an item is detected as fake, a social media company can intervene on the item and take an appropriate action, including hard intervention (e.g., removing an account) and soft intervention (e.g., labeling the item as "suspicious"). Given that fake news detectors are not 100% reliable, we study the problem of developing a cost aware intervention policy which decides whether to intervene based on the truthiness and popularity of the item. Our solution, Solomon, consists of three modular components - truthiness estimation, popularity estimation (with and without intervention), and intervention policy. Our extensive experiments on real and fake news from multiple domains show that Solomon can perform effective intervention.©VideoSizeß21.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452778&amp;file=3448016.3452778.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452778®Keywordsï™truthiness¨social media©fake news™popularity¨intervention¶Badges¿•Track∞research-article®Citation ®DownloadÃ”©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/JiJ21•titleŸXA-Tree: A Dynamic Data Structure for Efficiently Indexing Arbitrary Boolean Expressions.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457266©publisher±SIGMOD Conferenceßauthorsí™Shuping Ji≤Hans-Arno Jacobsen®Abstract⁄éEfficiently evaluating a large number of arbitrary Boolean expressions is needed in many applications such as advertising exchanges, complex event processing, and publish/subscribe systems. However, most solutions can support only conjunctive Boolean expression matching. The limited number of solutions that can directly work on arbitrary Boolean expressions present performance and flexibility limitations. Moreover, normalizing arbitrary Boolean expressions into conjunctive forms and then using existing methods for evaluating such expressions is not effective because of the potential exponential increase in the size of the expressions. Therefore, we propose the A-Tree data structure to efficiently index arbitrary Boolean expressions. A-Tree is a multirooted tree, in which predicates and subexpressions from different arbitrary Boolean expressions are aggregated and shared. A-Tree employs dynamic self-adjustment policies to adapt itself as the workload changes. Moreover, A-Tree adopts different event matching optimizations. Our comprehensive experiments show that A-Tree-based matching outperforms existing arbitrary Boolean expression matching algorithms in terms of memory use, matching time, and index construction time by up to 71%, 99% and 75%, respectively. Even on conjunctive expression workloads, A-Tree achieves a lower matching time than state-of-the-art conjunctive expression matching algorithms.©VideoSize®376.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457266&amp;file=3448016.3457266.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457266®KeywordsïΩarbitrary boolean expressions±publish/subscribe©algorithmÆdata structure∏complex event processing¶Badges¿•Track∞research-article®Citation ®DownloadÃ“©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/LinCZ21•titleŸCTENET: Joint Entity and Relation Linking with Coherence Relaxation.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457280©publisher±SIGMOD Conferenceßauthorsì´Xueling Lin≠Lei Chen 0002≠Chaorui Zhang®Abstract⁄áThe joint entity and relation linking task aims to connect the noun phrases (resp., relational phrases) extracted from natural language documents to the entities (resp., predicates) in general knowledge bases (KBs). This task benefits numerous downstream systems, such as question answering and KB population. Previous works on entity and relation linking rely on the global coherence assumption, i.e., entities and predicates within the same document are highly correlated with each other. However, this assumption is not always valid in many real-world scenarios. Due to KB incompleteness or data sparsity, sparse coherence among the entities and predicates within the same document is common. Moreover, there may exist isolated entities or predicates that are not related to any other linked concepts. In this paper, we propose TENET, a joint entity and relation linking technique, which relaxes the coherence assumption in an unsupervised manner. Specifically, we formulate the joint entity and relation linking task as a minimum-cost rooted tree cover problem on the knowledge coherence graph constructed based on the document. We then propose effective approximation algorithms with pruning strategies to solve this problem and derive the linking results. Extensive experiments on real-world datasets demonstrate the superior effectiveness and efficiency of our method against the state-of-the-art techniques.©VideoSize®234.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457280&amp;file=3448016.3457280.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457280®KeywordsìÆknowledge base∞relation linkingÆentity linking¶Badges¿•Track∞research-article®Citation ®DownloadÃœ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/VenturaKQM21•titleŸSExpand your Training Limits! Generating Training Data for ML-based Data Management.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457286©publisher±SIGMOD Conferenceßauthorsî±Francesco Ventura™Zoi Kaoudi∫Jorge-Arnulfo Quian√©-Ruiz¨Volker Markl®Abstract⁄ÕMachine Learning (ML) is quickly becoming a prominent method in many data management components, including query optimizers which have recently shown very promising results. However, the low availability of training data (i.e., large query workloads with execution time or output cardinality as labels) widely limits further advancement in research and compromises the technology transfer from research to industry. Collecting a labeled query workload has a very high cost in terms of time and money due to the development and execution of thousands of realistic queries/jobs. In this work, we face the problem of generating training data for data management components tailored to users' needs. We present DataFarm, an innovative framework for efficiently generating and labeling large query workloads. We follow a data-driven white-box approach to learn from pre-existing small workload patterns, input data, and computational resources. Our framework allows users to produce a large heterogeneous set of realistic jobs with their labels, which can be used by any ML-based data management component. We show that our framework outperforms the current state-of-the-art both in query generation and label estimation using synthetic and real datasets. It has up to 9x better labeling performance, in terms of R2 score. More importantly, it allows users to reduce the cost of getting labeled query workloads by 54x (and up to an estimated factor of 104x) compared to standard approaches.©VideoSize®310.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457286&amp;file=3448016.3457286.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457286®Keywordsî∞machine learningØactive learning∑learned data management±data augmentation¶Badges¿•Track∞research-article®Citation ®DownloadÃœ©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/ArasuCGGKPRRRSS21•titleŸ+FastVer: Making Data Integrity a Commodity.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457312©publisher±SIGMOD Conferenceßauthorsù¨Arvind Arasu¥Badrish ChandramouliØJohannes Gehrke™Esha GhoshØDonald Kossmann≤Jonathan ProtzenkoØRavi Ramamurthy≥Tahina Ramananandro≠Aseem Rastogi≥Srinath T. V. Setty¨Nikhil Swamy≥Alexander van Renen¶Min Xu®Abstract⁄ÅWe present FastVer, a high-performance key-value store with strong data integrity guarantees. FastVer is built as an extension of FASTER, an open-source, high-performance key-value store. It offers the same key-value API as FASTER plus an additional verify() method that detects if an unauthorized attacker tampered with the database and checks whether results of all read operations are consistent with historical updates. FastVer is based on a novel approach that combines the advantages of Merkle trees and deferred memory verification. We show that this approach achieves one to two orders of magnitudes higher throughputs than traditional approaches based on either Merkle trees or memory verification. We have formally proven the correctness of our approach in a proof assistant, ensuring that verify() detects any inconsistencies, except if a collision can be found on a cryptographic hash.©VideoSize®143.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457312&amp;file=3448016.3457312.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457312®KeywordsñØmemory checkingµcryptographic hashingÆdata integrity´merkle tree≤key-value database´concurrency¶Badges¿•Track∞research-article®Citation ®DownloadÃÃ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/GiladPM21•titleŸESynthesizing Linked Data Under Cardinality and Integrity Constraints.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457242©publisher±SIGMOD Conferenceßauthorsì™Amir Gilad¨Shweta Patwa∂Ashwin Machanavajjhala®Abstract⁄≤The generation of synthetic data is useful in multiple aspects, from testing applications to benchmarking to privacy preservation. Generating thelinks between relations, subject tocardinality constraints (CCs) andintegrity constraints (ICs) is an important aspect of this problem. Given instances of two relations, where one has a foreign key dependence on the other and is missing its foreign key ($FK$) values, and two types of constraints: (1) CCs that apply to the join view and (2) ICs that apply to the table with missing $FK$ values, our goal is to impute the missing $FK$ values such that the constraints are satisfied. We provide a novel framework for the problem based on declarative CCs and ICs. We further show that the problem is NP-hard and propose a novel two-phase solution that guarantees the satisfaction of the ICs. Phase I yields an intermediate solution accounting for the CCs alone, and relies on a hybrid approach based on CC types. For one type, the problem is modeled as an Integer Linear Program. For the others, we describe an efficient and accurate solution. We then combine the two solutions. Phase II augments this solution by incorporating the ICs and uses a coloring of the conflict hypergraph to infer the values of the $FK$ column. Our extensive experimental study shows that our solution scales well when the data and number of constraints increases. We further show that our solution maintains low error rates for the CCs.©VideoSize®455.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457242&amp;file=3448016.3457242.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457242®KeywordsìØdata generation¥database constraints´linked data¶Badges¿•Track∞research-article®Citation ®DownloadÃ…©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/LernerB21•titleŸ?Not your Grandpa's SSD: The Era of Co-Designed Storage Devices.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457540©publisher±SIGMOD ConferenceßauthorsíÆAlberto LernerØPhilippe Bonnet®Abstract⁄iGone is the time when a Solid-State Drive (SSD) was just a fast drop-in replacement for a Hard-Disk Drive (HDD). Thanks to the NVMe ecosystem, nowadays, SSDs are accessed through specific interfaces and modern I/O frameworks. SSDs have also grown versatile with time and can now support various use cases ranging from cold, high-density storage to hot, low-latency ones. The body of knowledge about building such different devices is mostly available, but it is less than accessible to non-experts. Finding which device variation can better support a given workload also requires deep domain knowledge. This tutorial's first goal is to make these tasks--understanding the design of SSDs and pairing them with the data-intensive workloads they support well--more inviting. The tutorial goes further, however, in that it suggests that a new kind of SSD plays an essential role in post-Moore computer systems. These devices can be co-designed to align their capabilities to an application's requirements. A salient feature of these devices is that they can run application logic besides just storing data. They can thus gracefully scale processing capabilities with the volume of data stored. The tutorial's second goal is thus to establish the design space for co-designed SSDs and show the tools available to hardware, systems, and databases researchers that wish to explore this space.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457540®Keywordsí≠ssd co-design¥database performance¶Badges¿•Track®tutorial®Citation ®DownloadÃ«©PaperRefsêã§Infoá§type≠inproceedings£keyºconf/sigmod/ArroyueloHNRRS21•titleŸ2Worst-Case Optimal Graph Joins in Almost No Space.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457256©publisher±SIGMOD ConferenceßauthorsñØDiego Arroyuelo´Aidan HoganØGonzalo NavarroØJuan L. Reutter¥Javiel Rojas-Ledesma¨Adri√°n Soto®Abstract⁄We present an indexing scheme that supports worst-case optimal (wco) joins over graphs within compact space. Supporting all possible wco joins using conventional data structures - based on B(+)-Trees, tries, etc. - requires 6 index orders in the case of graphs represented as triples. We rather propose a form of index, which we call a ring, that indexes each triple as a set of cyclic bidirectional strings of length 3. Rather than maintaining 6 orderings, we can use one ring to index them all. This ring replaces the graph and uses only sublinear extra space on top of the graph; in order words, the ring supports worst-case optimal graph joins in almost no space beyond storing the graph itself. We perform experiments using our representation to index a large graph (Wikidata) in memory, over which wco join algorithms are implemented. Our experiments show that the ring offers the best overall performance for query times while using only a small fraction of the space when compared with several state-of-the-art approaches.©VideoSize¶174 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457256&amp;file=3448016.3457256.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457256®KeywordsñÆgraph patternsπburrows-wheeler transform≠wavelet treesØgraph databases∏worst-case optimal joinsÆgraph indexing¶Badges¿•Track∞research-article®Citation ®DownloadÃ∆©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/LinTLCW21•titleŸtDon't Look Back, Look into the Future: Prescient Data Partitioning and Migration for Deterministic Database Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452827©publisher±SIGMOD Conferenceßauthorsï´Yu-Shan Lin™Ching Tsai©Tz-Yu LinØYun-Sheng Chang¨Shan-Hung Wu®Abstract⁄Deterministic database systems have been shown to significantly improve the availability and scalability of a distributed database system deployed on a shared-nothing architecture across WAN while ensuring strong consistency. However, their scalability and performance advantages highly depend on the quality of data partitioning due to the reduced flexibility in transaction processing. Although a deterministic database system can employ workload driven data (re-)partitioning and live data migration algorithms to partition data, we found that the effectiveness of these algorithms is limited in complex real-world environments due to the unpredictability of machine workloads. In this paper, we present Hermes, a deterministic database system prototype that, for the first time, does not rely on sophisticated data partitioning to achieve high scalability and performance. Hermes employs a novel transaction routing mechanism that jointly optimizes the balance of machine workloads, data (re-)partitioning, and live data migration by looking into the queued transactions to be executed in the near future. We conducted extensive experiments which show that Hermes is able to yield 29% to 137% increase in transaction throughput as compared to the state-of-the-art systems under complex real-world workloads.©VideoSize®616.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452827&amp;file=3448016.3452827.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452827®Keywordsî∂deterministic databaseÆdata migrationµthe prescient routing±data partitioning¶Badges¿•Track∞research-article®Citation ®DownloadÃ∆©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/HasaniTK021•titleŸOShahin: Faster Algorithms for Generating Explanations for Multiple Predictions.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457332©publisher±SIGMOD Conferenceßauthorsî´Sona HasaniªSaravanan Thirumuruganathan´Nick KoudasØGautam Das 0001®Abstract⁄FMachine learning (ML) models have achieved widespread adoption in the last few years. Generating concise and accurate explanations often increases user trust and understanding of the model prediction. Usually, the implementations of popular explanation algorithms are highly optimized for a single prediction. In practice, explanations often have to be generated in a batch for multiple predictions at a time. To the best of our knowledge, there has been no work for efficiently generating explanations for more than one prediction. While one could use multiple machines to generate explanations in parallel, this approach is sub-optimal as it does not leverage higher-level optimizations that are available in a batch setting. We propose a principled and lightweight approach for identifying redundant computations and several effective heuristics for dramatically speeding up explanation generation. Our techniques are general and could be applied to a wide variety of perturbation based explanation algorithms. We demonstrate this over a diverse set of algorithms including, LIME, Anchor, and SHAP. Our empirical experiments show that our methods impose very little overhead and require minimal modification to the explanation algorithms. They achieve significant speedup over baseline approaches that generate explanations in a sequential manner.©VideoSizeß21.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457332&amp;file=3448016.3457332.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457332®Keywordsñ∞machine learning¶anchor∏multi-query optimization¨explanations§shap§lime¶Badges¿•Track∞research-article®Citation ®DownloadÃ≈©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/HertzschuchMLMW21•titleŸ@Small Selectivities Matter: Lifting the Burden of Empty Samples.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452805©publisher±SIGMOD Conferenceßauthorsñ∞Axel HertzschuchØGuido MoerkotteØWolfgang Lehner™Norman May¨Florian Wolf´Lars Fricke®Abstract⁄cEvery year more and more advanced approaches to cardinality estimation are published, using learned models or other data and workload specific synopses. In contrast, the majority of commercial in-memory systems still relies on sampling. It is arguably the most general and easiest estimator to implement. While most methods do not seem to improve much over sampling-based estimators in the presence of non-selective queries, sampling struggles with highly selective queries due to limitations of the sample size. Especially in situations where no sample tuple qualifies, optimizers fall back to basic heuristics that ignore attribute correlations and lead to large estimation errors. In this work, we present a novel approach, dealing with these 0-Tuple Situations. It is ready to use in any DBMS capable of sampling, showing a negligible impact on optimization time. Our experiments on real world and synthetic data sets demonstrate up to two orders of magnitude reduced estimation errors. Enumerating single filter predicates according to our estimates reveals 1.3 to 1.8 times faster query responses for complex filters.©VideoSize§1 GB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452805&amp;file=3448016.3452805.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452805®Keywordsñ®sampling©in-memory§olap±small selectivity±beta distributionπfilter predicate ordering¶Badges¿•Track∞research-article®Citation ®DownloadÃƒ©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/ShanghooshabadK21•titleŸ5PGMJoins: Random Join Sampling with Graphical Models.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457302©publisher±SIGMOD ConferenceßauthorsñºAli Mohammadi Shanghooshabad∞Meghdad Kurmanji™Qingzhi Ma±Michael ShekelyanÆMehrdad Almasi≥Peter Triantafillou®Abstract⁄1Modern databases face formidable challenges when called to join (several) massive tables. Joins (especially when entailing many-to-many joins) are very time- and resource-consuming, join results can be too big to keep in memory, and performing analytics/learning tasks over them costs dearly in terms of time, resources, and money (in the cloud). Moreover, although random sampling is a promising idea to mitigate the above problems, the current state of the art leaves lots of room for improvements. With this paper we contribute a principled solution, coined PGMJoins. PGMJoins adapts Probabilistic Graphical Models to deriving provably random samples of the join result for (n-way) key joins, many-to-many joins, and cyclic and acyclic joins. PGMJoins contributes optimizations both for deriving the structure of the graph and for PGM inference. It also contributes a novel Sum-Product Message Passing Algorithm (SP-MPA) to make a uniform sample of the joint distribution (join result) efficiently and a novel way to deal with cyclic joins. Despite the use of PGMs, the learned joint distribution is not approximated, and the uniform samples are drawn from the true distribution. Our experimentation using queries and datasets from TPC-H, JOB, TPC-DS, and Twitter shows PGMJoins to outperform the state of the art (by 2X-28X).©VideoSizeß35.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457302&amp;file=3448016.3457302.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457302®Keywordsì∏PGMs for data managementØrandom sampling¨join queries¶Badges¿•Track∞research-article®Citation ®DownloadÃ¡©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/HoPW21•titleŸ1QuTE: Answering Quantity Queries from Web Tables.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452763©publisher±SIGMOD Conferenceßauthorsì≠Vinh Thinh Ho¨Koninika PalÆGerhard Weikum®Abstract⁄Quantities are financial, technological, physical and other measures that denote relevant properties of entities, such as revenue of companies, energy efficiency of cars or distance and brightness of stars and galaxies. Queries with filter conditions on quantities are an important building block for downstream analytics, and pose challenges when the content of interest is spread across a huge number of web tables and other ad-hoc datasets. Search engines support quantity lookups, but largely fail on quantity filters. The QuTE system presented in this paper aims to overcome these problems. It comprises methods for automatically extracting entity-quantity facts from web tables, as well as methods for online query processing, with new techniques for query matching and answer ranking.©VideoSize®461.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452763&amp;file=3448016.3452763.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452763®Keywordsì∂information extractionØquantity search™web tables¶Badges¿•Track´short-paper®Citation ®DownloadÃ¿©PaperRefsêã§Infoá§type≠inproceedings£keyªconf/sigmod/AugustineSER021•titleŸeA Generalized Approach for Reducing Expensive Distance Calls for A Broad Class of Proximity Problems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457303©publisher±SIGMOD ConferenceßauthorsïÆJees Augustine≠Suraj Shetiya∑Mohammadreza Esfandiari∞Senjuti Basu RoyØGautam Das 0001®Abstract⁄çIn this paper, we revisit a suite of popular proximity problems (such as, KNN, clustering, minimum spanning tree) that repeatedly perform distance computations to compare distances during their execution. Our effort here is to design principled solutions to minimize distance computations for such problems in general metric spaces, especially for the scenarios where calling an expensive oracle to resolve unknown distances are the dominant cost of the algorithms for these problems. We present a suite of techniques, including a novel formulation of the problem, that studies how distance comparisons between objects could be modelled as a system of linear inequalities that assists in saving distance computations, multiple graph based solutions, as well as a practitioners guide to adopt our solution frameworks to proximity problems. We compare our designed solutions conceptually and empirically with respect to a broad range of existing works. We finally present a comprehensive set of experimental results using multiple large scale real-world datasets and a suite of popular proximity algorithms to demonstrate the effectiveness of our proposed approaches.©VideoSize®559.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457303&amp;file=3448016.3457303.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457303®Keywordsñ∫k-nearest neighbour graphs∂minimum spanning trees¨metric space≤proximity problems∑lower bound computation™clustering¶Badges¿•Track∞research-article®Citation ®DownloadÃø©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/KimKFH21•titleŸÄCombining Sampling and Synopses with Worst-Case Optimal Runtime and Quality Guarantees for Graph Pattern Cardinality Estimation.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457246©publisher±SIGMOD Conferenceßauthorsî≠Kyoungmin Kim´Hyeonji KimØGeorge Fletcher≠Wook-Shin Han®Abstract⁄ÓGraph pattern cardinality estimation is the problem of estimating the number of embeddings of a query graph in a data graph. This fundamental problem arises, for example, during query planning in subgraph matching algorithms. There are two major approaches to solving the problem: sampling and synopsis. Synopsis (or summary)-based methods are fast and accurate if synopses capture information of graphs well. However, these methods suffer from large errors due to loss of information during summarization and inherent assumptions. Sampling-based methods are unbiased but suffer from large estimation variance due to large sample space. To address these limitations, we propose Alley, a hybrid method that combines both sampling and synopses. Alley employs 1) a novel sampling strategy, random walk with intersection, which effectively reduces the sample space, 2) branching to further reduce variance, and 3) a novel mining approach that extracts and indexes tangled patterns as synopses which are inherently difficult to estimate by sampling. By using them in the online estimation phase, we can effectively reduce the sample space while still ensuring unbiasedness. We establish that Alley has worst-case optimal runtime and approximation quality guarantees for any given error bound Œµ and required confidence Œº. In addition to the theoretical aspect of Alley, our extensive experiments show that Alley outperforms the state-of-the-art methods by up to orders of magnitude higher accuracy with similar efficiency.©VideoSize®114.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457246&amp;file=3448016.3457246.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457246®Keywordsî®sampling™graph data≤worst-case optimal∂cardinality estimation¶Badges¿•Track∞research-article®Citation ®DownloadÃø©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/ChenX0MQZ21•titleŸ?Hybrid Evaluation for Distributed Iterative Matrix Computation.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452843©publisher±SIGMOD Conferenceßauthorsñ™Zihao Chen¨Chen Xu 0001ÆJuan Soto 0001¨Volker Markl¨Weining Qian´Aoying Zhou®Abstract⁄?Distributed matrix computation is common in large-scale data processing and machine learning applications. Existing systems that support distributed matrix computation already explore incremental evaluation for iterative-convergent algorithms. However, they are oblivious to the fact that non-zero increments are scattered in different blocks in a distributed environment. Additionally, we observe that incremental evaluation does not always outperform full evaluation. To address these issues, we propose matrix reorganization to optimize the physical layout upon the state-of-art optimized partition schemes, and thereby accelerate the incremental evaluation. More importantly, we propose a hybrid evaluation to efficiently interleave full and incremental evaluation during the iterative process. In particular, it employs a cost model to compare the overhead costs of two types of evaluations and a selective comparison mechanism to reduce the overhead incurred by comparison itself. To demonstrate the efficiency of our techniques, we implement HyMAC, a hybrid matrix computation system based on SystemML. Our experiments show that HyMAC reduces execution time on large datasets by 23% on average in comparison to the state-of-art optimization technique and consequently outperforms SystemML, ScaLAPACK, and SciDB by an order of magnitude.©VideoSize®192.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452843&amp;file=3448016.3452843.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452843®Keywordsì©iteration≤matrix computation±hybrid evaluation¶Badges¿•Track∞research-article®Citation ®DownloadÃæ©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/Gou021•titleŸ^Sliding Window-based Approximate Triangle Counting over Streaming Graphs with Duplicate Edges.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452800©publisher±SIGMOD Conferenceßauthorsí≠Xiangyang Gou¨Lei Zou 0001®Abstract⁄AStreaming graph analysis is gaining importance in various fields due to the natural dynamicity in many real graph applications. However, approximately counting triangles in real-world streaming graphs with edge duplication and expiration remains an unsolved problem. In this paper, we propose SWTC algorithm to address approximate sliding-window triangle counting problem in streaming graphs with edge duplication. In SWTC, we propose a fixed-length slicing strategy that addresses both unbiased sampling and cardinality estimation issues with a bounded memory usage. We theoretically prove the superiority of our method in sample graph size and estimation accuracy under given memory upper bound. Extensive experiments also confirm that our approach has higher accuracy compared with the baseline method under the same memory usage.©VideoSizeß30.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452800&amp;file=3448016.3452800.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452800®Keywordsì∞streaming graphs∂approximate algorithms±triangle counting¶Badges¿•Track∞research-article®Citation ®DownloadÃº©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/WangWX21•titleŸ(Interactive Search for One of the Top-k.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457322©publisher±SIGMOD Conferenceßauthorsì≠Weicheng WangµRaymond Chi-Wing WongßMin Xie®Abstract⁄∞When a large dataset is given, it is not desirable for a user to read all tuples one-by-one in the whole dataset to find satisfied tuples. The traditional top-k query finds the best k tuples (i.e., the top-k tuples) w.r.t. the user's preference. However, in practice, it is difficult for a user to specify his/her preference explicitly. We study how to enhance the top-k query with user interaction. Specifically, we ask a user several questions, each of which consists of two tuples and asks the user to indicate which one s/he prefers. Based on the feedback, the user's preference is learned implicitly and one of the top-k tuples w.r.t. the learned preference is returned. Here, instead of directly following the top-k query to return all the top-k tuples, since it requires heavy user effort during the interaction (e.g., answering many questions), we reduce the output size to strike for a trade-off between the user effort and the output size. To achieve this, we present an algorithm 2D-PI which asks an asymptotically optimal number of questions in a 2-dimensional space, and two algorithms HD-PI and RH with provable performance guarantee in a d-dimensional space (d &gt;= 2), where they focus on the number of questions asked and the execution time, respectively. Experiments were conducted on synthetic and real datasets, showing that our algorithms outperform existing ones by asking fewer questions within less time to return satisfied tuples.©VideoSizeß30.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457322&amp;file=3448016.3457322.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457322®Keywordsì∞user interaction´top-k queryÆdata analytics¶Badges¿•Track∞research-article®Citation ®DownloadÃº©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/MhedhbiLKWS21•titleŸ-LSQB: a large-scale subgraph query benchmark.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464516©publisher±GRADES-NDA@SIGMODßauthorsï≠Amine Mhedhbi≤Matteo LissandriniÆLaurens Kuiper´Jack Waudby∞G√°bor Sz√°rnyas®Abstract⁄—We introduce LSQB, a new large-scale subgraph query benchmark. LSQB tests the performance of database management systems on an important class of subgraph queries overlooked by existing benchmarks. Matching a labelled structural graph pattern, referred to as subgraph matching, is the focus of LSQB. In relational terms, the benchmark tests DBMSs' join performance as a choke-point since subgraph matching is equivalent to multi-way joins between base Vertex and base Edge tables on ID attributes. The benchmark focuses on read-heavy workloads by relying on global queries which have been ignored by prior benchmarks. Global queries, also referred to as unseeded queries, are a type of queries that are only constrained by labels on the query vertices and edges. LSQB contains a total of nine queries and leverages the LDBC social network data generator for scalability. The benchmark gained both academic and industrial interest and is used internally by 5+ different vendors.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464516®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃπ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/SongGHW21•titleŸ9On Saving Outliers for Better Clustering over Noisy Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457271©publisher±SIGMOD Conferenceßauthorsî´Shaoxu SongßFei Gao≠Ruihong Huang™Yihan Wang®Abstract⁄Clustering is often distracted by errors, frequently observed in almost all areas, ranging from online questionnaire to sensor reading in IoT. The dirty data values not only make themselves (the corresponding tuples) outlying, but also mislead the clustering of remaining tuples, e.g., mistakenly splitting a cluster into two or distorting the cluster center. The reason is that the traditional clustering methods either simply ignore the outliers such as DBSCAN or assign them to the closest clusters anyway, e.g., in K-Means. In this paper, we propose to save the outliers for better clustering. The idea is to adjust the erroneous values (often minimally) of the outlier in order to make it appear normally. That is, the tuples after adjusting values are no longer outlying, and thus will be clustered without distracting others. The outlier saving by value adjustment is designed to work with any clustering methods (e.g., DBSCAN or K-Means). Our technical contributions include: (1) showing NPhardness of the outlier saving problem for clustering, (2) deriving lower and upper bounds of the optimal solutions, and (3) devising approximation algorithm with performance guarantees referring to the aforesaid bounds. Experiments on datasets with real-world outliers demonstrate the higher accuracy of our proposal, compared to the state-of-the-art approaches. Remarkably, we show that the adjusted data with outlier saving indeed improve significantly clustering, as well as other applications such as classification and record matching.©VideoSizeß23.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457271&amp;file=3448016.3457271.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457271®KeywordsíÆoutlier saving™clustering¶Badges¿•Track∞research-article®Citation ®DownloadÃπ©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/0004WZZQHDG21•titleŸ8Bidirectionally Densifying LSH Sketches with Empty Bins.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452833©publisher±SIGMOD Conferenceßauthorsò≠Peng Jia 0004¨Pinghui Wang¨Junzhou Zhao™Shuo Zhang®Yiyan Qi¶Min Hu©Chao Deng≠Xiaohong Guan®Abstract⁄ÕAs an efficient tool for approximate similarity computation and search, Locality Sensitive Hashing (LSH) has been widely used in many research areas including databases, data mining, information retrieval, and machine learning. Classical LSH methods typically require to perform hundreds or even thousands of hashing operations when computing the LSH sketch for each input item (e.g., a set or a vector); however, this complexity is still too expensive and even impractical for applications requiring processing data in real-time. To address this issue, several fast methods such as OPH and BCWS have been proposed to efficiently compute the LSH sketches; however, these methods may generate many sketches with empty bins, which may introduce large errors for similarity estimation and also limit their usage for fast similarity search. To solve this issue, we propose a novel densification method, i.e., BiDens. Compared with existing densification methods, our BiDens is more efficient to fill a sketch's empty bins with values of its non-empty bins in either the forward or backward directions. Furthermore, it also densifies empty bins to satisfy the densification principle (i.e., the LSH property). Theoretical analysis and experimental results on similarity estimation, fast similarity search, and kernel linearization using real-world datasets demonstrate that our BiDens is up to 106 times faster than state-of-the-art methods while achieving the same or even better accuracy.©VideoSizeß36.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452833&amp;file=3448016.3452833.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452833®Keywordsìªlocality similarity hashing™similarity¶sketch¶Badges¿•Track∞research-article®Citation ®DownloadÃ∏©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/WasayCI21•titleŸ*Deep Learning: Systems and Responsibility.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457541©publisher±SIGMOD Conferenceßauthorsì´Abdul Wasay≤Subarna ChatterjeeÆStratos Idreos®Abstract⁄®Deep learning enables numerous applications across diverse areas. Data systems researchers are also increasingly experimenting with deep learning to enhance data systems performance. We present a tutorial on deep learning, highlighting the data systems nature of neural networks as well as research opportunities for advancements through data management techniques. We focus on three critical aspects: (1) classic design tradeoffs in neural networks which we can enrich through a systems and data management perspective, e.g., thinking critically about storage, data movement, and computation; (2) classic design problems in data systems which we can reconsider with neural networks as a viable design option, e.g., to replace or help system components that make complex decisions such as database optimizers; and (3) essential considerations for responsible application of neural networks in critical human-facing problems in society and how these also link to data management and performance considerations. While these are seemingly a diverse set of rich topics, they are strongly interconnected through data management, and their combination offers rich opportunities for future research.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457541®Keywordsîµdeep learning systems∂deep learning fairness≠deep learning∞machine learning¶Badges¿•Track®tutorial®Citation ®DownloadÃ∏©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/LiG21•titleŸLImminence Monitoring of Critical Events: A Representation Learning Approach.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452804©publisher±SIGMOD Conferenceßauthorsí¶Yan Li´Tingjian Ge®Abstract⁄Complex event monitoring is an important problem in data streams that has drawn much attention. Most previous work assumes that the user knows and provides a complex event pattern for the system to continuously monitor. However, we observe that in many real applications, such as healthcare, security, and businesses, there are heterogeneous substreams and a diverse set of attributes. Often there is no simple uniform pattern prior to a critical event; nor is there clean simple language to describe the pattern leading to the critical event. People often only know it after the fact -- e.g., when something undesirable happens. We propose a novel approach based on relational machine learning and representation learning. We propose and learn probabilistic state machine patterns, which are used to monitor and predict the imminence of critical events. Our experiments demonstrate the efficiency and effectiveness of our approach, as well as its clear superiority over the closest previous approaches such as IL-Miner and LSTM based early prediction.©VideoSize®219.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452804&amp;file=3448016.3452804.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452804®Keywordsì∑representation learning¨data streams¥imminence monitoring¶Badges¿•Track∞research-article®Citation ®DownloadÃ∑©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/WuGJC0YCTY21•titleŸ<Vertex-Centric Visual Programming for Graph Neural Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452770©publisher±SIGMOD ConferenceßauthorsôßYidi Wu™Yuntao Gui´Tatiana Jin´James Cheng≠Xiao Yan 0002©Peiqi Yin©Yufei CaißBo Tang¶Fan Yu®Abstract⁄yGraph neural networks (GNNs) have achieved remarkable performance in many graph analytics tasks such as node classification, link prediction and graph clustering. Existing GNN systems (e.g., PyG and DGL) adopt a tensor-centric programming model and train GNNs with manually written operators. Such design results in poor usability due to the large semantic gap between the API and the GNN models, and suffers from inferior efficiency because of high memory consumption and massive data movement. We demonstrateSeastar, a novel GNN training framework that adopts avertex-centric programming paradigm and supportsautomatic kernel generation, to simplify model development and improve training efficiency. We will (i) show how to express GNN models succinctly using a visual "drag-and-drop'' interface or Seastar's vertex-centric python API; (ii) demonstrate the performance advantage of Seastar over existing GNN systems in convergence speed, training throughput and memory consumption; and (iii) illustrate how Seastar's optimizations (e.g., operator fusion and constant folding) improve training efficiency by profiling the run-time performance.©VideoSizeß15.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452770&amp;file=3448016.3452770.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452770®Keywordsí¥graph neural network¥deep learning system¶Badges¿•Track´short-paper®Citation ®DownloadÃ∑©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/SantosBCMF21•titleŸ>Correlation Sketches for Approximate Join-Correlation Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3458456©publisher±SIGMOD Conferenceßauthorsï≥A√©cio S. R. Santos´Aline Bessa≤Fernando Chirigati±Christopher MuscoÆJuliana Freire®Abstract⁄The increasing availability of structured datasets, from Web tables and open-data portals to enterprise data, opens up opportunities to enrich analytics and improve machine learning models through relational data augmentation. In this paper, we introduce a new class of data augmentation queries: join-correlation queries. Given a column Q and a join column KQ from a query table TQ, retrieve tables TX in a dataset collection such that TX is joinable with TQ on KQ and there is a column C ‚àà TX such that Q is correlated with C. A na√Øve approach to evaluate these queries, which first finds joinable tables and then explicitly joins and computes correlations between Q and all columns of the discovered tables, is prohibitively expensive. To efficiently support correlated column discovery, we 1) propose a sketching method that enables the construction of an index for a large number of tables and that provides accurate estimates for join-correlation queries, and 2) explore different scoring strategies that effectively rank the query results based on how well the columns are correlated with the query. We carry out a detailed experimental evaluation, using both synthetic and real data, which shows that our sketches attain high accuracy and the scoring strategies lead to high-quality rankings.©VideoSize®192.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3458456&amp;file=3448016.3458456.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3458456®Keywordsï¥sketching algorithmsºapproximate query processing¥confidence intervalsÆdataset searchªjoin-correlation estimation¶Badges¿•Track∞research-article®Citation ®DownloadÃµ©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/Tan21•title∂Deep Data Integration.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3460534©publisher±SIGMOD ConferenceßauthorsëÆWang-Chiew Tan®Abstract⁄òWe are witnessing the widespread adoption of deep learning techniques as avant-garde solutions to different computational problems in recent years. In data integration, the use of deep learning techniques has helped establish several state-of-the-art results in long standing problems, including information extraction, entity matching, data cleaning, and table understanding. In this talk, I will reflect on the strengths of deep learning and how that has helped move the needle in data integration. I will also discuss a few challenges associated with solutions based on deep learning techniques and describe some opportunities for the data management community.©VideoSizeß99.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3460534&amp;file=3448016.3460534.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3460534®Keywordsñ≥table understanding∂information extractionØentity matching∞data integration≠deep learning≠data cleaning¶Badges¿•Trackßkeynote®Citation ®DownloadÃ¥©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/YoonS0L21•titleŸ`Multiple Dynamic Outlier-Detection from a Data Stream by Exploiting Duality of Data and Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452810©publisher±SIGMOD Conferenceßauthorsî™Susik Yoon™Yooju Shin∞Jae-Gil Lee 0001≠Byung Suk Lee®Abstract⁄àReal-time outlier detection from a data stream has become increasingly important in the current hyperconnected world. This paper focuses on an important yet unaddressed challenge in continuous outlier detection: the multiplicity and dynamicity of queries. This challenge arises from various contexts of outliers evolving over time, but the state-of-the-art algorithms cannot handle the challenge effectively, as they can only process a fixed set of outlier detection queries for each data point separately. In this paper, we propose a novel algorithm, abbreviated as MDUAL, based on a new idea called duality-based unified processing. The underlying rationale is to exploit the duality of data and queries so that a group of similar data points are processed together by a group of similar queries incrementally. Two main techniques embodying the idea, data-query grouping and prioritized group processing, are employed. Comprehensive experiments showed that MDUAL runs 216 to 221 times faster while consuming 11 to 13 times less memory than the state-of-the-art algorithms through its efficient and effective handling of the multiplicity-dynamicity challenge.©VideoSize®272.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452810&amp;file=3448016.3452810.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452810®Keywordsì±outlier detection´data stream±anomaly detection¶Badges¿•Track∞research-article®Citation ®DownloadÃ≥©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/ShahvaraniJ21•titleºDistributed Stream KNN Join.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457269©publisher±SIGMOD Conferenceßauthorsí¥Amirhesam Shahvarani≤Hans-Arno Jacobsen®Abstract⁄skNN join over data streams is an important operation for location-aware systems, which correlates events from different sources based on their occurrence locations. Combining the complexity of kNN join and the dynamicity of data streams, kNN join in streaming environments is a computationally intensive operator, and its performance can be greatly improved by utilizing the computational capabilities of modern non-uniform memory access (NUMA) computing platforms. However, the conventional approaches to kNN join for prestored datasets do not work efficiently with the kind of highly dynamic data found in streaming environments. Therefore, in this paper, we introduce an adaptive scalable stream kNN join, named ADS-kNN, to address the challenges of performing the kNN join operation on highly dynamic data. We propose a multistage kNN execution plan that enables high-performance kNN queries in distributed settings by overlapping the computation and communication stages. Moreover, we propose an adaptive data partitioning scheme that dynamically adjusts the load among the operators according to the changes in the input values. Combining these two techniques, ADS-kNN provides a scalable and adaptive kNN join operator for data streams. Our experiments using a 56-core system show that ADS-kNN achieves a maximum throughput that is 21 times higher than that of a single-threaded approach.©VideoSize®557.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457269&amp;file=3448016.3457269.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457269®Keywordsì¨data streamsµnearest neighbor joinµdistributed computing¶Badges¿•Track∞research-article®Citation ®DownloadÃÆ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/WangBNM21•titleŸYDP-Sync: Hiding Update Patterns in Secure Outsourced Databases with Differential Privacy.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457306©publisher±SIGMOD ConferenceßauthorsîÆChenghong Wang´Johes Bater¨Kartik Nayak∂Ashwin Machanavajjhala®Abstract⁄#In this paper, we consider privacy-preserving update strategies for secure outsourced growing databases. Such databases allow appendonly data updates on the outsourced data structure while analysis is ongoing. Despite a plethora of solutions to securely outsource database computation, existing techniques do not consider the information that can be leaked via update patterns. To address this problem, we design a novel secure outsourced database framework for growing data, DP-Sync, which interoperate with a large class of existing encrypted databases and supports efficient updates while providing differentially-private guarantees for any single update. We demonstrate DP-Sync's practical feasibility in terms of performance and accuracy with extensive empirical evaluations on real world datasets.©VideoSize®161.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457306&amp;file=3448016.3457306.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457306®Keywordsì≥diffrential privacyÆupdate pattern≤encrypted database¶Badges¿•Track∞research-article®Citation ®DownloadÃÆ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/ArzamasovB21•titleŸ0REDS: Rule Extraction for Discovering Scenarios.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457301©publisher±SIGMOD ConferenceßauthorsíØVadim Arzamasov≠Klemens B√∂hm®Abstract⁄UScenario discovery is the process of finding areas of interest, known as scenarios, in data spaces resulting from simulations. For instance, one might search for conditions, i.e., inputs of the simulation model, where the system is unstable. Subgroup discovery methods are commonly used for scenario discovery. They find scenarios in the form of hyperboxes, which are easy to comprehend. Given a computational budget, results tend to get worse as the number of inputs of the simulation model and the cost of simulations increase. We propose a new procedure for scenario discovery from few simulations, dubbed REDS. A key ingredient is using an intermediate machine learning model to label data for subsequent use by conventional subgroup discovery methods. We provide statistical arguments why this is an improvement. In our experiments, REDS reduces the number of simulations required by 50--75% on average, depending on the quality measure. It is also useful as a semi-supervised subgroup discovery method and for discovering better scenarios from third-party data, when a simulation model is not available.©VideoSizeß24.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457301&amp;file=3448016.3457301.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457301®Keywordsî§PRIMØrule extraction≤subgroup discovery≤scenario discovery¶Badges¿•Track∞research-article®Citation ®DownloadÃ≠©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Liu0W0YY21•titleŸ7Joint Open Knowledge Base Canonicalization and Linking.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452776©publisher±SIGMOD Conferenceßauthorsñ©Yinan Liu≠Wei Shen 0004¨Yuanfei Wang≤Jianyong Wang 0001¨Zhenglu Yang¨Xiaojie Yuan®Abstract⁄Open Information Extraction (OIE) methods extract a large number of OIE triples (noun phrase, relation phrase, noun phrase) from text, which compose large Open Knowledge Bases (OKBs). However, noun phrases (NPs) and relation phrases (RPs) in OKBs are not canonicalized and often appear in different paraphrased textual variants, which leads to redundant and ambiguous facts. To address this problem, there are two related tasks: OKB canonicalization (i.e., convert NPs and RPs to canonicalized form) and OKB linking (i.e., link NPs and RPs with their corresponding entities and relations in a curated Knowledge Base (e.g., DBPedia). These two tasks are tightly coupled, and one task can benefit significantly from the other. However, they have been studied in isolation so far. In this paper, we explore the task of joint OKB canonicalization and linking for the first time, and propose a novel framework JOCL based on factor graph model to make them reinforce each other. JOCL is flexible enough to combine different signals from both tasks, and able to extend to fit any new signals. A thorough experimental study over two large scale OIE triple data sets shows that our framework outperforms all the baseline methods for the task of OKB canonicalization (OKB linking) in terms of average F1 (accuracy).©VideoSize®170.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452776&amp;file=3448016.3452776.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452776®KeywordsìŸ$open knowledge base canonicalization≤factor graph modelªopen knowledge base linking¶Badges¿•Track∞research-article®Citation ®DownloadÃ¨©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/WagnerK021•titleŸ6Self-Tuning Query Scheduling for Analytical Workloads.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457260©publisher±SIGMOD ConferenceßauthorsìØBenjamin Wagner´Andr√© Kohn≥Thomas Neumann 0001®Abstract⁄WMost database systems delegate scheduling decisions to the operating system. While such an approach simplifies the overall database design, it also entails problems. Adaptive resource allocation becomes hard in the face of concurrent queries. Furthermore, incorporating domain knowledge to improve query scheduling is difficult. To mitigate these problems, many modern systems employ forms of task-based parallelism. The execution of a single query is broken up into small, independent chunks of work (tasks). Now, fine-grained scheduling decisions based on these tasks are the responsibility of the database system. Despite being commonplace, little work has focused on the opportunities arising from this execution model. In this paper, we show how task-based scheduling in database systems opens up new areas for optimization. We present a novel lock-free, self-tuning stride scheduler that optimizes query latencies for analytical workloads. By adaptively managing query priorities and task granularity, we provide high scheduling elasticity. By incorporating domain knowledge into the scheduling decisions, our system is able to cope with workloads that other systems struggle with. Even at high load, we retain near optimal latencies for short running queries. Compared to traditional database systems, our design often improves tail latencies by more than 10x.©VideoSize®347.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457260&amp;file=3448016.3457260.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457260®Keywordsî∞query scheduling´parallelism´self-tuning∞database systems¶Badges¿•Track∞research-article®Citation ®DownloadÃ´©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/HaynesDHMBCC21•titleŸ*VSS: A Storage System for Video Analytics.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459242©publisher±SIGMOD ConferenceßauthorsóÆBrandon Haynes¨Maureen DaumßDong HeØAmrita Mazumdar¥Magdalena Balazinska¨Alvin Cheung©Luis Ceze®Abstract⁄We present a new video storage system (VSS) designed to decouple high-level video operations from the low-level details required to store and efficiently retrieve video data. VSS is designed to be the storage subsystem of a video data management system (VDBMS) and is responsible for: (1) transparently and automatically arranging the data on disk in an efficient, granular format; (2) caching frequently-retrieved regions in the most useful formats; and (3) eliminating redundancies found in videos captured from multiple cameras with overlapping fields of view. Our results suggest that VSS can improve VDBMS read performance by up to 54%, reduce storage costs by up to 45%, and enable developers to focus on application logic rather than video storage and retrieval.©VideoSizeß47.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459242&amp;file=3448016.3459242.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459242®KeywordsìØstorage systemsµvideo data managementØvideo analytics¶Badges¿•Track∞research-article®Citation ®DownloadÃ´©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/JinQZ21•titleŸJEfficient String Sort with Multi-Character Encoding and Adaptive Sampling.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457319©publisher±SIGMOD ConferenceßauthorsìßWen Jin¨Weining Qian´Aoying Zhou®Abstract⁄˘Sorting plays a fundamental role in computer science. It has far reaching applications in database operations and data science tasks. An important class of sorting keys are strings and among all string sorting methods, radix sort is a simple but effective algorithm. Many works have been studied to accelerate radix string sort. One typical approach is to process multiple characters in each sorting pass. However, this approach incurs the crucial issue of the radix being too large. To address the problem, we introduce a novel multi-character encoding based method that can significantly reduce the radix. This new encoding scheme takes advantage of the sparse alphabet space usage as well as the sparsity of distinguishing prefixes of the inputs which are commonly seen in real-world datasets. Combining the effective encoding scheme with an adaptive sampling process to generate the encoding efficiently, our proposed sorting algorithm essentially blends radix sort with sample sort and achieves substantial improvement over other sorting approaches. The results on both real datasets and synthetic datasets show that our method yields an average 4.85√ó performance improvement over C++ STL sort[21], 1.47√ó improvement over the state-of-the-art Radix Sort on strings implementation[19] and 2.55√ó over the multikey quicksort[6]. Preliminary tests in a multi-core environment also show it is competitive or better than the most recent parallel string sorting algorithm pS5[8] which demonstrates the scalability of our method.©VideoSize®168.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457319&amp;file=3448016.3457319.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457319®Keywordsîßsorting´sample sortÆstring dataset™radix sort¶Badges¿•Track∞research-article®Citation ®DownloadÃ™©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/0002TGL21•titleŸ5Pre-Trained Web Table Embeddings for Table Discovery.§year§2021£doiŸ'https://doi.org/10.1145/3464509.3464892©publisher´aiDM@SIGMODßauthorsîµMichael G√ºnther 0002´Maik ThieleÆJulius GonsiorØWolfgang Lehner®Abstract⁄îPre-trained word embedding models have become the de-facto standard to model text in state-of-the-art analysis tools and frameworks. However, while there are massive amounts of textual data stored in tables, word embedding models are usually pre-trained on large documents. This mismatch can lead to narrowed performance on tasks where text values in tables are analyzed. To improve analysis and retrieval tasks working with tabular data, we propose a novel embedding technique to be pre-trained directly on a large Web table corpus. In an experimental evaluation, we employ our models for various data analysis tasks on different data sources. Our evaluation shows that models using pre-trained Web table embeddings outperform the same models when applied to embeddings pre-trained on text. Moreover, we show that by using Web table embeddings state-of-the-art models for the investigated tasks can be outperformed.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3464509.3464892®Keywordsì™Web tables∑learned representationsØtable discovery¶Badges¿•Track∞research-article®Citation ®DownloadÃ©©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/Ross21•titleŸcUtilizing (and Designing) Modern Hardware for Data-Intensive Computations: The Role of Abstraction.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3460535©publisher±SIGMOD ConferenceßauthorsëØKenneth A. Ross®Abstract⁄ Modern information-intensive systems, including data management systems, operate on data that is mostly resident in RAM. As a result, the data management community has shifted focus from I/O optimization to addressing performance issues higher in the memory hierarchy. In this keynote, I will give a personal perspective of these developments, illustrated by work from my group at Columbia University. I will use the concept of abstraction as a lens through which various kinds of optimizations for modern hardware platforms can be understood and evaluated. Through this lens, some "cute implementation tricks" can be seen as much more than mere implementation details. I will discuss abstractions at various granularities, from single lines of code to whole programming/query languages. I will touch on software and hardware design for data-intensive computations. I will also discuss data processing in a conventional programming language, and how the data management community might contribute to the design of compilers.©VideoSize®166.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3460535&amp;file=3448016.3460535.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3460535®Keywordsñ®hardware´performance©compilers≠data locality∞query processing™contention¶Badges¿•Trackßkeynote®Citation ®DownloadÃ®©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/LuoJYJ21•titleŸeAutomatic Optimization of Matrix Implementations for Distributed Machine Learning and Linear Algebra.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457317©publisher±SIGMOD Conferenceßauthorsî´Shangyu Luo∞Dimitrije Jankov¨Binhang YuanÆChris Jermaine®Abstract⁄ZMachine learning (ML) computations are often expressed using vectors, matrices, or higher-dimensional tensors. Such data structures can have many different implementations, especially in a distributed environment: a matrix could be stored as row or column vectors, tiles of different sizes, or relationally, as a set of (rowIndex, colIndex, value) triples. Many other storage formats are possible. The choice of format can have a profound impact on the performance of a ML computation. In this paper, we propose a framework for automatic optimization of the physical implementation of a complex ML or linear algebra (LA) computation in a distributed environment, develop algorithms for solving this problem, and show, through a prototype on top of a distributed relational database system, that our ideas can radically speed up common ML and LA computations.©VideoSizeß25.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457317&amp;file=3448016.3457317.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457317®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃß©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/GalhotraFSS21•titleŸ/BEER: Blocking for Effective Entity Resolution.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452747©publisher±SIGMOD Conferenceßauthorsî∞Sainyam Galhotra±Donatella Firmani™Barna Saha±Divesh Srivastava®Abstract⁄wBlocking is a key component of Entity Resolution (ER) that aims to improve efficiency by quickly pruning out non-matching record pairs. However, depending on the noise in the dataset and the distribution of entity cluster sizes, existing techniques can be either (a) too aggressive, such that they help scale but can adversely affect the ER effectiveness, or (b) too permissive, potentially harming ER efficiency. We propose a new methodology of progressive blocking that enables both efficient and effective ER and works across different entity cluster size distributions without manual fine tuning. In this paper, we demonstrate BEER (Blocking for Effective Entity Resolution), the first end-to-end system that leverages intermediate ER output in a feedback loop to refine the blocking result in a data-driven fashion, thereby enabling effective entity resolution. BEER allows the user to explore the different components of the ER pipeline, analyze the effectiveness of alternative blocking techniques and understand the interaction between blocking and ER. BEER supports visualization of the different entities present in a block, explains the change in blocking output with every round of feedback and allows the end-user to interactively compare different techniques. BEER has been developed as open-source software; the code and the demonstration video are available at beer-system.github.io.©VideoSizeß31.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452747&amp;file=3448016.3452747.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452747®Keywordsî®blocking±entity resolution∞data integrationÆde-duplication¶Badges¿•Track´short-paper®Citation ®DownloadÃ¶©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/HilprechtB21•titleŸ:ReStore - Neural Data Completion for Relational Databases.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457264©publisher±SIGMOD Conferenceßauthorsí≤Benjamin HilprechtÆCarsten Binnig®Abstract⁄òClassical approaches for OLAP assume that the data of all tables is complete. However, in case of incomplete tables with missing tuples, classical approaches fail since the result of a SQL aggregate query might significantly differ from the results computed on the full dataset. Today, the only way to deal with missing data is to manually complete the dataset which causes not only high efforts but also requires good statistical skills to determine when a dataset is actually complete. In this paper, we propose an automated approach for relational data completion called ReStore using a new class of (neural) schema-structured completion models that are able to synthesize data which resembles the missing tuples. As we show in our evaluation, this efficiently helps to reduce the relative error of aggregate queries by up to 390% on real-world data compared to using the incomplete data directly for query answering.©VideoSizeß37.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457264&amp;file=3448016.3457264.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457264®KeywordsïØincomplete dataØrelational data¥data-driven learning∫deep autoregressive modelsØdata completion¶Badges¿•Track∞research-article®Citation ®DownloadÃ¶©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/MayerJ21•titleŸVHybrid Edge Partitioner: Partitioning Large Power-Law Graphs under Memory Constraints.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457300©publisher±SIGMOD Conferenceßauthorsí´Ruben Mayer≤Hans-Arno Jacobsen®Abstract⁄Distributed systems that manage and process graph-structured data internally solve a graph partitioning problem to minimize their communication overhead and query run-time. Besides computational complexity---optimal graph partitioning is NP-hard---another important consideration is the memory overhead. Real-world graphs often have an immense size, such that loading the complete graph into memory for partitioning is not economical or feasible. Currently, the common approach to reduce memory overhead is to rely on streaming partitioning algorithms. While the latest streaming algorithms lead to reasonable partitioning quality on some graphs, they are still not completely competitive to in-memory partitioners. In this paper, we propose a new system, Hybrid Edge Partitioner (HEP), that can partition graphs that fit partly into memory while yielding a high partitioning quality. HEP can flexibly adapt its memory overhead by separating the edge set of the graph into two sub-sets. One sub-set is partitioned by NE++, a novel, efficient in-memory algorithm, while the other sub-set is partitioned by a streaming approach. Our evaluations on large real-world graphs show that in many cases, HEP outperforms both in-memory partitioning and streaming partitioning at the same time. Hence, HEP is an attractive alternative to existing solutions that cannot fine-tune their memory overheads. Finally, we show that using HEP, we achieve a significant speedup of distributed graph processing jobs on Spark/GraphX compared to state-of-the-art partitioning algorithms.©VideoSizeß44.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457300&amp;file=3448016.3457300.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457300®Keywordsí≤graph partitioningºdistributed graph processing¶Badges¿•Track∞research-article®Citation ®DownloadÃ¶©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/FarhatDQ21•titleŸ<Klink: Progress-Aware Scheduling for Streaming Data Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452794©publisher±SIGMOD Conferenceßauthorsì´Omar Farhat∞Khuzaima Daudjee±Leonardo Querzoni®Abstract⁄¿Modern stream processing engines (SPEs) process large volumes of events propagated at high velocity through multiple queries. To improve performance, existing SPEs generally aim to minimize query output latency by minimizing, in turn, the propagation delay of events in query pipelines. However, for queries containing commonly used blocking operators such as windows, this scheduling approach can be inefficient. Watermarks are events popularly utilized by SPEs to correctly process window operators. Watermarks are injected into the stream to signify that no events preceding their timestamp should be further expected. Through the design and development of Klink, we leverage these watermarks to robustly infer stream progress based on window deadlines and network delay, and to schedule query pipeline execution that reflects stream progress. Klink aims to unblock window operators and to rapidly propagate events to output operators while performing judicious memory management. We integrate Klink into the popular open source SPE Apache Flink and demonstrate that Klink delivers significant performance gains over existing scheduling policies on benchmark workloads for both scale-up and scale-out deployments.©VideoSizeß45.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452794&amp;file=3448016.3452794.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452794®Keywordsîßwindows™watermarks±stream processing™scheduling¶Badges¿•Track∞research-article®Citation ®DownloadÃ§©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/BhattacherjeeLH21•titleŸ6BullFrog: Online Schema Evolution via Lazy Evaluation.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452842©publisher±SIGMOD Conferenceßauthorsî¥Souvik Bhattacherjee©Gang Liao≤Michael Hicks 0001ØDaniel J. Abadi®Abstract⁄ºBullFrog is a relational DBMS that supports single-step schema migrations --- even those that are backwards incompatible --- without downtime, and without need for advanced warning. When a schema migration is submitted, BullFrog initiates a logical switch to the new schema, but physically migrates affected data lazily, as it is accessed by incoming transactions. BullFrog's internal concurrency control algorithms and data structures enable concurrent processing of schema migration operations with post-migration transactions, while ensuring exactly-once migration of all old data into the physical layout required by the new schema. BullFrog is implemented as an open source extension to PostgreSQL. Experiments using this prototype over a TPC-C based workload (supplemented to include schema migrations) show that BullFrog can achieve zero-downtime migration to non-trivial new schemas with near-invisible impact on transaction throughput and latency.©VideoSize•31 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452842&amp;file=3448016.3452842.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452842®Keywordsñ≤schema maintenance∞schema evolutionŸ!highly available database systemsµcontinuous deployment∞schema migrationµlazy schema migration¶Badges¿•Track∞research-article®Citation ®DownloadÃ£©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/SongHG021•titleŸ8Why Not Match: On Explanations of Event Pattern Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452818©publisher±SIGMOD Conferenceßauthorsî´Shaoxu Song≠Ruihong Huang¶Yu Gao±Jianmin Wang 0001®Abstract⁄äQueries over event data are posed in a form of event patterns, for example, to retrieve the flights from IAH to LGA without a stopover. If the expected answer is not returned, one may ask why not, also known as explanations of non-answers. Analogous to the relational data, the explanations over event data lie in two aspects. (1) The pattern consistency explanation indicates that the patterns specified in the query are wrong (inconsistent), that is, there exists no tuple of events that can match the query. (2) The timestamp modification explanation speculates that the instance of event tuple is incorrect, for example, the timestamps of some events are imprecise and need modification. To the best of our knowledge, this is the first study on explaining non-answers over event data. We prove that both explanation problems are NP-complete. By encoding event patterns as a novel notation, we identify the special cases that can be efficiently solved or approximated. General cases are addressed by utilizing the solutions of special cases. Extensive experiments over real and synthetic datasets demonstrate both effectiveness and efficiency of our proposal.©VideoSizeß24.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452818&amp;file=3448016.3452818.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452818®Keywordsì≠event pattern™event data´explanation¶Badges¿•Track∞research-article®Citation ®DownloadÃ£©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/FengD21•titleŸ@Allign: Aligning All-Pair Near-Duplicate Passages in Long Texts.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457548©publisher±SIGMOD Conferenceßauthorsí™Weiqi FengÆDong Deng 0001®Abstract⁄€In this paper, we study the problem of aligning all-pair near-duplicate passages in two long texts. A passage is a sequence of consecutive words in a text. It can begin and end with any word in the text, whether around a period or not. Due to the high computation cost of this problem, existing work all compromise to heuristic alignment methods, which can harm the recall of downstream applications, such as deduplication and plagiarism detection. To address this problem, in this paper, we propose a min-hash based method Allign to find all near-duplicate passage pairs in two long texts. Allign generates a few min-hash values for each passage in the texts and reports all the passage pairs sharing enough common min-hash values. However, for a pair of texts with n and m words, there are in total O(n2m2) passage pairs (each text contains O(n2) and O(m2) passages respectively). Thus it is prohibitively expensive to enumerate all passage pairs in two texts and count their common min-hash values. To address this issue, Allign packs a large number of nearby and overlapping passages with the same min-hash to a "compact window". In total, Allign generates O(n) compact windows to represent all the O(n2) passages in a text with n words. Next, a pair of compact windows in two texts are matched if they have the same min-hash. The rest of unmatched compact windows are removed. Finally, Allign reports all the passage pairs contained by enough number of matched compact window pairs, which must share the same enough number of min-hash values. In this way, Allign avoids enumerating the enormous number of passage pairs. Last but not least, to make the reported near-duplicate passages more relevant and Allign more efficient, we show how to support a few practical constraints efficiently, including reporting only longest near-duplicates and sentence-level near-duplicates. Experimental results on real-world datasets show that Allign significantly outperforms the state-of-the-art text alignment methods.©VideoSizeß99.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457548&amp;file=3448016.3457548.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457548®Keywordsï∏near-duplicate detection≠text curation≠deduplicationÆtext alignment¥plagiarism detection¶Badges¿•Track∞research-article®Citation ®DownloadÃ¢©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/ZhuKKTPMKZMJSDI21•titleŸ1KEA: Tuning an Exabyte-Scale Data Infrastructure.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457569©publisher±SIGMOD Conferenceßauthorsü©Yiwen ZhuÆSubru Krishnan∂Konstantinos Karanasos™Isha Tarte´Conor Power≠Abhishek Modi´Manoj Kumar™Deli Zhang±Kartheek Muthyala¨Nick Jurgens≤Sarvesh Sakalanaga≠Sudhir Darbha©Minu IyerÆAnkita Agarwal¨Carlo Curino®Abstract⁄ÀMicrosoft's internal big-data infrastructure is one of the largest in the world---with over 300k machines running billions of tasks from over 0.6M daily jobs. Operating this infrastructure is a costly and complex endeavor, and efficiency is paramount. In fact, for over 15 years, a dedicated engineering team has tuned almost every aspect of this infrastructure, achieving state-of-the-art efficiency (&gt;60% average CPU utilization across all clusters). Despite rich telemetry and strong expertise, faced with evolving hardware/software/workloads this manual tuning approach had reached its limit---we had plateaued. In this paper, we present KEA, a multi-year effort to automate our tuning processes to be fully data/model-driven. KEA leverages a mix of domain knowledge and principled data science to capture the essence of our cluster dynamic behavior in a set of machine learning (ML) models based on collected system data. These models power automated optimization procedures for parameter tuning, and inform our leadership in critical decisions around engineering and capacity management (such as hardware and data center design, software investments, etc.). We combine "observational'' tuning (i.e., using models to predict system behavior without direct experimentation) with judicious use of "flighting'' (i.e., conservative testing in production). This allows us to support a broad range of applications that we discuss in this paper. KEA continuously tunes our cluster configurations and is on track to save Microsoft tens of millions of dollars per year. At the best of our knowledge, this paper is the first to discuss research challenges and practical learnings that emerge when tuning an exabyte-scale data infrastructure.©VideoSizeß64.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457569&amp;file=3448016.3457569.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457569®Keywordsì∑automatic configurationÆdata analytics≥resource management¶Badges¿•Track∞research-article®Citation ®DownloadÃ¢©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/KorberGS21•titleŸ3Index-Accelerated Pattern Matching in Event Stores.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457245©publisher±SIGMOD ConferenceßauthorsìØMichael K√∂rber¥Nikolaus GlombiewskiØBernhard Seeger®Abstract⁄_IoT applications require a new type of database systems termed event stores for ingesting fast arriving event streams and efficiently supporting analytical ad-hoc queries over time. One of the most important operations in this regard is sequential pattern matching also known as Match\_Recognize, which matches user defined predicates to subsequences of events. While Match\_Recognize is well known in the field of event processing, it has only recently become part of the SQL standard. Despite of that, Match\_Recognize has received little attention in the database area so far. We present a novel approach to speed up an important class of Match\_Recognize queries on event stores by utilizing off-the-shelf secondary indexes on non-temporal attributes (e.g., B$^+$-trees, LSM-trees) and a cost model for selecting the most appropriate indexes. Our approach keeps temporal and sequential information in secondary indexes to prune large parts of the stream from further processing. However, simply using as many secondary indexes as available is not the right choice because the access cost for the index scans can exceed the processing time of the na√Ø ve approach that scans the entire stream and replays it into an event processing system. In order to address this problem, we present a first cost model to estimate the total execution cost of a Match\_Recognize query for a set of available indexes. Based on this cost model, we devise an efficient index selection strategy that avoids a full enumeration of index configurations. Prototypical implementations of our approach are available in our open-source research prototype, a commercial database system, and Apache Flink. In experiments with synthetic and real-world data sets, all our index-based implementations clearly outperform the na√Ø ve replay strategy that is currently offered in commercial database systems and Flink.©VideoSizeß82.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457245&amp;file=3448016.3457245.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457245®Keywordsó∞event processing∞pattern matching´event store±stream processing£cepØmatch recognize®indexing¶Badges¿•Track∞research-article®Citation ®DownloadÃ¢©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/0002LG21•titleŸBOnline Topic-Aware Entity Resolution Over Incomplete Data Streams.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457238©publisher±SIGMOD Conferenceßauthorsì∞Weilong Ren 0002™Xiang Lian∞Kambiz Ghazinour®Abstract⁄vIn many real applications such as the data integration, social network analysis, and the Semantic Web, the entity resolution (ER) is an important and fundamental problem, which identifies and links the same real-world entities from various data sources. While prior works usually consider ER over static and complete data, in practice, application data are usually collected in a streaming fashion, and often incur missing attributes (due to the inaccuracy of data extraction techniques). Therefore, in this paper, we will formulate and tackle a novel problem, topic-aware entity resolution over incomplete data streams (TER-iDS), which online imputes incomplete tuples and detects pairs of topic-related matching entities from incomplete data streams. In order to effectively and efficiently tackle the TER-iDS problem, we propose an effective imputation strategy, carefully design effective pruning strategies, as well as indexes/synopsis, and develop an efficient TER-iDS algorithm via index joins. Extensive experiments have been conducted to evaluate the effectiveness and efficiency of our proposed TER-iDS approach over real data sets.©VideoSizeß25.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457238&amp;file=3448016.3457238.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457238®Keywordsì∑incomplete data streamsßter-idsΩtopic-aware entity resolution¶Badges¿•Track∞research-article®Citation ®DownloadÃ°©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Zhang21•titleŸ>Memory-Efficient Search Trees for Database Management Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3461470©publisher±SIGMOD ConferenceßauthorsëÆHuanchen Zhang®Abstract⁄ÀThe growing cost gap between DRAM and storage together with increasing database sizes means that database management systems (DBMSs) now operate with a lower memory to storage size ratio than before. On the other hand, modern DBMSs rely on in-memory search trees (e.g., indexes and filters) to achieve high throughput and low latency. These search trees, however, consume a large portion of the total memory available to the DBMS. Existing compression techniques for search trees often rely on general-purpose block compression algorithms such as Snappy and LZ4. These algorithms, however, impose too much computational overhead for in-memory search trees because the DBMS is unable to operate directly on the index data without having to decompress it first. Simply getting rid of all or part of the search trees is also suboptimal because they are crucial to query performance. This dissertation seeks to address the challenge of building compact yet fast in-memory search trees to allow more efficient use of memory in data processing systems. We first present techniques to obtain maximum compression on fast static (i.e., read-optimized) search trees. We identified sources of memory waste in existing trees and designed new succinct data structures to show that we can push the memory consumption of a search tree to the theoretical limit without compromising its query performance. Next, we introduce the hybrid index architecture as a way to efficiently modifying the aforementioned static data structures with bounded and amortized cost in performance and space. Finally, instead of structural compression, we approach the problem from an orthogonal direction by compressing the actual keys. We built a fast string compressor that can encode arbitrary input keys while preserving their order so that search trees can serve range queries directly based on compressed keys. Together, these three pieces form a practical recipe for achieving memory-efficiency in search trees and in DBMSs.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3461470®Keywordsñ±database indexingØrange filtering´search tree∏succinct data structuresØkey compression±memory-efficiency¶Badges¿•Track¨invited-talk®Citation ®DownloadÃû©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/MoellerYLL21•titleŸ6Toto - Benchmarking the Efficiency of a Cloud Service.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457555©publisher±SIGMOD ConferenceßauthorsîÆJustin Moeller•Zi Ye≠Katherine Lin´Willis Lang®Abstract⁄	Microsoft aims to increase the efficiency of Azure SQL DB by maximizing the number of databases that can be hosted in a cluster. However, resource contention among customers increases when changing the configurations, policies, and features that control database co-location on cluster nodes. Tuning and evaluating the efficiency and customer impact of these variables in a scientific manner in production, with a dynamic system and customer workloads, is difficult or infeasible. Here, we present Toto, a benchmark framework for evaluating the efficiency of any cloud service that leverages orchestrators like Service Fabric or Kubernetes. Toto allows for reliable and repeatable specification of a benchmarking scenario of arbitrary scale, complexity, and time-length. An implementation of Toto is deployed in all SQL DB staging clusters and is used to evaluate system efficiency and behaviors. As an example of Toto's capabilities, we present a study to explore the balance between cluster database density and quality of service.©VideoSize¶150 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457555&amp;file=3448016.3457555.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457555®Keywordsî¨benchmarking≠orchestrationØcloud databases™efficiency¶Badges¿•Track∞research-article®Citation ®DownloadÃù©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/Amer-YahiaMY21•titleŸ*Exploring Ratings in Subjective Databases.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457259©publisher±SIGMOD Conferenceßauthorsì∞Sihem Amer-Yahia©Tova MiloÆBrit Youngmann®Abstract⁄dSubjective data links people to content items and reflects who likes or dislikes what. The valuable information this data contains is virtually infinite and satisfies various information needs. Yet, as of today, dedicated tools to explore this data are lacking. In this paper, we develop a framework for Subjective Data Exploration (SDE). Our solution enables the joint exploration of items, people, and people's opinions on items, in a guided multi-step process where each step aggregates the most useful and diverse trends in the form of rating maps. Because of the large search space of possible rating maps, we leverage pruning strategies based on confidence intervals and multi-armed bandits. Our large-scale experiments with human subjects and real datasets, demonstrate the need for dedicated SDE frameworks and the effectiveness and efficiency of our approach.©VideoSizeß40.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457259&amp;file=3448016.3457259.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457259®KeywordsìØsubjective data≥recommender systems∞data exploration¶Badges¿•Track∞research-article®Citation ®DownloadÃö©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/00040HZ21•titleŸWMetaInsight: Automatic Discovery of Structured Knowledge for Exploratory Data Analysis.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457267©publisher±SIGMOD Conferenceßauthorsî±Pingchuan Ma 0004≠Rui Ding 0001ßShi Han≠Dongmei Zhang®Abstract⁄óAutomatic Exploratory Data Analysis (EDA) focuses on automatically discovering pieces of knowledge in the form of interesting data patterns. However, the conveyed knowledge by these suggested data patterns are disjointed or lack organization. Therefore, it is difficult for users to gain structured knowledge, and as the number of suggested patterns grows, these stand-alone patterns are less likely to motive users to conduct follow-up analysis, which hinders it from being effectively utilized to facilitate EDA. In this paper, we propose MetaInsight, a structured representation of knowledge extracted from multi-dimensional data aiming to facilitate EDA automatically and effectively. Specifically, we propose a novel formulation of basic data pattern to capture essential characteristics of raw data distribution to achieve knowledge extraction. Then based on the mined Homogeneous Data Patterns (HDP) and inter-pattern similarity, MetaInsight is identified by categorizing basic data patterns (within an HDP) into commonness(es) and exceptions thus achieving structured knowledge representation. The commonness(es) and exceptions concretize the knowledge obtained by induction and validation processes which are two typical analysis mechanisms conducted in EDA. We propose a novel scoring function to quantify the usefulness of MetaInsight, an effective and efficient mining procedure and a ranking algorithm to automatically discover high-quality MetaInsights from multi-dimensional data. We demonstrate the effectiveness and efficiency of MetaInsights (w.r.t. facilitating EDA) through evaluation on real-world datasets and user studies on both expert users and non-expert users.©VideoSizeß44.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457267&amp;file=3448016.3457267.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457267®Keywordsì≥analysis mechanismsπexploratory data analysis¥structured knowledge¶Badges¿•Track∞research-article®Citation ®DownloadÃö©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/Zalipynis21•titleŸPConvergence of Array DBMS and Cellular Automata: A Road Traffic Simulation Case.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3458457©publisher±SIGMOD ConferenceßauthorsëŸ Ramon Antonio Rodriges Zalipynis®Abstract⁄ÇArray DBMSs manage big N-d arrays, are not yet widely known, but are experiencing an R&amp;D surge due to the rapid growth of array volumes. Cellular automata (CA) operate on a discrete lattice of cells that can be modeled by an N-d array. CA are successfully applied to model fire spread, land cover change, road traffic, and other processes. We made traffic CA simulations possible by array DBMS due to novel components: native UDF language, proactive exec plans, convolution operator, retiling strategy, array versioning, locks, virtual axes, etc. A database approach to CA brings powerful parallelization, data fusion, array processing, and interoperability to name a few. To our best knowledge, our work is the first to run end-to-end CA simulations completely inside array DBMS: we enable array DBMS to simulate the physical world for the first time. Paper homepage: http://sigmod2021.gis.gg/©VideoSize®364.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3458457&amp;file=3448016.3458457.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3458457®Keywordsî™array dbms±cellular automata≥physical simulation≠urban traffic¶Badges¿•Track∞research-article®Citation ®DownloadÃô©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Zamanian21•titleŸLScalable Distributed Transaction Processing on Modern RDMA-enabled Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3461469©publisher±SIGMOD ConferenceßauthorsëÆErfan Zamanian®Abstract⁄\The scalability of existing shared-nothing databases quickly degrades in the presence of distributed transactions with the networking being the main reason. When distributed databases were designed, networks had low bandwidth, high latency, and high CPU overhead per message because of the traditional network stack. As a result, distributed database were designed to avoid distributed transactions as much as possible. Yet, with the advent of next generation of high-speed RDMA-enabled networks, it is time to revisit this design mantra of distributed DBMSs. In my thesis, the main focus was thus on the redesign of distributed transaction processing systems by efficiently leveraging modern RDMA-enabled networks. With Remote Direct Memory Access (RDMA), it is possible to bypass the CPU when transferring data from one machine to another. Moreover, the current generation of these networks is already able to provide a bandwidth similar to that of the main memory. However, our first finding is that simply upgrading the network does not automatically yield scalability without redesigning the underlying distributed database. In the quest for building fast, scalable and highly available OLTP databases, my thesis revisited the design of distributed DBMSs for RDMA and made various core contributions to the DBMS community: 1- Do we need a new DBMS architecture? We propose a new abstraction called Network-Attached-Memory (NAM) which fully exploits RDMA for distributed DBMSs. The main idea is to decouple compute and storage to enable independent scalability, and allow compute nodes to access data on the storage nodes using RDMA operations. Using this architecture, physical co-location of compute and storage to improve performance becomes a second class design consideration, as opposed to being a necessity for scalability. 2- Do we need new transaction protocols? Using the NAM architecture, it is possible to design scalable OLTP systems which efficiently leverage low overhead RDMA. However, it requires revisiting existing data structures and transaction protocols as they were designed in the era of slow networks with high per message cost. To this end, we presented the design of our novel scalable OLTP engine called NAM-DB. It implements the very common Snapshot Isolation scheme but tailored for RDMA. For example, NAM-DB builds on a scalable timestamp generation algorithm which efficiently utilizes one-sided RDMA operations to decentralize this task. The experiments on NAM-DB show that distributed transactions can indeed scale, without an inherent bottleneck other than those imposed by the workload itself (as I will discuss next). 3- Do we need new partitioning strategies? The primary goal of existing partitioning techniques in distributed DBMSs is to minimize the number cross-partition transactions, simply because network used to be the dominant bottleneck. In modern networks, however, we found that the new bottleneck which hinders scalability is data contention, while minimizing network communication plays only a subordinate role. To this end, we developed a new solution called Chiller that extends NAM-DB in two directions: (1) a novel commit protocol based on re-ordering transaction operations with the goal of minimizing the lock duration for contended records, and (2) contention-aware partitioning so that the most critical records can be updated without additional coordination. 4- Do we need new high-availability protocols? Finally, we revisited high availability for NAM-DB. Same as for partitioning, the main goal of existing high availability approaches is to minimize the network overhead which is no longer a bottleneck with fast RDMA-enables networks. This calls also for new protocols to fully unleash the potential of RDMA networks for high availability. Hence, as a last contribution we present a novel strongly consistent replication scheme called Active-Memory. Our proposed primary-backup replication algorithm allows an RDMA-based OLTP system to maintain its high performance in the presence of failures through an efficient RDMA-based undo-logging scheme, achieving much better performance compared to the existing techniques.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3461469®Keywordsî±high availability§RDMA±data partitioning∏distributed transactions¶Badges¿•Track¨invited-talk®Citation ®DownloadÃò©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Chen021•titleŸNEfficient Approximate Algorithms for Empirical Entropy and Mutual Information.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457255©publisher±SIGMOD ConferenceßauthorsíÆXingguang ChenÆSibo Wang 0001®Abstract⁄PEmpirical entropy is a classic concept in data mining and the foundation of many other important concepts like mutual information. However, computing the exact empirical entropy/mutual information on large datasets can be expensive. Some recent research work explores sampling techniques on the empirical entropy/mutual information to speed up the top-k and filtering queries. However, their solution still aims to return the exact answers to the queries, resulting in high computational costs. Motivated by this, in this work, we present approximate algorithms for the top-k queries and filtering queries on empirical entropy and empirical mutual information. The approximate algorithm allows user-specified tunable parameters to control the trade-off between the query efficiency and accuracy. We design effective stopping rules to return the approximate answers with improved query time. We further present theoretical analysis and show that our proposed solutions achieve improved time complexity over previous solutions. We experimentally evaluate our proposed algorithms on real datasets with up to 31M records and 179 attributes. Our experimental results show that the proposed algorithm consistently outperforms the state of the art in terms of computational efficiency, by an order of magnitude in most cases, while providing the same accurate result.©VideoSize®109.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457255&amp;file=3448016.3457255.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457255®Keywordsì®samplingºempirical mutual information±empirical entropy¶Badges¿•Track∞research-article®Citation ®DownloadÃó©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Ni00021•titleŸ@When the Recursive Diversity Anonymity Meets the Ring Signature.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452825©publisher±SIGMOD Conferenceßauthorsî©Wangze NiØPeng Cheng 0003≠Lei Chen 0048ØXuemin Lin 0001®Abstract⁄ÁIn privacy-preserving blockchain systems, to protect a sender's identity of a transaction in privacy-preserving blockchain systems, ring signature (RS) schemes have been widely implemented, which allow users to obscure consumed tokens via including "mixin'' (i.e., chaff tokens). However, recent works point out that existing RS schemes are vulnerable to the "chain-reaction'' analysis, where adversaries eliminate mixins of RSs by utilizing the fact that each token can only be consumed in a RS. By "chain-reaction'' analysis, adversaries can find some definite token-RS pair sets (DTRSs) to confirm the sender's identity of a RS. Besides, the existing RS schemes do not consider the diversity of mixins when generating a RS. Moreover, since the transaction fee is proportional to the number of mixins, a use is motivated to use a RS with the minimum number of mixins. In this paper, we formally define the diversity-aware mixins selection (DA-MS) problem, which aims to generate a RS with the minimum number of mixins satisfying the constraints of its diversity and the anonymity of other RSs. We prove the DA-MS problem is $\#P$ and propose a breadth-first search algorithm to get the optimal solution. Furthermore, to efficiently solve the DA-MS problem, we propose two practical configurations and two approximation algorithms with theoretic guarantees. Through comprehensive experiments on real data sets as well as synthetic data sets, we illustrate the effectiveness and the efficiency of our solutions.©VideoSizeß39.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452825&amp;file=3448016.3452825.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452825®Keywordsì™blockchainßprivacyÆring signature¶Badges¿•Track∞research-article®Citation ®DownloadÃñ©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/DeutchFGM21•titleŸKOn Optimizing the Trade-off between Privacy and Utility in Data Provenance.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452835©publisher±SIGMOD Conferenceßauthorsî≠Daniel Deutch±Ariel Frankenthal™Amir Gilad∞Yuval Moskovitch®Abstract⁄êOrganizations that collect and analyze data may wish or be mandated by regulation to justify and explain their analysis results. At the same time, the logic that they have followed to analyze the data, i.e., their queries, may be proprietary and confidential. Data provenance, a record of the transformations that data underwent, was extensively studied as means of explanations. In contrast, only a few works have studied the tension between disclosing provenance and hiding the underlying query. This tension is the focus of the present paper, where we formalize and explore for the first time the tradeoff between the utility of presenting provenance information and the breach of privacy it poses with respect to the underlying query. Intuitively, our formalization is based on the notion of provenance abstraction, where the representation of some tuples in the provenance expressions is abstracted in a way that makes multiple tuples indistinguishable. The privacy of a chosen abstraction is then measured based on how many queries match the obfuscated provenance, in the same vein as k-anonymity. The utility is measured based on the entropy of the abstraction, intuitively how much information is lost with respect to the actual tuples participating in the provenance. Our formalization yields a novel optimization problem of choosing the best abstraction in terms of this tradeoff. We show that the problem is intractable in general, but design greedy heuristics that exploit the provenance structure towards a practically efficient exploration of the search space. We experimentally prove the effectiveness of our solution using the TPC-H benchmark and the IMDB dataset.©VideoSize•83 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452835&amp;file=3448016.3452835.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452835®Keywordsìßprivacy´abstraction™provenance¶Badges¿•Track∞research-article®Citation ®DownloadÃñ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/LaiHLZ0K21•titleŸ5Top-K Deep Video Analytics: A Probabilistic Approach.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452786©publisher±SIGMOD Conferenceßauthorsñ´Ziliang Lai´Chenxia Han©Chris Liu≠Pengfei Zhang¨Eric Lo 0001ßBen Kao®Abstract⁄ßThe impressive accuracy of deep neural networks (DNNs) has created great demands on practical analytics over video data. Although efficient and accurate, the latest video analytic systems have not supported analytics beyond selection and aggregation queries. In data analytics, Top-K is a very important analytical operation that enables analysts to focus on the most important entities. In this paper, we present Everest, the first system that supports efficient and accurate Top-K video analytics. Everest ranks and identifies the most interesting frames/moments from videos with probabilistic guarantees. Everest is a system built with a careful synthesis of deep computer vision models, uncertain data management, and Top-K query processing. Evaluations on real-world videos and the latest Visual Road benchmark show that Everest achieves between 14.3x to 20.6x higher efficiency than baseline approaches with high result accuracy.©VideoSize®136.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452786&amp;file=3448016.3452786.movßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452786®Keywordsì¥deep video analytics∫uncertain query processing∂top-k query processing¶Badges¿•Track∞research-article®Citation ®DownloadÃñ©PaperRefsêã§Infoá§type≠inproceedings£keyªconf/sigmod/LivshitsKTIKR21•titleŸ3Properties of Inconsistency Measures for Databases.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457310©publisher±SIGMOD ConferenceßauthorsñÆEster LivshitsÆRina Kochirgan™Segev Tsur≠Ihab F. IlyasØBenny Kimelfeld´Sudeepa Roy®Abstract⁄¿How should we quantify the inconsistency of a database that violates integrity constraints? Proper measures are important for various tasks, such as progress indication and action prioritization in cleaning systems, and reliability estimation for new datasets. To choose an appropriate inconsistency measure, it is important to identify the desired properties in the application and understand which of these is guaranteed or at least expected in practice. For example, in some use cases, the inconsistency should reduce if constraints are eliminated; in others, it should be stable and avoid jitters and jumps in reaction to small changes in the database. We embark on a systematic investigation of properties for database inconsistency measures. We investigate a collection of basic measures that have been proposed in the past in both the Knowledge Representation and Database communities, analyze their theoretical properties, and empirically observe their behavior in an experimental study. We also demonstrate how the framework can lead to new inconsistency measures by introducing a new measure that, in contrast to the rest, satisfies all of the properties we consider and can be computed in polynomial time.©VideoSize•50 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457310&amp;file=3448016.3457310.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457310®Keywordsïµintegrity constraints∂inconsistency measures∞database repairs≠data cleaning∂inconsistent databases¶Badges¿•Track∞research-article®Citation ®DownloadÃñ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/MishraS21•titleŸ0RUSLI: Real-time Updatable Spline Learned Index.§year§2021£doiŸ'https://doi.org/10.1145/3464509.3464886©publisher´aiDM@SIGMODßauthorsí≠Mayank Mishra≠Rekha Singhal®Abstract⁄õMachine learning algorithms have accelerated data access through ‚Äòlearned index‚Äô, where a set of data items is indexed by a model learned on the pairs of data key and the corresponding record‚Äôs position in the memory. Most of the learned indexes require retraining of the model for new data insertions in the data set. The retraining is expensive and takes as much time as the model training. So, today, learned indexes are updated by retraining on batch inserts to amortize the cost. However, real-time applications, such as data-driven recommendation applications need to access users‚Äô feature store in real-time both for reading data of existing users and adding new users as well.  This motivates us to present a real-time updatable spline learned index, RUSLI, by learning the distribution of data keys with their positions in memory through splines. We have extended RadixSpline&nbsp;[8] to build the updatable learned index while supporting real-time inserts in a data set without affecting the lookup time on the updated data set. We have shown that RUSLI can update the index in constant time with an additional temporary memory of size proportional to the number of splines. We have discussed how to reduce the size of the presented index using the distribution of spline keys while building the radix table. RULSI is shown to incur &nbsp;270ns for lookup and &nbsp;50ns for insert operations. Further, we have shown that RUSLI supports concurrent lookup and insert operations with a throughput of &nbsp;40 million ops/sec. We have presented and discussed performance numbers of RUSLI for single and concurrent inserts, lookup, and range queries on SOSD&nbsp;[9] benchmark.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3464509.3464886®Keywordsì≠Learned index±Real-time inserts≤Spline-based index¶Badges¿•Track∞research-article®Citation ®DownloadÃï©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/YeLHLS21•titleŸFGPU-Accelerated Graph Label Propagation for Real-Time Fraud Detection.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452774©publisher±SIGMOD Conferenceßauthorsï®Chang YeÆYuchen Li 0001¨Bingsheng HeßZhao Li¨Jianling Sun®Abstract⁄HFraud detection is a pressing challenge for most financial and commercial platforms. In this paper, we study the processing pipeline of fraud detection in a large e-commerce platform of TaoBao. Graph label propagation (LP) is a core component in this pipeline to detect suspicious clusters from the user-interaction graph.Furthermore, the run-time of the LP component occupies 75% overhead of TaoBao's automated detection pipeline. To enable real-time fraud detection, we propose a GPU-based framework, called GLP, to support large-scale LP workloads in enterprises.We have identified two key challenges when integrating GPU acceleration into TaoBao's data processing pipeline: (1)programmability for evolving fraud detection logics; (2)demand for real-time performance. Motivated by these challenges, we offer a set of expressive APIs that data engineers can customize and deploy efficient LP algorithms on GPUs with ease. We propose novel GPU-centric optimizations by leveraging the community as well as power-law properties of large graphs. Extensive experiments have confirmed the effectiveness of our proposed optimizations. With a single GPU, GLP supports a real billion-scale graph workload from the fraud detection pipeline of TaoBao and achieves 8.2x speedup to the current in-house distributed solution running on high-end multicore machines.©VideoSizeß52.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452774&amp;file=3448016.3452774.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452774®Keywordsî±label propagationØfraud detection≥parallel algorithms≠gpu computing¶Badges¿•Track∞research-article®Citation ®DownloadÃï©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/WangXQ0O00I21•titleŸIConsistent and Flexible Selectivity Estimation for High-Dimensional Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452772©publisher±SIGMOD Conferenceßauthorsò´Yaoshu WangØChuan Xiao 0001´Jianbin Qin¨Rui Mao 0001ÆMakoto Onizuka≠Wei Wang 0011ÆRui Zhang 0003≤Yoshiharu Ishikawa®Abstract⁄∞Selectivity estimation aims at estimating the number of database objects that satisfy a selection criterion. Answering this problem accurately and efficiently is essential to many applications, such as density estimation, outlier detection, query optimization, and data integration. The estimation problem is especially challenging for large-scale high-dimensional data due to the curse of dimensionality, the large variance of selectivity across different queries, and the need to make the estimator consistent (i.e., the selectivity is non-decreasing in the threshold). We propose a new deep learning-based model that learns a query-dependent piecewise linear function as selectivity estimator, which is flexible to fit the selectivity curve of any distance function and query object, while guaranteeing that the output is non-decreasing in the threshold. To improve the accuracy for large datasets, we propose to partition the dataset into multiple disjoint subsets and build a local model on each of them. We perform experiments on real datasets and show that the proposed model consistently outperforms state-of-the-art models in accuracy in an efficient way and is useful for real applications.©VideoSizeß47.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452772&amp;file=3448016.3452772.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452772®Keywordsî∂selectivity estimationπpiecewise linear function≥deep neural networkµhigh-dimensional data¶Badges¿•Track∞research-article®Citation ®DownloadÃï©PaperRefsêã§Infoá§type´proceedings£key¥conf/sigmod/2021aidm•titleŸoaiDM '21: Fourth Workshop in Exploiting AI Techniques for Data Management, Virtual Event, China, 25 June, 2021.§year§2021£doiøhttps://doi.org/10.1145/3464509©publisher´aiDM@SIGMODßauthorsî±Rajesh Bordawekar∞Yael Amsterdamer¨Oded Shmueli≠Nesime Tatbul®Abstract∂No abstract available.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃï©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/PengWLBYXCRW21•titleŸXDataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457330©publisher±SIGMOD Conferenceßauthorsô¨Jinglin Peng™Weiyuan Wu∞Brandon LockhartÆSong Bian 0002ØJing Nathan Yan™Linghao Xu´Zhixuan Chi∑Jeffrey M. Rzeszotarski¨Jiannan Wang®Abstract⁄ˇExploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.©VideoSize®361.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457330&amp;file=3448016.3457330.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457330®Keywordsñ∞data explorationπexploratory data analysis¥statistical modelingÆdata profiling¶python∞data preparation¶Badges¿•Track∞research-article®Citation ®DownloadÃî©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/GalhotraGT21•titleŸ/Adaptive Rule Discovery for Labeling Text Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457334©publisher±SIGMOD Conferenceßauthorsì∞Sainyam GalhotraÆBehzad GolshanÆWang-Chiew Tan®Abstract⁄•Creating and collecting labeled data is one of the major bottlenecks in machine learning pipelines and the emergence of automated feature generation techniques such as deep learning, which typically requires a lot of training data, has further exacerbated the problem. While weak-supervision techniques have circumvented this bottleneck, existing frameworks either require users to write a set of diverse, high-quality rules to label data (e.g., Snorkel), or require a labeled subset of the data to automatically mine rules (e.g., Snuba). The process of manually writing rules can be tedious and time consuming. At the same time, creating a labeled subset of the data can be costly and even infeasible in imbalanced settings. To address these shortcomings, we present DARWIN, an interactive system designed to alleviate the task of writing rules for labeling text data in weakly-supervised settings. Given an initial labeling rule, DARWIN automatically generates a set of candidate rules for the labeling task at hand, and utilizes the annotator's feedback to adapt the candidate rules. We describe how DARWIN is scalable and versatile. It can operate over large text corpora (i.e., more than 1 million sentences) and supports a wide range of labeling functions (i.e., any function that can be specified using a context free grammar). Finally, we demonstrate with a suite of experiments over five real-world datasets that DARWIN enables annotators to generate weakly-supervised labels efficiently and with a small cost. In fact, our experiments show that rules discovered by DARWIN on average identify 40% more positive instances compared to Snuba even when it is provided with 1000 labeled instances.©VideoSizeß26.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457334&amp;file=3448016.3457334.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457334®Keywordsì∞weak supervision∂information extractionÆdata labelling¶Badges¿•Track∞research-article®Citation ®DownloadÃî©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/ChodpathumwanTR21•titleŸ;Structural Generalizability: The Case of Similarity Search.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457316©publisher±SIGMOD Conferenceßauthorsñ∏Yodsawalai ChodpathumwanØArash Termehchy±Stephen A. RamseyÆAayam Shrestha®Amy Glen©Zheng Liu®Abstract⁄!Supervised and Unsupervised ML algorithms are widely used over graphs. They use the structural properties of the data to deliver effective results. It is known that the same information can be represented under various graph structures. Thus, these algorithms may be effective on some structural variations of the data and ineffective on others. One would like to have an algorithm that is effective and generalizes to all structural variations of a data graph. We define the concept of structural generalizability for algorithms over graphs. We focus on the problem of similarity search, which is a popular task and the building block of many ML algorithms on graphs, and propose a structurally generalizable similarity search algorithm. As this algorithm may require users to specify features in a rather complex language, we modify this algorithm so that it requires only simple guidance from the user. Our extensive empirical study show that our algorithms are structurally generalizable while being efficient and more effective than current algorithms.©VideoSizeß29.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457316&amp;file=3448016.3457316.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457316®Keywordsì±similarity searchµstructural variationsªstructural generalizability¶Badges¿•Track∞research-article®Citation ®DownloadÃì©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/GevayQM21•titleŸ]The Power of Nested Parallelism in Big Data Processing - Hitting Three Flies with One Slap -.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457287©publisher±SIGMOD Conferenceßauthorsì∞G√°bor E. G√©vay∫Jorge-Arnulfo Quian√©-Ruiz¨Volker Markl®Abstract⁄ÉMany common data analysis tasks, such as performing hyperparameter optimization, processing a partitioned graph, and treating a matrix as a vector of vectors, offer natural opportunities for nested-parallel operations, i.e., launching parallel operations from inside other parallel operations. However, state-of-the-art dataflow engines, such as Spark and Flink, do not support nested parallelism. Users must implement workarounds, causing orders of magnitude slowdowns for their tasks, let alone the implementation effort. We present Matryoshka, a system that enables dataflow engines to support nested parallelism, even in the presence of control flow statements at inner nesting levels. Matryoshka achieves this via a novel two-phase flattening process, which translates nested-parallel programs to flat-parallel programs that can efficiently run on existing dataflow engines. The first phase introduces novel nesting primitives into the code, which allows for dynamic optimizations based on intermediate data characteristics in the second phase at runtime. We validate our system using several common data analysis tasks, such as PageRank and K-means.©VideoSize®126.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457287&amp;file=3448016.3457287.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457287®Keywordsìªnested parallel collections´nested data∫nested parallel operations¶Badges¿•Track∞research-article®Citation ®DownloadÃí©PaperRefsêã§Infoá§type´proceedings£key¥conf/sigmod/2021deem•titleŸƒProceedings of the Fifth Workshop on Data Management for End-To-End Machine Learning, In conjunction with the 2021 ACM SIGMOD/PODS Conference, DEEM@SIGMOD 2021, Virtual Event, China, 20 June, 2021§year§2021£doiøhttps://doi.org/10.1145/3462462©publisher´DEEM@SIGMODßauthorsì≥Matthias Boehm 0001±Julia Stoyanovich¨Steven Whang®Abstract∂No abstract available.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃí©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/0001021•titleŸ}Towards understanding end-to-end learning in the context of data: machine learning dancing over semirings &amp; Codd's table.§year§2021£doiŸ'https://doi.org/10.1145/3462462.3468878©publisher´DEEM@SIGMODßauthorsíÆWentao Wu 0001≠Ce Zhang 0001®Abstract⁄Recent advances in machine learning (ML) systems have made it incredibly easier to train ML models given a training set. However, our understanding of the behavior of the model training process has not been improving at the same pace. Consequently, a number of key questions remain: How can we systematically assign importance or value to training data with respect to the utility of the trained models, may it be accuracy, fairness, or robustness? How does noise in the training data, either injected by noisy data acquisition processes or adversarial parties, have an impact on the trained models? How can we find the right data that can be cleaned and labeled to improve the utility of the trained models? Just when we start to understand these important questions for ML models in isolation recently, we now have to face the reality that most real-world ML applications are way more complex than a single ML model. In this article---an extended abstract for an invited talk at the DEEM workshop---we will discuss our current efforts in revisiting these questions for an end-to-end ML pipeline, which consists of a noise model for data and a feature extraction pipeline, followed by the training of an ML model. In our opinion, this poses a unique challenge on the joint analysis of data processing and learning. Although we will describe some of our recent results towards understanding this interesting problem, this article is more of a "confession" on our technical struggles and a "cry for help" to our data management community.©VideoSize®309.1 MB©VideoLinkŸ]https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3462462.3468878&amp;file=a1-wu.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3462462.3468878®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadÃí©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/0001PM021•titleŸ#Spatial Independent Range Sampling.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452806©publisher±SIGMOD Conferenceßauthorsî≠Dong Xie 0001∞Jeff M. PhillipsØMichael MathenyÆFeifei Li 0001®Abstract⁄zThanks to the wide adoption of GPS-equipped devices, the volume of collected spatial data is exploding. To achieve interactive exploration and analysis over big spatial data, people are willing to trade off accuracy for performance through approximation. As a foundation in many approximate algorithms, data sampling now requires more flexibility and better performance. In this paper, we study the spatial independent range sampling (SIRS) problem aiming at retrieving random samples with independence over points residing in a query region. Specifically, we have designed concise index structures with careful data layout based on various space decomposition strategies. Moreover, we propose novel algorithms for both uniform and weighted SIRS queries with low theoretical cost and complexity as well as excellent practical performance. Last but not least, we demonstrate how to support data updates and trade-offs between different sampling methods in practice. According to comprehensive evaluations conducted on real-world datasets, our methods achieve orders of magnitude performance improvement against baselines derived by existing works.©VideoSizeß34.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452806&amp;file=3448016.3452806.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452806®KeywordsìÆrange samplingµspatial data sampling§sirs¶Badges¿•Track∞research-article®Citation ®DownloadÃí©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/LuYZ021•titleŸ9Graph Iso/Auto-morphism: A Divide-&amp;-Conquer Approach.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452820©publisher±SIGMOD Conferenceßauthorsî¶Can Lu≠Jeffrey Xu Yu¨Zhiwei ZhangØHong Cheng 0001®Abstract⁄˛The graph isomorphism is to determine whether two graphs are isomorphic. A closely related problem is graph automorphism (symmetry) detection, where an isomorphism between two graphs is a bijection between their vertex sets that preserves adjacency, and an automorphism is an isomorphism from a graph to itself. By graph automorphism, we deal with symmetric subgraph matching (SSM), which is to find all subgraphs in a graph G that are symmetric to a given subgraph q in G. To test two graphs for isomorphism, canonical labeling has been studied to relabel a graph in such a way that isomorphic graphs are identical after relabeling. Efficient canonical labeling algorithms are designed by individualization-refinement. They enumerate all permutations using a search tree, and select the minimum one as the canonical labeling. These algorithms face difficulties in handling massive graphs, and the search trees used are for pruning purposes which cannot answer symmetric subgraphs matching. In this paper, we design a new efficient canonical labeling algorithm DviCL based on the observation that we can use the k-th minimum permutation as the canonical labeling. Different from previous algorithms, we take a divide-and-conquer approach to partition a graph G. By partitioning G, an AutoTree is constructed, which preserves symmetric structures as well as the automorphism group of G. The canonical labeling for a tree node can be obtained by composing those of its child nodes, and the canonical labeling for the root is the one for G. Such AutoTree can also be effectively used to answer the automorphism group and symmetric subgraphs. We conducted extensive performance studies using 22 large graphs, and confirmed that DviCL is much more efficient and robust than the state-of-the-art.©VideoSizeß24.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452820&amp;file=3448016.3452820.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452820®Keywordsì∞graph algorithms≤graph automorphism±graph isomorphism¶Badges¿•Track∞research-article®Citation ®DownloadÃë©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/NesenB21•titleŸcTowards situational awareness with multimodal streaming data fusion: serverless computing approach.§year§2021£doiŸ'https://doi.org/10.1145/3460866.3461769©publisher≠BiDEDE@SIGMODßauthorsí´Alina Nesen≤Bharat K. Bhargava®Abstract⁄VThe availability of large quantities of data has given an impulse for methods and techniques to extract unseen useful knowledge and process it in a fast and scalable manner. In order to extract the most complete possible knowledge from the continuous data stream, it is necessary to use the heterogeneous data sources and process information from multiple modalities. The systems that utilize multimodal data must take advantage of the up-to-date approaches for data storage, usage, cleaning and storage. Neural networks and machine learning approaches are widely used for data-heavy software where pattern extractions and predictions need to be conducted while serverless computing frameworks are being increasingly used for machine learning solutions to optimize cost and speed of such systems. This work presents a framework for processing data from multimodal sources where the task of feature and pattern extraction is performed on a serverless computing platform. The use cases for public safety solutions to increase situational awareness are described and compared with other implementation approaches.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3460866.3461769®Keywordsìªmultimodal machine learning¥serverless computingµfunction-as-a-service¶Badges¿•Track∞research-article®Citation ®DownloadÃê©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/MuhligT21•titleŸDMxTasks: How to Make Efficient Synchronization and Prefetching Easy.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457268©publisher±SIGMOD Conferenceßauthorsí´Jan M√ºhlig¨Jens Teubner®Abstract⁄€The hardware environment has changed rapidly in recent years: Many cores, multiple sockets, and large amounts of main memory have become a commodity. To benefit from these highly parallel systems, the software has to be adapted. Sophisticated latch-free data structures and algorithms are often meant to address the situation. But they are cumbersome to develop and may still not provide the desired scalability. As a remedy, we present MxTasking, a task-based framework that assists the design of latch-free and parallel data structures. MxTasking eases the information exchange between applications and the operating system, resulting in novel opportunities to manage resources in a truly hardware- and application-conscious way.©VideoSize®155.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457268&amp;file=3448016.3457268.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457268®Keywordsì∂task-based parallelism´prefetchingØsynchronization¶Badges¿•Track∞research-article®Citation ®DownloadÃê©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/JiangLPCME21•titleŸ8Good to the Last Bit: Data-Driven Encoding with CodecDB.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457283©publisher±SIGMOD Conferenceßauthorsñ©Hao Jiang´Chunwei LiuØJohn PaparrizosØAndrew A. Chien©Jihong MaØAaron J. Elmore®Abstract⁄AColumnar databases rely on specialized encoding schemes to reduce storage requirements. These encodings also enable efficient in-situ data processing. Nevertheless, many existing columnar databases are encoding-oblivious. When storing the data, these systems rely on a global understanding of the dataset or the data types to derive simple rules for encoding selection. Such rule-based selection leads to unsatisfactory performance. Specifically, when performing queries, the systems always decode data into memory, ignoring the possibility of optimizing access to encoded data. We develop CodecDB, an encoding-aware columnar database, to demonstrate the benefit of tightly-coupling the database design with the data encoding schemes. CodecDB chooses in a principled manner the most efficient encoding for a given data column and relies on encoding-aware query operators to optimize access to encoded data. Storage-wise, CodecDB achieves on average 90% accuracy for selecting the best encoding and improves the compression ratio by up to 40% compared to the state-of-the-art encoding selection solution. Query-wise, CodecDB is on average one order of magnitude faster than the latest open-source and commercial columnar databases on the TPC-H benchmark, and on average 3x faster than a recent research project on the Star-Schema Benchmark (SSB).©VideoSizeß19.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457283&amp;file=3448016.3457283.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457283®Keywordsî´compression±columnar database≤encoding selection∞query processing¶Badges¿•Track∞research-article®Citation ®DownloadÃè©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/DixitK21•titleŸRCAvSAT: Answering Aggregation Queries over Inconsistent Databases via SAT Solving.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452749©publisher±SIGMOD ConferenceßauthorsíÆAkhil A. Dixit≥Phokion G. Kolaitis®Abstract⁄æConsistent Query Answering (CQA) is a rigorous and principled approach to answering queries posed against inconsistent databases. Computing consistent answers to a Select-Project-Join (SPJ) query or an SPJ query with aggregation operators on a given inconsistent database can be an intractable problem. We demonstrate CAvSAT, a system for CQA that leverages a set of natural reductions from a given CQA instance to boolean Satisfiability (SAT) and its optimization variants. CAvSAT is the first system that is capable of handling unions of SPJ queries with aggregation operators SUM and COUNT, and databases that are inconsistent w.r.t. key constraints, functional dependencies, and denial constraints.©VideoSizeß62.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452749&amp;file=3448016.3452749.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452749®Keywordsì∏range consistent answers∫consistent query answering´sat solving¶Badges¿•Track´short-paper®Citation ®DownloadÃé©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/NeutatzBA21•titleŸlEnforcing Constraints for Machine Learning Systems via Declarative Feature Selection: An Experimental Study.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457295©publisher±SIGMOD Conferenceßauthorsì≠Felix NeutatzØFelix Biessmann∞Ziawasch Abedjan®Abstract⁄1Responsible usage of Machine Learning (ML) systems in practice does not only require enforcing high prediction quality, but also accounting for other constraints, such as fairness, privacy, or execution time. One way to address multiple user-specified constraints on ML systems is feature selection. Yet, optimizing feature selection strategies for multiple metrics is difficult to implement and has been underrepresented in previous experimental studies. Here, we propose Declarative Feature Selection (DFS) to simplify the design and validation of ML systems satisfying diverse user-specified constraints. We benchmark and evaluate a representative series of feature selection algorithms. From our extensive experimental results, we derive concrete suggestions on when to use which strategy and show that a meta-learning-driven optimizer can accurately predict the right strategy for an ML task at hand. These results demonstrate that feature selection can help to build ML systems that meet combinations of user-specified constraints, independent of the ML methods used.©VideoSize®106.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457295&amp;file=3448016.3457295.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457295®Keywordsõ§bias≠meta learning™robustness±feature selection®fairnessºdeclarative machine learningßprivacyÆdeclarative ml£DFS∞machine learningΩdeclarative feature selection¶Badges¿•Track∞research-article®Citation ®DownloadÃç©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/TangLLCYZ21•titleŸ4Learning-Aided Heuristics Design for Storage System.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457554©publisher±SIGMOD Conferenceßauthorsñ≠Yingtian Tang¶Han Lu®Xijun Li®Lei Chen≠Mingxuan Yuan®Jia Zeng®Abstract⁄ÅComputer systems such as storage systems normally require transparent white-box algorithms that are interpretable for human experts. In this work, we propose a learning-aided heuristic design method, which automatically generates human-readable strategies from Deep Reinforcement Learning (DRL) agents. This method benefits from the power of deep learning but avoids the shortcoming of its black-box property. Besides the white-box advantage, experiments in our storage production's resource allocation scenario also show that this solution outperforms the system's default settings and the elaborately handcrafted strategy by human experts.©VideoSize®327.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457554&amp;file=3448016.3457554.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457554®Keywordsìªreal-time operating systems∂reinforcement learning≠rule learning¶Badges¿•Track∞research-article®Citation ®DownloadÃå©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/0019ANNW21•titleŸGEIRES: Efficient Integration of Remote Data in Event Stream Processing.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457304©publisher±SIGMOD Conferenceßauthorsï¨Bo Zhao 0019ÆHan van der Aa∞Thanh Tam NguyenµQuoc Viet Hung Nguyen±Matthias Weidlich®Abstract⁄ŒTo support reactive and predictive applications, complex event processing (CEP) systems detect patterns in event streams based on predefined queries. To determine the events that constitute a query match, their payload data may need to be assessed together with data from remote sources. Such dependencies are problematic, since waiting for remote data to be fetched interrupts the processing of the stream. Yet, without event selection based on remote data, the query state to maintain may grow exponentially. In either case, the performance of the CEP system degrades drastically. To tackle these issues, we present EIRES, a framework for efficient integration of static data from remote sources in CEP. It employs a cost-model to determine when to fetch certain remote data elements and how long to keep them in a cache for future use. EIRES combines strategies for (i) prefetching that queries remote data based on anticipated use and (ii) lazy evaluation that postpones the event selection based on remote data without interrupting the stream processing. Our experiments indicate that the combination of these strategies improves the latency of query evaluation by up to 3,725x for synthetic data and 47x for real-world data.©VideoSize®204.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457304&amp;file=3448016.3457304.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457304®Keywordsì∏complex event processing∞data prefetching¥query result caching¶Badges¿•Track∞research-article®Citation ®DownloadÃà©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/Crooks21•titleŸ6A Client-centric Approach to Transactional Datastores.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3461471©publisher±SIGMOD ConferenceßauthorsëÆNatacha Crooks®Abstract⁄‡Traditional brick-and-mortar services are increasingly moving online and companies rely on ever larger data analytics to optimise their business logic. Similarly, most medical practices favour electronic health records over paper documents: 84% of American hospitals store medical records electronically [19], an 8-fold increase since 2008. In this data-driven world, data is money. It must be collected efficiently, even as it spans multiple heterogeneous, geo-distributed sources. Data must be stored reliably, even in the presence of failures. As the sensitivity of the data being stored increases, so does the need to store it securely in the presence of human attacks. Data must also be accessed consistently, even under high load.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3461471®Keywordsîßprivacy¨transactions≥distributed systems´consistency¶Badges¿•Track¨invited-talk®Citation ®DownloadÃÖ©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/ShiZP0P21•titleŸ1At-the-time and Back-in-time Persistent Sketches.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452802©publisher±SIGMOD Conferenceßauthorsï™Benwei Shi¨Zhuoyue Zhao¨Yanqing PengÆFeifei Li 0001∞Jeff M. Phillips®Abstract⁄aIn the era of big data, more and more applications require the information of historical data to support rich analytics, learning, and mining operations. In these cases, it is highly desirable to retrieve information of previous versions of data. Traditionally, multi-version databases can be used to store all historical values of the data in order to support historical queries. However, storing all the historical data can be impractical due to its large space consumption. In this paper, we propose the concept of at-the-time persistent (ATTP) and back-in-time persistent (BITP) sketches, which are sketches that approximately answer queries on previous versions of data with small space. We then provide several implementations of ATTP/BITP sketches which are shown to be more efficient compared to existing state-of-the-art solutions in our empirical studies.©VideoSize®173.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452802&amp;file=3448016.3452802.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452802®KeywordsîØrandom sampling¥streaming algorithmsπpersistent data structureÆdata sketching¶Badges¿•Track∞research-article®Citation ®DownloadÃÑ©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/Guo21•titleŸ8Learning Algorithms for Automatic Data Structure Design.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450570©publisher±SIGMOD Conferenceßauthorsë®Demi Guo®Abstract⁄We present practical learning-based search algorithms that make it possible to automatically design key-value data structures. Our work allows searching within a space of 10100 possible designs for the optimal data structure. The input is a workload specification and the output is an abstract syntax tree which can be absorbed by modern code-generation techniques. Given the vast design space our solution is based on learning and we show that it can find the close to optimal data structure design in a matter of a few seconds.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450570®Keywordsì¨data systems∞machine learningÆdata structure¶Badges¿•Track®abstract®Citation ®DownloadÃÑ©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Wang021•titleŸ<Secure Yannakakis: Join-Aggregate Queries over Private Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452808©publisher±SIGMOD Conferenceßauthorsí™Yilei Wang™Ke Yi 0001®Abstract⁄ËIn this paper, we describe a secure version of the classical Yannakakis algorithm for computing free-connex join-aggregate queries. This protocol can be used in the secure two-party computation model, where the parties would like to evaluate a query without revealing their own data. Our protocol presents a dramatic improvement over the state-of-the-art protocol based on Yao's garbled circuit. In theory, its cost (both running time and communication) is linear in data size and polynomial in query size, whereas that of the garbled circuit is polynomial in data size and exponential in query size. This translates to a reduction in running time in practice from years to minutes, as tested on a number of TPC-H queries of varying complexity.©VideoSizeß97.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452808&amp;file=3448016.3452808.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452808®KeywordsìÆjoin algorithm∞query processingæsecure multi-party computation¶Badges¿•Track∞research-article®Citation ®DownloadÃÑ©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/Zhao21•titleŸcEfficiently Supporting Adaptive Multi-Level Serializability Models in Distributed Database Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450579©publisher±SIGMOD Conferenceßauthorsë¨Zhanhao Zhao®Abstract⁄ŸInformally, serializability means that transactions appear to have occurred in some total order. In this paper, we show that only the serializability guarantee with some total order is not enough for many real applications. As a complement, extra partial orders of transactions, like real-time order and program order, need to be introduced. Motivated by this observation, we present a framework that models serializable transactions by adding extra partial orders, namely multi-level serializability models. Following this framework, we propose a novel concurrency control algorithm, called bi-directionally timestamp adjustment (BDTA), to supporting multi-level serializability models in distributed database systems. We integrate the framework and BDTA into Greenplum and Deneva to show the benefits of our work. Our experiments show the performance gaps among serializability levels and confirm BDTA achieves up to 1.7√ó better than state-of-the-art concurrency control algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450579®KeywordsîØserializability®database¨transactions±consistency model¶Badges¿•Track®abstract®Citation ®DownloadÃÇ©PaperRefsêã§Infoá§type≠inproceedings£keyºconf/sigmod/KalamatianosFM21•titleŸ*Proportionality in Spatial Keyword Search.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457309©publisher±SIGMOD ConferenceßauthorsìµGeorgios Kalamatianos≥Georgios John FakasÆNikos Mamoulis®Abstract⁄PMore often than not, spatial objects are associated with some context, in the form of text, descriptive tags (e.g. points of interest, flickr photos), or linked entities in semantic graphs (e.g. Yago2, DBpedia). Hence, location-based retrieval should be extended to consider not only the locations but also the context of the objects, especially when the retrieved objects are too many and the query result is overwhelming. In this paper, we study the problem of selecting a subset of the query result, which is the most representative. We argue that objects with similar context and nearby locations should proportionally be represented in the selection. Proportionality dictates the pairwise comparison of all retrieved objects and hence bears a high cost. We propose novel algorithms which greatly reduce the cost of proportional object selection in practice. Extensive empirical studies on real datasets show that our algorithms are effective and efficient. A user evaluation verifies that proportional selection is more preferable than random selection and selection based on object diversification.©VideoSizeß65.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457309&amp;file=3448016.3457309.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457309®Keywordsñßranking©diversityªPtolemy's spatial diversityØproportionality¨spatial dataÆkeyword search¶Badges¿•Track∞research-article®Citation ®DownloadÃÅ©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/Fariha0MRG21•titleŸbCoCo: Interactive Exploration of Conformance Constraints for Data Understanding and Data Cleaning.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452750©publisher±SIGMOD Conferenceßauthorsï´Anna Fariha≤Ashish Tiwari 0001∞Alexandra Meliou≤Arjun Radhakrishna≠Sumit Gulwani®Abstract⁄{Data profiling refers to the task of extracting technical metadata or profiles and has numerous applications such as data understanding, validation, integration, and cleaning. While a number of data profiling primitives exist in the literature, most of them are limited to categorical attributes. A few techniques consider numerical attributes; but, they either focus on simple relationships involving a pair of attributes (e.g., correlations) or convert the continuous semantics of numerical attributes to a discrete semantics, which results in information loss. To capture more complex relationships involving the numerical attributes, we developed a new data-profiling primitive called conformance constraints, which can model linear arithmetic relationships involving multiple numerical attributes. We present CoCo, a system that allows interactive discovery and exploration of Conformance Constraints for understanding trends involving the numerical attributes of a dataset, with a particular focus on the application of data cleaning. Through a simple interface, CoCo enables the user to guide conformance constraint discovery according to their preferences. The user can examine to what extent a new, possibly dirty, dataset satisfies or violates the discovered conformance constraints. Further, CoCo provides useful suggestions for cleaning dirty data tuples, where the user can interactively alter cell values, and verify by checking change in conformance constraint violation due to the alteration. We demonstrate how CoCo can help in understanding trends in the data and assist the users in interactive data cleaning, using conformance constraints.©VideoSizeß54.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452750&amp;file=3448016.3452750.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452750®Keywordsì∑conformance constraints≠data cleaning¨data profile¶Badges¿•Track´short-paper®Citation ®DownloadÃÄ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/0016MH21•titleŸ:On m-Impact Regions and Standing Top-k Influence Problems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452832©publisher±SIGMOD Conferenceßauthorsì¨Bo Tang 0016≥Kyriakos Mouratidis™Mingji Han®Abstract⁄uIn this paper, we study the m-impact region problem (mIR). In a context where users look for available products with top-k queries, mIR identifies the part of the product space that attracts the most user attention. Specifically, mIR determines the kind of attribute values that lead a (new or existing) product to the top-k result for at least a fraction of the user population. mIR has several applications, ranging from effective marketing to product improvement. Importantly, it also leads to (exact and efficient) solutions for standing top-k impact problems, which were previously solved heuristically only, or whose current solutions face serious scalability limitations. We experiment, among others, on data mined from actual user reviews for real products, and demonstrate the practicality and efficiency of our algorithms, both for mIR and for standing top-k impact problems.©VideoSizeß34.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452832&amp;file=3448016.3452832.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452832®KeywordsìØmarket analysis∫multi-dimensional datasets´top-k query¶Badges¿•Track∞research-article®Citation ®DownloadÃÄ©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/ZhangLBZJ21•titleŸ/Minimizing the Regret of an Influence Provider.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457257©publisher±SIGMOD Conferenceßauthorsï¨Yipeng ZhangÆYuchen Li 0001´Zhifeng Bao¨Baihua ZhengÆH. V. Jagadish®Abstract⁄pInfluence maximization has been studied extensively from the perspective of the influencer. However, the influencer typically purchases influence from a provider, for example in the form of purchased advertising. In this paper, we study the problem from the perspective of the influence provider. Specifically, we focus on influence providers who sell Out-of-Home (OOH) advertising on billboards. Given a set of requests from influencers, how should an influence provider allocate resources to minimize regret, whether due to forgone revenue from influencers whose needs were not met or due to over-provisioning of resources to meet the needs of influencers? We formalize this as the \underlineM inimizing \underlineR egret for the \underlineO OH \underlineA dvertising \underlineM arket problem (\problem). We show that \problem is both NP-hard and NP-hard to approximate within any constant factor. The regret function is neither monotone nor submodular, which renders any straightforward greedy approach ineffective. Therefore, we propose a randomized local search framework with two neighborhood search strategies, and prove that one of them ensures an approximation factor to a dual problem of \problem. Experiments on real-world user movement and billboard datasets in New York City and Singapore show that on average our methods outperform the baselines in effectiveness by five times.©VideoSize®133.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457257&amp;file=3448016.3457257.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457257®Keywordsì≤influence provider≥regret minimization≥outdoor advertising¶Badges¿•Track∞research-article®Citation ®DownloadÃÄ©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Helal21•titleŸ5Data Lakes Empowered by Knowledge Graph Technologies.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450584©publisher±SIGMOD Conferenceßauthorsë´Ahmed Helal®Abstract⁄‚With the emergence of open data, governments [19, 25], Kaggle [13], OpenML [26], and organizations [2, 20] have started making data available on the web. This data represents an opportunity for Artificial Intelligence (AI) practitioners to accomplish their tasks including but not limited to improving the performance of models in Machine Learning or predicting insights about the future in Scenario Planning. However with the proliferation of available data, finding the most relevant one is time-consuming and cumbersome. Practitioners tend to spend considerable time looking for data to enrich their existing datasets to accomplish their tasks. For instance, in Deep Learning, engineers need a lot of data to train their models [27]. Moreover, they face several problems such as increasing the size of their datasets with similar data or including more features that can contribute to better results [3, 8, 27]. These problems emerge because data lakes are schema-agnostic repositories [30].©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450584®Keywordsî£RDF∞knowledge graphs¶schemaØstructured data¶Badges¿•Track®abstract®Citation ®Download~©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/PalaciosZBA21•titleŸ3Auditable serverless computing for farm management.§year§2021£doiŸ'https://doi.org/10.1145/3460866.3461770©publisher≠BiDEDE@SIGMODßauthorsîØServio Palacios≠Drew ZabrockiØBharat BhargavaØVaneet Aggarwal®Abstract⁄&Currently, a multitude of applications that target farm management activities has been proposed. Unfortunately, those applications do not interoperate and require that the farmer utilize several of them, making operations and task management cumbersome. Similarly, those applications require a tiered pricing model ranging from a restricted/limited free tier to several thousand dollars per year---mainly in a software as a service SaaS model. In this paper, we propose a novel mix of serverless functions, shared ledgers, webhooks, and REST APIs to enhance Agriculture/Farm Management Systems, providing an integrated solution for Task, User, and Field Management that exploits a fine-grained pricing model. Further, our technique utilizes serverless oblivious smart contracts as a building block. To the best of our knowledge, this is the first solution that leverages serverless functions and shared ledgers to provide an elastic, pay-as-you-go, and auditable task management system for the AG industry. Our work has a significant impact on providing an open-source solution released and used in production that can pave the way for future relevant ideas in the AG industry. Therefore, we demonstrate the system feasibility exposing evidence of the system performance for auditable task creation and chat mirroring.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3460866.3461770®Keywordsñæauditable serverless computingØfarm management¥serverless functions¥serverless computingŸ$serverless oblivious smart contractsπauditable farm management¶Badges¿•Track∞research-article®Citation ®Download}©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Dong021•titleŸ@Residual Sensitivity for Differentially Private Multi-Way Joins.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452813©publisher±SIGMOD Conferenceßauthorsí®Wei Dong™Ke Yi 0001®Abstract⁄hA general-purpose query engine that supports a large class of SQLs under differential privacy is the holy grail in privacy-preserving query release. The join operator presents a major difficulty towards realizing this goal, since a single tuple may affect a large number of query results, and the problem worsens as more relations are involved in the join. The traditional approach of global sensitivity fails to work as it assumes pessimistically that every pair of tuples from two different relations may join. To address the issue, instance-dependent sensitivity measures have been proposed, but so far none has met the following three desiderata for it to be truly practical: (1) the released answer should have low noise levels (i.e., high utility); (2) it can be computed efficiently; and (3) the method can be easily integrated into an existing relational database. This paper presents the first differentially private mechanism for multi-way joins that satisfies all three desiderata while supporting any number of private relations, moving us one step closer to a full-featured query engine for private relational data.©VideoSize®157.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452813&amp;file=3448016.3452813.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452813®KeywordsìÆcounting query§join¥differential privacy¶Badges¿•Track∞research-article®Citation ®Download{©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/Chen0KY21•titleŸ-Evaluating Temporal Queries Over Video Feeds.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452803©publisher±SIGMOD Conferenceßauthorsî¨Yueting ChenØXiaohui Yu 0001´Nick Koudas™Ziqiang Yu®Abstract⁄ëRecent advances in Computer Vision and Deep Learning have made possible the efficient extraction of structured information from frames of video feeds. As such, a stream of objects and their associated classes along with unique object identifiers derived via object tracking can be generated, providing unique objects as they are captured across frames. In this paper we initiate a study of temporal queries involving objects and their co-occurrences in video feeds. For example, queries that identify video segments during which the same two red cars and the same two humans appear jointly for five minutes are of interest to many applications ranging from law enforcement to security and safety. We take the first step and define such queries in a way that they incorporate certain physical aspects of video capture such as object occlusion. We present an architecture consisting of three layers, namely object detection/tracking, intermediate data generation, and query evaluation. We propose two techniques, Marked Frame Set (MFS) and Sparse State Graph (SSG), to organize all detected objects in the intermediate data generation layer, which effectively, given the queries, minimizes the number of objects and frames that have to be considered during query evaluation. We also introduce an algorithm called SSG-CM that processes incoming frames against the SSG and efficiently prunes objects and frames unrelated to query evaluation, while maintaining all states required for succinct query evaluation. We present the results of a thorough experimental evaluation utilizing both real and synthetic data, establishing the trade-offs between MFS and SSG. We stress various parameters of interest in our evaluation and demonstrate that the proposed query evaluation methodology coupled with the proposed algorithms is capable to evaluate temporal queries over video feeds efficiently, achieving orders of magnitude performance benefits.©VideoSize®120.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452803&amp;file=3448016.3452803.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452803®KeywordsìØdata management≠video queries∞temporal queries¶Badges¿•Track∞research-article®Citation ®Downloadz©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/DaiDHS21•titleŸgActive Sampling Count Sketch (ASCS) for Online Sparse Estimation of a Trillion Scale Covariance Matrix.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457327©publisher±SIGMOD Conferenceßauthorsî´Zhenwei Dai¨Aditya DesaiØReinhard HeckelµAnshumali Shrivastava®Abstract⁄Estimating and storing the covariance (or correlation) matrix of high-dimensional data is computationally challenging because both memory and computational requirements scale quadratically with the dimension. Fortunately, high-dimensional covariance matrices as observed in text, click-through, meta-genomics datasets, etc are often sparse. In this paper, we consider the problem of efficient sparse estimation of covariance matrices with possibly trillions of entries. The size of the datasets we target requires the algorithm to be online, as more than one pass over the data is prohibitive. In this paper, we propose Active Sampling Count Sketch (ASCS), an online and one-pass sketching algorithm, that recovers the large entries of the covariance matrix accurately. Count Sketch (CS), and other sub-linear compressed sensing algorithms, offer a natural solution to the problem in theory. However, vanilla CS does not work well in practice due to a low signal-to-noise ratio (SNR). At the heart of our approach is a novel active sampling strategy that increases the SNR of classical CS. We demonstrate the practicality of our algorithm with synthetic data and real-world high dimensional datasets. ASCS significantly improves over vanilla CS, demonstrating the merit of our active sampling strategy.©VideoSize®119.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457327&amp;file=3448016.3457327.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457327®Keywordsìºcovariance matrix estimationØactive sampling¨count sketch¶Badges¿•Track∞research-article®Citation ®Downloadv©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/SchelterGD21•titleŸJHedgeCut: Maintaining Randomised Trees for Low-Latency Machine Unlearning.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457239©publisher±SIGMOD Conferenceßauthorsì≤Sebastian Schelter±Stefan Grafberger´Ted Dunning®Abstract⁄ÂSoftware systems that learn from user data with machine learning (ML) have become ubiquitous over the last years. Recent law such as the "General Data Protection Regulation" (GDPR) requires organisations that process personal data to delete user data upon request (enacting the "right to be forgotten"). However, this regulation does not only require the deletion of user data from databases, but also applies to ML models that have been learned from the stored data. We therefore argue that ML applications should offer users to unlearn their data from trained models in a timely manner. We explore how fast this unlearning can be done under the constraints imposed by real world deployments, and introduce the problem of low-latency machine unlearning: maintaining a deployed ML model in-place under the removal of a small fraction of training samples without retraining. We propose HedgeCut, a classification model based on an ensemble of randomised decision trees, which is designed to answer unlearning requests with low latency. We detail how to efficiently implement HedgeCut with vectorised operators for decision tree learning. We conduct an experimental evaluation on five privacy-sensitive datasets, where we find that HedgeCut can unlearn training samples with a latency of around 100 microseconds and answers up to 36,000 prediction requests per second, while providing a training time and predictive accuracy similar to widely used implementations of tree-based ML models such as Random Forests.©VideoSizeß46.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457239&amp;file=3448016.3457239.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457239®KeywordsìÆdecision treesØserving systems≤machine unlearning¶Badges¿•Track∞research-article®Citation ®Downloadv©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/BuonoPSI021•titleŸ8Transforming ML Predictive Pipelines into SQL with MASQ.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452771©publisher±SIGMOD Conferenceßauthorsï≥Francesco Del Buono∞Matteo PaganelliÆPaolo Sottovia±Matteo InterlandiµFrancesco Guerra 0001®Abstract⁄ﬁInference of Machine Learning (ML) models, i.e. the process of obtaining predictions from trained models, is often an overlooked problem. Model inference is however one of the main contributors of both technical debt in ML applications and infrastructure complexity. MASQ is a framework able to run inference of ML models directly on DBMSs. MASQ not only averts expensive data movements for those predictive scenarios where data resides on a database, but it also naturally exploits all the "Enterprise-grade" features such as governance, security and auditability which make DBMSs the cornerstone of many businesses. MASQ compiles trained models and ML pipelines implemented in scikit-learn directly into standard SQL: no UDFs nor vendor-specific syntax are used, and therefore queries can be readily executed on any DBMS. In this demo, we will showcase MASQ's capabilities through a GUI allowing attendees to: (1) train ML pipelines composed of data featurizers and ML models; (2) compile the trained pipelines into SQL, and deploy them on different DBMSs (MySQL and SQLServer in the demo); and (3) compare the related performance under different configurations (e.g., the original pipeline on the ML framework against the SQL implementations).©VideoSizeß38.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452771&amp;file=3448016.3452771.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452771®Keywordsì∞machine learning∂in-rdbms ml prediction¥relational databases¶Badges¿•Track´short-paper®Citation ®Downloadv©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Schleich21•titleŸAStructure-Aware Machine Learning over Multi-Relational Databases.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3461670©publisher±SIGMOD Conferenceßauthorsë≥Maximilian Schleich®Abstract⁄\We consider the problem of computing machine learning models over multi-relational databases. The mainstream approach involves a costly repeated loop that data scientists have to deal with on a daily basis: select features from data residing in relational databases using feature extraction queries involving joins, projections, and aggregations; export the training dataset defined by such queries; convert this dataset into the format of an external learning tool; and learn the desired model using this tool. In this thesis, we advocate for an alternative approach that avoids this loop and instead tightly integrates the query and learning tasks into one unified solution. By integrating these two tasks, we can exploit structure in the data and the query to optimize the end-to-end learning problem. We provide a framework for structure-aware learning for a variety of commonly used machine learning models that achieves runtime guarantees that can be asymptotically faster than the mainstream approach that first constructs the training dataset. In practice, this asymptotic gap translates into several orders of magnitude performance improvements over state-of-the-art machine learning packages such as TensorFlow, MADlib, scikit-learn, and mlpack. The thesis is composed of three parts. First, we present the methodology and theoretical foundation of structure-aware learning. Then, we report on the design and implementation of LMFAO, an in-memory engine for structure-aware learning over databases. Finally, we present an extensive experimental evaluation. In following, we briefly highlight each of these three parts.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3461670®Keywordsî∏machine learning systemsªfactorized query evaluation≤query optimizationŸ$data management for machine learning¶Badges¿•Track¨invited-talk®Citation ®Downloadu©PaperRefsêã§Infoá§type≠inproceedings£keyªconf/sigmod/GrafbergerGSS21•titleŸGMLINSPECT: A Data Distribution Debugger for Machine Learning Pipelines.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452759©publisher±SIGMOD Conferenceßauthorsî±Stefan Grafberger´Shubha Guha±Julia Stoyanovich≤Sebastian Schelter®Abstract⁄Machine Learning (ML) is increasingly used to automate impactful decisions, and the risks arising from this wide-spread use are garnering attention from policymakers, scientists, and the media. ML applications are often very brittle with respect to their input data, which leads to concerns about their reliability, accountability, and fairness. While bias detection cannot be fully automated, computational tools can help pinpoint particular types of data issues. We recently proposed mlinspect, a library that enables lightweight lineage-based inspection of ML preprocessing pipelines. In this demonstration, we show how mlinspect can be used to detect data distribution bugs in a representative pipeline. In contrast to existing work, mlinspect operates on declarative abstractions of popular data science libraries like estimator/transformer pipelines, can handle both relational and matrix data, and does not require manual code instrumentation. The library is publicly available at https://github.com/stefan-grafberger/mlinspect.©VideoSize®294.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452759&amp;file=3448016.3452759.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452759®Keywordsîªdata distribution debugging∫machine learning pipelines∏responsible data scienceÆtechnical bias¶Badges¿•Track´short-paper®Citation ®Downloadt©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/WuGWZ21•titleŸYUnifying the Global and Local Approaches: An Efficient Power Iteration with Forward Push.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457298©publisher±SIGMOD Conferenceßauthorsî¶Hao Wu™Junhao Gan™Zhewei Wei©Rui Zhang®Abstract⁄[Personalized PageRank (PPR) is a critical measure of the importance of a node t to a source node s in a graph. The Single-Source PPR (SSPPR) query computes the PPR's of all the nodes with respect to s on a directed graph G with n nodes and m edges; and it is an essential operation widely used in graph applications. In this paper, we propose novel algorithms for answering two variants of SSPPR queries: (i) high-precision queries and (ii) approximate queries. For high-precision queries, Power Iteration (PowItr) and Forward Push (FwdPush) are two fundamental approaches. Given an absolute error threshold Œª (which is typically set to as small as 10-8), the only known bound of FwdPush is O(m/Œª), much worse than the O(m log 1/Œª)-bound of PowItr. Whether FwdPush can achieve the same running time bound as PowItr does still remains an open question in the research community. We give a positive answer to this question. We show that the running time of a common implementation of FwdPush is actually bounded by O(m ¬∑ log 1/Œª). Based on this finding, we propose a new algorithm, called Power Iteration with Forward Push (PowerPush), which incorporates the strengths of both PowItr and FwdPush. For approximate queries (with a relative error Œµ), we propose a new algorithm, called SpeedPPR, with overall expected time bounded by $O(n ¬∑ log n ¬∑ log 1/Œµ) on scale-free graphs. This improves the state-of-the-art O((n ¬∑ log n)/Œµ) bound. We conduct extensive experiments on six real datasets. The experimental results show that PowerPush outperforms the state-of-the-art high-precision algorithm BePi by up to an order of magnitude in both efficiency and accuracy. Furthermore, our SpeedPPR also outperforms the state-of-the-art approximate algorithm FORA by up to an order of magnitude in all aspects including query time, accuracy, pre-processing time as well as index size.©VideoSize®562.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457298&amp;file=3448016.3457298.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457298®Keywordsìµpersonalized pagerank¨forward pushØpower iteration¶Badges¿•Track∞research-article®Citation ®Downloadt©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/WangBLJLC21•titleŸ^Towards Enhancing Database Education: Natural Language Generation Meets Query Execution Plans.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452822©publisher±SIGMOD Conferenceßauthorsñ´Weiguo Wang≤Sourav S. Bhowmick¶Hui LiÆShafiq R. Joty™Siyuan Liu©Peng Chen®Abstract⁄)The database systems course is offered as part of an undergraduate computer science degree program in many major universities. A key learning goal of learners taking such a course is to understand how sql queries are processed in a rdbms in practice. Since aquery execution plan (qep ) describes the execution steps of a query, learners can acquire the understanding by perusing the qep s generated by a rdbms. Unfortunately, in practice, it is often daunting for a learner to comprehend these qep s containing vendor-specific implementation details, hindering her learning process. In this paper, we present a novel, end-to-end,generic system called lantern that generates a natural language description of a qep to facilitate understanding of the query execution steps. It takes as input an sql query and its qep, and generates a natural language description of the execution strategy deployed by the underlying rdbms. Specifically, it deploys adeclarative framework called pool that enablessubject matter experts to efficiently create and maintain natural language descriptions of physical operators used in qep s. Arule-based framework called rule-lantern is proposed that exploits pool to generate natural language descriptions of qep s. Despite the high accuracy of rule-lantern, our engagement with learners reveal that, consistent with existing psychology theories, perusing such rule-based descriptions lead toboredom due to repetitive statements across different qep s. To address this issue, we present a noveldeep learning-based language generation framework called neural -lantern that infuses language variability in the generated description by exploiting a set ofparaphrasing tools andword embedding. Our experimental study with real learners shows the effectiveness of lantern in facilitating comprehension of qep s.©VideoSizeß50.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452822&amp;file=3448016.3452822.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452822®Keywordsó¥rule-based frameworkºnatural language description¥query execution plan∞neural framework¥relational databases≤database education®learners¶Badges¿•Track∞research-article®Citation ®Downloadt©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/BalakrishnanNKZ21•titleŸ/TreeToaster: Towards an IVM-Optimized Compiler.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3459244©publisher±SIGMOD ConferenceßauthorsîµDarshana Balakrishnan¨Carl NuessleÆOliver Kennedy≠Lukasz Ziarek®Abstract⁄_A compiler's optimizer operates over abstract syntax trees (ASTs), continuously applying rewrite rules to replace subtrees of the AST with more efficient ones. Especially on large source repositories, even simply finding opportunities for a rewrite can be expensive, as optimizer traverses the AST naively. In this paper, we leverage the need to repeatedly find rewrites, and explore options for making the search faster through indexing and incremental view maintenance (IVM). Concretely, we consider bolt-on approaches that make use of embedded IVM systems like DBToaster, as well as two new approaches: Label-indexing and TreeToaster, an AST-specialized form of IVM. We integrate these approaches into an existing just-in-time data structure compiler and show experimentally that TreeToaster can significantly improve performance with minimal memory overheads.©VideoSizeß17.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3459244&amp;file=3448016.3459244.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3459244®Keywordsî®indexingµabstract syntax tress©compilersºincremental view maintenance¶Badges¿•Track∞research-article®Citation ®Downloadr©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/Lai21•titleŸMEfficient Deterministic Concurrency Control Under Practical Isolation Levels.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450580©publisher±SIGMOD Conferenceßauthorsë´Ziliang Lai®Abstract⁄9Deterministic databases are able to run transactions efficiently in a distributed setting with minimal coordination. Research has shown many benefits from having determinism, from more lightweight database replication, to no/cheaper distributed commit, to high throughput during live migration. Besides those known applications, permissioned blockchain can also be viewed as an application of the deterministic database with security added. A permissioned blockchain maintains a fully-replicated database, where each replica is synchronized by getting blocks of updates. Early permissioned blockchains run a consensus protocol (e.g., PBFT) to agree on the input and rely on the replicas to execute the transactions serially to uphold determinism. Fabric allows concurrent transaction executions but rely on running consensus to agree on the read-write-sets and broadcasting them, which incurs an excessive volume of network traffic. Deterministic concurrency control combines the best of both worlds. It enables solving consensus on the small input, and each replica is able to execute transactions independently with high concurrency. Although powerful, contemporary deterministic databases are still primitive because they support only one isolation level -- Serializable. Furthermore, many deterministic concurrency control schemes are overly pessimistic and cause many unnecessary transaction aborts. These factors motivate us to design DCC, a suite of better deterministic concurrency control schemes, not only for Serializable but also for practical isolation levels beyond Serializable.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450580®KeywordsíŸ!deterministic concurrency control™blockchain¶Badges¿•Track®abstract®Citation ®Downloadp©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Bonifati21•titleŸ,Graph processing systems back to the future.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464687©publisher±GRADES-NDA@SIGMODßauthorsëØAngela Bonifati®Abstract⁄PGraphs are data model abstractions that are becoming pervasive in several real-life applications and use cases. In these settings, users focus on entities and their relationships, further enhanced with multiple labels and properties to form the so called property graphs. Modern graph processing systems need to keep pace with the increasing fundamental requirements of these applications and to tackle unforeseen challenges. Motivated by a community vision on future graph processing systems [6], in this talk I will present the system challenges that are lying behind the current research topics on graph processing and graph analytics. Many current graph query engines support subsets of graph queries that they can efficiently evaluate, thus disregarding more expressive query fragments on top of property graphs [2]. It becomes crucial to address efficient query evaluation for complex graph queries, as well the extensibility of the underlying graph query and constraint languages [1, 3]. Moreover, the dynamic aspects [5] of evaluating queries on streaming graphs are equally important and need to be considered in ongoing and future benchmarking efforts [4]. The overarching goal of my talk is to touch upon our past and ongoing work on these topics and to pinpoint the research directions shaping the already bright future of graph processing systems.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464687®KeywordsñØgraph streaming¥regular path queries≠graph queriesØproperty graphs∞graph processingØgraph analytics¶Badges¿•Track¨invited-talk®Citation ®Downloado©PaperRefsêã§Infoá§type´proceedings£key∂conf/sigmod/2021grades•titleŸ–GRADES-NDA '21: Proceedings of the 4th ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA), Virtual Event, China, 20 June 2021.§year§2021£doiøhttps://doi.org/10.1145/3461837©publisher±GRADES-NDA@SIGMODßauthorsí∞Vasiliki Kalavri∞Nikolay Yakovets®Abstract⁄πThe purpose of this workshop is to bring together researchers from academia, industry,
                     and government to create a forum for discussing recent advances in large-scale graph
                     data management and analytics systems, as well as propose and discuss novel methods
                     and techniques towards addressing domain specific challenges and handling noise in
                     real-world graphs.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track¨invited-talk®Citation ®Downloado©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/OzcanLQE21•titleŸ0Semantic enrichment of data for AI applications.§year§2021£doiŸ'https://doi.org/10.1145/3462462.3468881©publisher´DEEM@SIGMODßauthorsî¨Fatma √ñzcan©Chuan Lei¨Abdul Quamar±Vasilis Efthymiou®Abstract⁄ÒIn this work, we use semantic knowledge sources, such as cross-domain knowledge graphs (KGs) and domain-specific ontologies, to enrich structured data for various AI applications. By enriching our understanding of the underlying data with semantics brought in from external ontologies and KGs, we can better interpret the data as well as the queries to answer more questions, provide more complete answers, and deal with entity disambiguation. To semantically enrich the data with external knowledge sources, we need to find the correspondences between the structured data and the entities in the cross-domain KGs and/or the domain-specific ontologies. In this paper, we break this problem into several steps, and provide detailed solutions for each step. We showcase the practical value of semantic enrichment of data using our proposed techniques in entity disambiguation, natural language querying and conversational interfaces to data, query relaxation, as well as query answering, with promising results.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3462462.3468881®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloadn©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/LeeQKO21•titleŸ\Boomerang: Proactive Insight-Based Recommendations for Guiding Conversational Data Analysis.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452748©publisher±SIGMOD Conferenceßauthorsî≤Doris Jung Lin Lee¨Abdul Quamar≠Eser Kandogan¨Fatma √ñzcan®Abstract⁄Natural-language interfaces are gaining popularity due to their potential to democratize access to data and insights by making the interaction with data more natural and accessible for a wide range of business users. To fully embrace the goal of democratization, it is also necessary to provide effective and continuous guidance support for data exploration. Conversational interfaces enable exploration of the data and insights search space in small incremental steps as the conversation with the data progresses. In this demo, we describe Boomerang, a system that recommends data-driven insights to guide exploration of datasets through a conversational interface. Boomerang aggregates recommendations from a variety of statistical, collaborative, and content-based recommenders, and selects insights that match closely to the user's current state of data exploration, represented as the \em conversational context. Boomerang combines various metrics, such as \em relevance, \em interestingness and \em timeliness, to rank the insights and recommends the insights based on current conversational context. In the demo, we will show how Boomerang enables guided data exploration on a sales dataset, containing information about products, retailers, sales, orders, inventory levels and regions.©VideoSize®208.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452748&amp;file=3448016.3452748.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452748®Keywordsñ∂recommendation systems∂conversational systems®chatbots≤data visualization∑guided data exploration∞data exploration¶Badges¿•Track´short-paper®Citation ®Downloadm©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/HalawaR21•titleŸ3Position paper: bitemporal dynamic graph analytics.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464514©publisher±GRADES-NDA@SIGMODßauthorsí≠Hassan Halawa≠Matei Ripeanu®Abstract⁄ÔMost of today's graph analytics systems model static graphs and do not support business use cases that require the ability to: (i) query the dynamic graph data for a time-evolving system, (ii) carry out investigations on its historical evolution, and (iii) audit past business decisions made with potentially stale or incorrect data. This position paper presents our vision for bi-temporal dynamic graph analytics, and sketches a design for a system that efficiently supports these requirements.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464514®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloadm©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/SpothKLHL21•titleŸ,Reducing Ambiguity in Json Schema Discovery.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452801©publisher±SIGMOD Conferenceßauthorsï≠William SpothÆOliver KennedyßYing LuºBeda Christoph Hammerschmidt¨Zhen Hua Liu®Abstract⁄nAd-hoc data models like Json simplify schema evolution and enable multiplexing various data sources into a single stream. While useful when writing data, this flexibility makes Json harder to validate and query, forcing such tasks to rely on automated schema discovery techniques. Unfortunately, ambiguity in the schema design space forces existing schema discovery systems to make simplifying, data-independent assumptions about schema structure. When these assumptions are violated, most notably by APIs, the generated schemas are imprecise, creating numerous opportunities for false positives during validation. In this paper, we propose Jxplain, a Json schema discovery algorithm with heuristics that mitigate common forms of ambiguity. Although Jxplain is slightly slower than state of the art schema extractors, we show that it produces significantly more precise schemas.©VideoSizeß42.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452801&amp;file=3448016.3452801.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452801®Keywordsî∞entity detectionØsemi-structured§json´json-schema¶Badges¿•Track∞research-article®Citation ®Downloadl©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/QuWC21•titleŸ^Joint blockchain and federated learning-based offloading in harsh edge computing environments.§year§2021£doiŸ'https://doi.org/10.1145/3460866.3461765©publisher≠BiDEDE@SIGMODßauthorsì™Guanjin Qu™Huaming Wu¨Naichuan Cui®Abstract⁄ëWith the popularity of edge computing, numerous Internet of Things (IoT) applications have been developed and applied to various fields. However, for the harsh environment with network fluctuations and potential attacks, traditional task offloading decision-making schemes cannot meet the requirements of real-time and security. For this reason, we propose a novel task offloading decision framework to cope with the special requirements of the environment. This framework uses a task offloading decision model based on deep reinforcement learning algorithms, and is located on the user side to reduce the impact of network fluctuations. To improve the efficiency and security of the model in harsh edge computing environments, we adopt federated learning and introduce the blockchain into the process of parameter upload and decentralization of federated learning. In addition, we design a new blockchain consensus algorithm to reduce the waste of computing resources and improve the embedding and propagation speeds of the blockchain. Furthermore, we demonstrate the effect of task offloading of this model by performing offloading decisions on a simulation platform.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3460866.3461765®Keywordsî™blockchain≤harsh environmentsÆedge computing≤federated learning¶Badges¿•Track∞research-article®Citation ®Downloadj©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/LiCCHC21•titleŸMAuto-FuzzyJoin: Auto-Program Fuzzy Similarity Joins Without Labeled Examples.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452824©publisher±SIGMOD ConferenceßauthorsïßPeng Li´Xiang Cheng¶Xu ChußYeye He±Surajit Chaudhuri®Abstract⁄2Fuzzy similarity join is an important database operator widely used in practice. So far the research community has focused exclusively on optimizing fuzzy joinscalability. However, practitioners today also struggle to optimize fuzzy-joinquality, because they face a daunting space of parameters (e.g., distance-functions, distance-thresholds, tokenization-options, etc.), and often have to resort to a manual trial-and-error approach to program these parameters in order to optimize fuzzy-join quality. This key challenge of automatically generating high-quality fuzzy-join programs has received surprisingly little attention thus far. In this work, we study the problem of "auto-program'' fuzzy-joins. Leveraging a geometric interpretation of distance-functions, we develop an unsupervised Auto-FuzzyJoin framework that can infer suitable fuzzy-join programs on given input tables, without requiring explicit human input such as labelled training data. Using Auto-FuzzyJoin, users only need to provide two input tables L and R, and a desired precision target œÑ (say 0.9). Auto-FuzzyJoin leverages the fact that one of the input is a reference table to automatically program fuzzy-joins that meet the precision target œÑ in expectation, while maximizing fuzzy-join recall (defined as the number of correctly joined records). Experiments on both existing benchmarks and a new benchmark with 50 fuzzy-join tasks created from Wikipedia data suggest that the proposed Auto-FuzzyJoin significantly outperforms existing unsupervised approaches, and is surprisingly competitive even against supervised approaches (e.g., Magellan and DeepMatcher) when 50% of ground-truth labels are used as training data. We have released our code and benchmark on GitHub\footnote\urlhttps://github.com/chu-data-lab/AutomaticFuzzyJoin to facilitate future research.©VideoSize®100.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452824&amp;file=3448016.3452824.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452824®Keywordsî±entity resolutionØsimilarity join™fuzzy joinµunsupervised learning¶Badges¿•Track∞research-article®Citation ®Downloadj©PaperRefsêã§Infoá§type´proceedings£key∂conf/sigmod/2021bidede•titleŸŸBiDEDE '21: Proceedings of the International Workshop on Big Data in Emergent Distributed Environments, In conjunction with the 2021 ACM SIGMOD/PODS Conference, BiDEDE@SIGMOD 2021, Virtual Event, China, 20 June, 2021.§year§2021£doiøhttps://doi.org/10.1145/3460866©publisher≠BiDEDE@SIGMODßauthorsì´Sven Groppe¨Le GruenwaldØChing-Hsien Hsu®Abstract⁄The goal of this workshop is to bring together academic researchers and industry practitioners
                     to address the challenges and report and exchange the research findings in Big Data
                     in emergent distributed environments, including new approaches, techniques and applications,
                     make substantial theoretical and empirical contributions to, and significantly advance
                     the state of the art of Big Data in emergent distributed environments.©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloadj©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Kumar21•titleŸEAutomation of Data Prep, ML, and Data Science: New Cure or Snake Oil?§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457537©publisher±SIGMOD ConferenceßauthorsëØArun Kumar 0001®Abstract⁄4As machine learning (ML), artificial intelligence (AI), and Data Science grow in practical importance, a large part of the ML/AI software industry claims to have built tools and platforms to automate the entire workflow of ML. That includes vexing problems of data preparation (prep), studied intensively by the database (DB) community for decades, with basically no resolution so far. Such claims by the ML/AI industry face a stunning lack of scientific scrutiny from the DB and ML research worlds, largely due to the lack of meaningful, large, and objective benchmarks. As such tools rapidly gain adoption among enterprises and other customers, this panel will debate whether the new ML/AI industry is basically selling "snake oil" to such users, how to evolve away from the status quo by instituting meaningful new benchmarks, creating new partnerships between industry and academia for this, and other pressing questions in this important arena. We aim to spur vigorous conversations that will hopefully lead to genuine new cures for an age-old affliction in Data Science.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457537®Keywordsï∞data preparation∞machine learning≠data cleaning¶automl≤benchmark datasets¶Badges¿•Track•panel®Citation ®Downloadj©PaperRefsêã§Infoá§type≠inproceedings£keyºconf/sigmod/ShafieinejadKI21•titleŸKPCOR: Private Contextual Outlier Release via Differentially Private Search.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452812©publisher±SIGMOD ConferenceßauthorsìµMasoumeh Shafieinejad≤Florian Kerschbaum≠Ihab F. Ilyas®Abstract⁄ØOutlier detection plays a significant role in various real world applications such as intrusion, malfunction, and fraud detection. Traditionally, outlier detection techniques are applied to find outliers in the context of the whole dataset. However, this practice neglects the data points, namely contextual outliers, that are not outliers in the whole dataset but in some specific neighborhoods. Contextual outliers are particularly important in data exploration and targeted anomaly explanation and diagnosis. In these scenarios, the data owner computes the following information: i) The attributes that contribute to the abnormality of an outlier (metric), ii) Contextual description of the outlier's neighborhoods (context), and iii) The utility score of the context, e.g. its strength in showing the outlier's significance, or in relation to a particular explanation for the outlier. However, revealing the outlier's context leaks information about the other individuals in the population as well, violating their privacy. We address the issue of population privacy violations in this paper. There are two main challenges in defining and applying privacy in contextual outlier release. In this setting, the data owner is required to release a valid context for the queried record, i.e. a context in which the record is an outlier. Hence, the first major challenge is that the privacy technique must preserve the validity of the context for each record. We propose techniques to protect the privacy of individuals through a relaxed notion of differential privacy to satisfy this requirement. The second major challenge is applying the proposed techniques efficiently, as they impose intensive computation to the base algorithm. To overcome this challenge, we propose a graph structure to map the contexts to, and introduce differentially private graph search algorithms as efficient solutions for the computation problem caused by differential privacy techniques.©VideoSizeß41.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452812&amp;file=3448016.3452812.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452812®Keywordsî¨graph searchºcontextual outlier detection∞private sampling¥differential privacy¶Badges¿•Track∞research-article®Citation ®Downloadi©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/SahuS21•titleŸOGraphsurge: Graph Analytics on View Collections Using Differential Computation.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452837©publisher±SIGMOD ConferenceßauthorsíØSiddhartha SahuØSemih Salihoglu®Abstract⁄`This paper presents the design and implementation of a new open-source view-based graph analytics system called Graphsurge. Graphsurge is designed to support applications that analyze multiple snapshots or views of a large-scale graph. Users program Graphsurge through a declarative graph view definition language (GVDL) to create views over input graphs and a Differential Dataflow-based programming API to write analytics computations. A key feature of GVDL is the ability to organize views into view collections, which allows Graphsurge to automatically share computation across views, without users writing any incrementalization code, by performing computations differentially. We then introduce two optimization problems that naturally arise in our setting. First is the collection ordering problem to determine the order of views that leads to minimum differences across consecutive views. We prove this problem is NP-hard and show a constant-factor approximation algorithm drawn from literature. Second is the collection splitting problem to decide on which views to run computations differentially vs from scratch, for which we present an adaptive solution that makes decisions at runtime. We present extensive experiments to demonstrate the benefits of running computations differentially for view collections and our collection ordering and splitting optimizations.©VideoSizeß31.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452837&amp;file=3448016.3452837.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452837®Keywordsñ´graph views¥dataflow computationØview collection≤adaptive execution≥collection ordering∏differential computation¶Badges¿•Track∞research-article®Citation ®Downloade©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/PoppeLMRR21•titleŸSTo Share, or not to Share Online Event Trend Aggregation Over Bursty Event Streams.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452785©publisher±SIGMOD Conferenceßauthorsï™Olga Poppe©Chuan Lei¶Lei Ma≠Allison RozetµElke A. Rundensteiner®Abstract⁄yComplex event processing (CEP) systems continuously evaluate large workloads of pattern queries under tight time constraints. Event trend aggregation queries with Kleene patterns are commonly used to retrieve summarized insights about the recent trends in event streams. State-of-art methods are limited either due to repetitive computations or unnecessary trend construction. Existing shared approaches are guided by statically selected and hence rigid sharing plans that are often sub-optimal under stream fluctuations. In this work, we propose a novel framework Hamlet that is the first to overcome these limitations. Hamlet introduces two key innovations. First, Hamlet adaptively decides at run time whether to share or not to share computations depending on the current stream properties to harvest the maximum sharing benefit. Second, Hamlet is equipped with a highly efficient shared trend aggregation strategy that avoids trend construction. Our experimental study on both real and synthetic data sets demonstrates that Hamlet consistently reduces query latency by up to five orders of magnitude compared to state-of-the-art approaches.©VideoSizeß42.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452785&amp;file=3448016.3452785.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452785®Keywordsï´event trend∏complex event processing≥computation sharing≤query optimization∑incremental aggregation¶Badges¿•Track∞research-article®Citation ®Downloade©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/WieseH21•titleŸáNNCompare: a framework for dataset selection, data augmentation and comparison of different neural networks for medical image analysis.§year§2021£doiŸ'https://doi.org/10.1145/3462462.3468884©publisher´DEEM@SIGMODßauthorsí™Lena WieseØDeborah H√∂ltje®Abstract⁄&In our institute we capture a variety of medical image data - in particular, microscopy data for the use case of bronchoconstriction. In order to alleviate the manual intervention for the image analysis, we developed a comprehensive data analysis pipeline that automates the image preprocessing, data augmentation, as well as neural network training and deployment with a web-based user interface. We evaluate the pipeline on a real-world medical image dataset and comparatively analyze the performance of four different neural network architectures.©VideoSizeß22.3 MB©VideoLinkŸ`https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3462462.3468884&amp;file=a6-wiese.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3462462.3468884®KeywordsìÆmedical imagesØneural networksÆimage analysis¶Badges¿•Track∞research-article®Citation ®Downloade©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/HuangCBCZ21•titleŸgMIDAS: Towards Efficient and Effective Maintenance of Canned Patterns in Visual Graph Query Interfaces.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457251©publisher±SIGMOD Conferenceßauthorsï©Kai Huang≠Huey-Eng Chua≤Sourav S. Bhowmick™Byron Choi≠Shuigeng Zhou®Abstract⁄bSeveral visual graph query interfaces (a.k.a gui) expose a set of canned patterns (i.e., small subgraph patterns) to expedite subgraph query formulation by enabling pattern-at-a-time construction. Unfortunately, manual generation of canned patterns is not only labour intensive but also may lack diversity to support efficient visual formulation of a wide range of subgraph queries. Recent efforts have taken a data-driven approach to select high-quality canned patterns for a gui automatically from the underlying graph database. However, as the underlying database evolves, these selected patterns may become stale and adversely impact efficient query formulation. In this paper, we present a novel framework called Midas for efficient and effective maintenance of the canned patterns as the database evolves. Specifically, it adopts a selective maintenance strategy that guarantees progressive gain of coverage of the patterns without sacrificing their diversity and cognitive load. Experimental study with real-world datasets and visual graph interfaces demonstrates the effectiveness of Midas compared to static guis.©VideoSize®223.1 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457251&amp;file=3448016.3457251.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457251®KeywordsòÆcognitive load∞database updatesΩvisual graph query interfacesØcanned patterns®coverage±query formulation≥pattern maintenance©diversity¶Badges¿•Track∞research-article®Citation ®Downloadd©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/KimC21•titleŸTPool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457326©publisher±SIGMOD Conferenceßauthorsí™Hakbin Kim≠Dong-Wan Choi®Abstract⁄In spite of the great success of deep learning technologies, training and delivery of a practically serviceable model is still a highly time-consuming process. Furthermore, a resulting model is usually too generic and heavyweight, and hence essentially goes through another expensive model compression phase to fit in a resource-limited device like embedded systems. Inspired by the fact that a machine learning task specifically requested by mobile users is often much simpler than it is supported by a massive generic model, this paper proposes a framework, called Pool of Experts (PoE), that instantly builds a lightweight and task-specific model without any training process. For a realtime model querying service, PoE first extracts a pool of primitive components, called experts, from a well-trained and sufficiently generic network by exploiting a novel conditional knowledge distillation method, and then performs our train-free knowledge consolidation to quickly combine necessary experts into a lightweight network for a target task. Thanks to this train-free property, in our thorough empirical study, PoE can build a fairly accurate yet compact model in a realtime manner, whereas it takes a few minutes per query for the other training methods to achieve a similar level of the accuracy.©VideoSizeß28.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457326&amp;file=3448016.3457326.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457326®Keywordsî±model compression∂knowledge distillation±model unification¥model specialization¶Badges¿•Track∞research-article®Citation ®Downloadd©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/HirnG21•titleŸ'One WITH RECURSIVE is Worth Many GOTOs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457272©publisher±SIGMOD Conferenceßauthorsí™Denis Hirn≠Torsten Grust®Abstract⁄ÀPL/SQL integrates an imperative statement-by-statement style of programming with the plan-based evaluation of SQL queries. The disparity of both leads to friction at runtime, slowing PL/SQL execution down significantly. This work describes a compiler from PL/SQL UDFs to plain SQL queries. Post-compilation, evaluation entirely happens on the SQL side of the fence. With the friction gone, we observe execution times to improve by about a factor of 2, even for complex UDFs. The compiler builds on techniques long established by the programming language community. In particular, it uses trampolined style to compile arbitrarily nested iterative control flow in PL/SQL into SQL's recursive common table expressions.©VideoSize®449.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457272&amp;file=3448016.3457272.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457272®Keywordsï¶PL/SQL´compilation©recursion±trampolined style£SQL¶Badges¿•Track∞research-article®Citation ®Downloadc©PaperRefsêã§Infoá§type≠inproceedings£key∏conf/sigmod/SchuleSBK021•titleŸ.TardisDB: Extending SQL to Support Versioning.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452767©publisher±SIGMOD ConferenceßauthorsïµMaximilian E. Sch√ºle∞Josef Schmei√üer´Thomas Blum≠Alfons Kemper≥Thomas Neumann 0001®Abstract⁄Online encyclopaedias such as Wikipedia implement their own version control above database systems to manage multiple revisions of the same page. In contrast to temporal databases that restrict each tuple's validity to a time range, a version affects multiple tuples. To overcome the need for a separate version layer, we have created TardisDB, the first database system with incorporated data versioning across multiple relations. This paper presents the interface for TardisDB with an extended SQL to manage and query data from different branches. We first give an overview of TardisDB's architecture that includes an extended table scan operator: a branch bitmap indicates a tuple's affiliation to a branch and a chain of tuples tracks the different versions. This is the first database system that combines chains for multiversion concurrency control with a bitmap for each branch to enable versioning. Afterwards, we describe our proposed SQL extension to create, query and modify tables across different, named branches. In our demonstration setup, we allow users to interactively create and edit branches and display the lineage of each branch.©VideoSizeß16.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452767&amp;file=3448016.3452767.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452767®KeywordsíØversion control£sql¶Badges¿•Track´short-paper®Citation ®Downloadb©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/ZhangSCA21•titleŸPFairRover: explorative model building for fair and responsible machine learning.§year§2021£doiŸ'https://doi.org/10.1145/3462462.3468882©publisher´DEEM@SIGMODßauthorsî≠Hantian Zhang≠Nima Shahbazi¶Xu ChuØAbolfazl Asudeh®Abstract⁄çThe potential harms and drawbacks of automated decision making has become a challenge as data science blends into our lives. In particular, fairness issues with deployed machine learning models have drawn significant attention from the research community. Despite the myriad of algorithmic fairness work in various research communities, in practice data scientists still face many roadblocks in ensuring the fairness of their machine learning models. This is primarily because there does not exist an end-to-end system that guides the users in building a fair machine learning model in a responsible way from model auditing, to model explanation, to bias mitigation. We propose a explorative model building system FairRover for responsible fair model building. FairRover guides users in (1) discovering the potential biases in the model; (2) providing explanation to the discovered biases so as to help users in understanding potential causes of the biases; and (3) mitigating the most important biases selected by the users. Because of the impossibility theorem of fairness, and the well-known trade-off between fairness and accuracy, it is generally impossible to achieve a completely fair and accurate machine learning model. Therefore, this responsible model building process is naturally performed iteratively until a satisfying trade-off is reached. Human users are involved in the loop to make various decisions guided by FairRover. We demonstrate a case study on the Adult Census dataset, which shows how FairRover guides users in iteratively building a fair income prediction model in a responsible way. We discuss the current limitations of FairRover and future work.©VideoSize•78 MB©VideoLinkŸ`https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3462462.3468882&amp;file=a5-zhang.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3462462.3468882®Keywordsê¶Badges¿•Track∞research-article®Citation ®Downloada©PaperRefsêã§Infoá§type≠inproceedings£keyªconf/sigmod/PersonnazABFS21•titleŸYBalancing Familiarity and Curiosity in Data Exploration with Deep Reinforcement Learning.§year§2021£doiŸ'https://doi.org/10.1145/3464509.3464884©publisher´aiDM@SIGMODßauthorsï≥Aur√©lien Personnaz∞Sihem Amer-Yahia¥Laure Berti-√âquille¥Maximilian Fabricius¥Srividya Subramanian®Abstract⁄eThe ability to find a set of records in Exploratory Data Analysis (EDA) hinges on the scattering of objects in the data set and the on users‚Äô knowledge of data and their ability to express their needs. This yields a wide range of EDA scenarios and solutions that differ in the guidance they provide to users. In this paper, we investigate the interplay between modeling curiosity and familiarity in Deep Reinforcement Learning (DRL) and expressive data exploration operators. We formalize curiosity as intrinsic reward and familiarity as extrinsic reward. We examine the behavior of several policies learned for different weights for those rewards. Our experiments on SDSS, a very large sky survey data set1 provide several insights and justify the need for a deeper examination of combining DRL and data exploration operators that go beyond drill-downs and roll-ups.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3464509.3464884®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download_©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/KhuranaH21•titleŸ-Shedding Light on Opaque Application Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457252©publisher±SIGMOD Conferenceßauthorsí≠Kapil Khurana±Jayant R. Haritsa®Abstract⁄xWe investigate a new query reverse-engineering problem of unmasking SQL queries hidden within database applications. The diverse use-cases for this problem range from resurrecting legacy code to query rewriting. As a first step in addressing the unmasking challenge, we present UNMASQUE, an active-learning extraction algorithm that can expose a basal class of hidden warehouse queries. A special feature of our design is that the extraction is non-invasive wrt the application, examining only the results obtained from repeated executions on databases derived with a combination of data mutation and data generation techniques. Further, potent optimizations are incorporated to minimize the extraction overheads. A detailed evaluation over applications hosting hidden SQL queries, or their imperative versions, demonstrates that UNMASQUE correctly and efficiently extracts these queries.©VideoSizeß73.6 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457252&amp;file=3448016.3457252.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457252®Keywordsïπquery reverse engineering£sqlŸ&imperative-declarative code conversionØactive learning¥black-box extraction¶Badges¿•Track∞research-article®Citation ®Download^©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/GlenisK21•titleŸIPyExplore: Query Recommendations for Data Exploration without Query Logs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452762©publisher±SIGMOD Conferenceßauthorsí∞Apostolos Glenis∞Georgia Koutrika®Abstract⁄áHelping users explore data becomes increasingly more important as databases get larger and more complex. In this demo, we present PyExplore, a data exploration tool aimed at helping end users formulate queries over new datasets. PyExplore takes as input an initial query from the user along with some parameters and provides interesting queries by leveraging data correlations and diversity.©VideoSizeß53.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452762&amp;file=3448016.3452762.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452762®Keywordsì∞data exploration™clusteringµquery recommendations¶Badges¿•Track´short-paper®Citation ®Download^©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/GlasbergenWD21•titleŸ.Dendrite: Bolt-on Adaptivity for Data Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452755©publisher±SIGMOD ConferenceßauthorsìØBrad Glasbergen©Fangyu Wu∞Khuzaima Daudjee®Abstract⁄˚Client application workloads for data systems are known to vary in load and access patterns over time. This variability can place undue stress on data systems, tying up resources and degrading performance. To meet this challenge, systems must adapt by adjusting resource allocation and processing techniques to ameliorate contention and to deliver stable performance. We demonstrate Dendrite, a system designed to bootstrap adaptivity for data systems through its widely-applicable approach for extracting metrics, developing adaption rules, and applying them through user-defined functions to effect system behaviour changes. We highlight Dendrite's features and capabilities through a proof-of-concept implementation with the popular PostgreSQL database system.©VideoSize®175.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452755&amp;file=3448016.3452755.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452755®Keywordsî∞system behaviour≠debug logging¨self-driving™adaptivity¶Badges¿•Track´short-paper®Citation ®Download]©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/GaoXAY21•titleŸ4Efficiently Answering Durability Prediction Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457305©publisher±SIGMOD Conferenceßauthorsî´Junyang Gao®Yifan Xu±Pankaj K. Agarwal≠Jun Yang 0001®Abstract⁄*We consider a class of queries called durability prediction queries that arise commonly in predictive analytics, where we use a given predictive model to answer questions about possible futures to inform our decisions. Examples of durability prediction queries include "what is the probability that this financial product will keep losing money over the next 12 quarters before turning in any profit?" and "what is the chance for our proposed server cluster to fail the required service-level agreement before its term ends?" We devise a general method called Multi-Level Splitting Sampling (MLSS) that can efficiently handle complex queries and complex models---including those involving black-box functions---as long as the models allow us to simulate possible futures step by step. Our method addresses the inefficiency of standard Monte Carlo (MC) methods by applying the idea of importance splitting to let one "promising" sample path prefix generate multiple "offspring" paths, thereby directing simulation efforts toward more promising paths. We propose practical techniques for designing splitting strategies, freeing users from manual tuning. Experiments show that our approach is able to achieve unbiased estimates and the same error guarantees as standard MC while offering an order-of-magnitude cost reduction.©VideoSizeß42.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457305&amp;file=3448016.3457305.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457305®Keywordsï®sampling∂probabilistic database¥predictive analyticsµrare-event simulation≠temporal data¶Badges¿•Track∞research-article®Citation ®Download\©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/NoceraCTKKS21•titleŸSCrosstown Foundry: A Scalable Data-driven Journalism Platform for Hyper-local News.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452751©publisher±SIGMOD ConferenceßauthorsñÆLuciano Nocera≥George Constantinou¨Luan V. Tran´Seon Ho Kim¨Gabriel Kahn≠Cyrus Shahabi®Abstract⁄«Generating hyper-local news at scale is challenging because publicly available data is not provided at the desired spatial and temporal granularity. Besides, there is a lack of automated analytical and publishing tools. Crosstown Foundry, which is being actively developed and used by engineers and journalists, is a novel data-driven system that leverages a massive multi-modal dataset to generate personalized newsletters for Los Angeles County readers.©VideoSizeß84.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452751&amp;file=3448016.3452751.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452751®Keywordsî∂data-driven journalism´data mining≠visualization∞hyper-local news¶Badges¿•Track´short-paper®Citation ®Download[©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/ChowDS021•titleŸ.SRA: Smart Recovery Advisor for Cyber Attacks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452766©publisher±SIGMOD Conferenceßauthorsî™Ka Ho ChowØUmesh Deshpande≤Sangeetha Seshadri≠Ling Liu 0001®Abstract⁄UContinuous Data Protection (CDP) is becoming instrumental in recovering applications from crypto-ransomware attacks. It enables fine-grained recovery through journaling, allowing the applications (its volumes) to recover to any previous state. While zero data loss can be achieved during recovery with CDP, the timestamp of the desired restore point, i.e., the one just prior to the attack, needs to be provided to reconstruct the volume. Such information is often unavailable in practice, and system administrators can only adopt a trial-and-error strategy to narrow down the time range of desired restore points by making multiple time-consuming recovery attempts. The recovery systems offer little guidance in pointing to the restore points containing a valid application state and reducing data loss. To address this problem, we equip the CDP-based recovery with machine intelligence. This demonstration showcases Smart Recovery Advisor (SRA), which offers interpretable, data-driven, and feedback-aware restore point recommendations that reduce the number of recovery attempts while minimizing data loss.©VideoSize®218.2 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452766&amp;file=3448016.3452766.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452766®Keywordsî™ransomware∫continuous data protection∞storage recovery∞machine learning¶Badges¿•Track´short-paper®Citation ®DownloadX©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Pocol21•titleŸ9Index-Based Join Size Estimation Using Adaptive Sampling.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450572©publisher±SIGMOD Conferenceßauthorsë¨Sergiu Pocol®Abstract⁄
Cost-based query optimizers rely on cardinality estimates of intermediate results to avoid suboptimal query execution plans. However, when confronted with ad-hoc queries on big data, said optimizers can produce large estimation errors, resulting in drastic decreases in overall performance. Such errors occur because many estimation algorithms for joins make use of strong independence and uniformity assumptions. Moreover, equi-joins on skewed data with filter predicates tend to cause the aforementioned assumptions to fail [2]. Since the cardinality estimate of a result with many joins depends on estimates of the underlying joins, it has been shown that improving the accuracy of join size estimates in a "bottom-up" order can significantly improve performance [2]. Thus, our research aims at improving the estimation of two-table join sizes. Certain join size estimation approaches that use offline samples fare poorly with filtering and can suffer from insufficient sample size [2]. Alternatively, query optimizers may make use of persisted histograms. However, the associated storage space is a large deterrent, as is the case with persisting offline samples [4]. In turn, algorithms have been presented wherein adaptive, block-level sampling is conducted during query optimization [5]. To the best of our knowledge, there is no algorithm for two-table join size estimation that 1) samples filtered base tables, 2) makes use exclusively of persisted counts in B+-tree indexes and 3) attempts to provide statistical confidence on estimates. In this paper, we present and evaluate a novel join size estimation algorithm prototyped on SAP IQ. Our algorithm can be easily incorporated into query optimizers that utilize bottom-up enumeration and evaluate filter predicates prior to optimization. In contrast with existing machine learning based approaches [1], the algorithm is simpler to implement or derive from existing support for sampling. Join size estimates are produced using a combination of variance-based calculations from Oracle 12c [5] along with the usage of persisted counts in indexes from index-based sampling [2]. In doing so, the amount of sampling conducted is minimized until either a parameterized budget is surpassed or there is sufficient statistical confidence in an estimate. On a subset of industry-standard benchmark queries involving joins on skewed data, our method improved the overall execution time by 16%. Amongst queries with execution plans altered by our estimates, the mean percentage improvement in individual execution time was 34%.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450572®Keywordsñ±adaptive sampling∂cardinality estimation≠join ordering´cardinality∞database indexes≤query optimization¶Badges¿•Track®abstract®Citation ®DownloadU©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/Alipourlangouri21•titleŸ!Temporal Dependencies for Graphs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450586©publisher±SIGMOD Conferenceßauthorsë∑Morteza Alipourlangouri®Abstract⁄hGraphs are increasingly being used to model information about entities, their properties, and relationships between entities. Examples include relationships between customers, their product purchases, and inter-relationships between products. Many of these graphs are not static, and operate in dynamic data environments. Large-scale knowledge bases such as DBpedia, Yago, Wikidata, and Amazon product graphs operate in such dynamic environments, where Entities in these temporal graphs are constantly changing. Having accurate and complete information is critical for downstream decision making and fact checking. Data dependencies help us to preserve accuracy of the data and have been well studied for relational data and static graphs. However, the need for FDs is also evident for temporal graphs since they specify the semantic of the data to detect inconsistencies.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450586®KeywordsìØtemporal graphs≠data cleaning•TGFDs¶Badges¿•Track®abstract®Citation ®DownloadT©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Kosyfaki21•titleŸ1Flow Provenance in Temporal Interaction Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450581©publisher±SIGMOD Conferenceßauthorsë≥Chrysanthi Kosyfaki®Abstract⁄§In temporal interaction networks, such as financial transaction networks, vertices model entities which exchange quantities (e.g., money) over time. We study the problem of identifying the origin of the quantities that flow into the vertices of the network over time. We consider various models of flow relay, which are related to different application scenarios and develop corresponding techniques for flow provenance.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450581®Keywordsì®quantity™provenance¥interaction networks¶Badges¿•Track®abstract®Citation ®DownloadS©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/SongMLW21•titleŸHGRIP: Constraint-based Explanation of Missing Answers for Graph Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452758©publisher±SIGMOD ConferenceßauthorsîßQi Song™Hanchao Ma®Peng Lin™Yinghui Wu®Abstract⁄⁄A useful feature in graph query engines is to clarify "Why certain entities (nodes, attribute values or edges) are missing" in query answers. This task is even more challenging when the relevant data is already missing in the underlying data source. Missing data, on the other hand, can be inferred by enforcing data constraints for graphs. We demonstrate GRIP, a system that exploits data constraints to clarify missing answers for graph queries. (1) Constraint-based ex- planation. Given a desired yet missing entity in the query answer, GRIP ensures to generate finite and minimal sequences of data constraints (an "explanation") that should be consecutively enforced to to ensure its occurrence for the same query. (2) Answering ?why" and "how" questions. Users can query GRIP with both "Why" ("Why" the element is missing) and "How" questions ("How" to refine the graph to include the missing answer). GRIP engine supports run- time generation of explanations by incrementally maintaining a set of bi-directional search trees. (3) Interactive exploration. GRIP provides user-friendly GUI to support interactive ad visual exploration of explanations, including both automated generation and step-by-step inspection of graph manipulations.©VideoSize®118.3 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452758&amp;file=3448016.3452758.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452758®KeywordsìØdata provenance∞data constraints¶graphs¶Badges¿•Track´short-paper®Citation ®DownloadR©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Edian21•titleŸ:Accelerating Product Quantization Query Execution Runtime.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450574©publisher±SIGMOD ConferenceßauthorsëÆIkraduya Edian®Abstract⁄âProduct Quantization is a method for approximate nearest neighbor search. I review and analyze two recent SIMD-based query acceleration techniques and propose an algorithmic solution agnostic to the underlying hardware. This new method is competitive in terms of runtime to the SIMD-based methods and alleviates their drawbacks: reduced accuracy and limited encoding bit budget configurations.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450574®Keywordsî≤k-nearest neighborºapproximate nearest neighbor§simd¥product quantization¶Badges¿•Track®abstract®Citation ®DownloadQ©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/WeiTA21•titleŸmDemonstrating Robust Voice Querying with MUVE: Optimally Visualizing Results of Phonetically Similar Queries.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452753©publisher±SIGMOD Conferenceßauthorsì©Ziyun Wei∞Immanuel TrummerØConnor Anderson®Abstract⁄ùRecently proposed voice query interfaces translate voice input into SQL queries. Unreliable speech recognition on top of the intrinsic challenges of text-to-SQL translation makes it hard to reliably interpret user input. We present MUVE (Multiplots for Voice quEries), a system for robust voice querying. MUVE reduces the impact of ambiguous voice queries by filling the screen with multiplots, capturing results of phonetically similar queries. It maps voice input to a probability distribution over query candidates, executes a selected subset of queries, and visualizes their results in a multiplot. Our goal is to maximize probability to show the correct query result. Also, we want to optimize the visualization (e.g., by coloring a subset of likely results) in order to minimize expected time until users find the correct result. Via a user study, we validate a simple cost model estimating the latter overhead. The resulting optimization problem is NP-hard. We propose an exhaustive algorithm, based on integer programming, as well as a greedy heuristic. As shown in a corresponding user study, MUVE enables users to identify accurate results faster, compared to prior work.©VideoSizeß45.9 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452753&amp;file=3448016.3452753.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452753®KeywordsìØdata processing∫voice query disambiguation∂visualization planning¶Badges¿•Track´short-paper®Citation ®DownloadQ©PaperRefsêã§Infoá§type≠inproceedings£keyΩconf/sigmod/Lev-AriTSDFABDG21•titleŸ$QuiCK: A Queuing System in CloudKit.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457567©publisher±SIGMOD Conferenceßauthorsö¨Kfir Lev-Ari™Yizuo Tian∞Alexander Shraer≠Chris Douglas¶Hao FuÆAndrey Andreev≠Kevin Beranek´Scott Dugas¨Alec Grieser¨Jeremy Hemmo®Abstract⁄UWe present QuiCK, a queuing system built for managing asynchronous tasks in CloudKit, Apple's storage backend service. QuiCK stores queued messages along with user data in CloudKit, and supports CloudKit's tenancy model including isolation, fair resource allocation, observability, and tenant migration. QuiCK is built on the FoundationDB Record Layer, an open source transactional DBMS. It employs massive two-level sharding, with tens of billions of queues on the first level (separately storing the queued items for each user of every CloudKit app), and hundreds of queues on a second level (one per FoundationDB cluster used by CloudKit). Our evaluation demonstrates that QuiCK scales linearly with additional consumer resources, effectively avoids contention, provides fairness across CloudKit tenants, and executes deferred tasks with low latency.©VideoSize®432.8 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457567&amp;file=3448016.3457567.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457567®Keywordsî≥large-scale systemsªdistributed queuing systems∞mobile back-endsªmulti-tenant cloud services¶Badges¿•Track∞research-article®Citation ®DownloadP©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/GeppertBR21•titleŸULarge-scale influence maximization with the influence maximization benchmarker suite.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464510©publisher±GRADES-NDA@SIGMODßauthorsì≠Heiko GeppertØSukanya BhowmikÆKurt Rothermel®Abstract⁄,Maximizing the influence of a fixed size seed set in a graph was investigated intensively in the past decade. Two very relevant questions are 1) how to solve the influence maximization problem on very large graphs within short time and 2) how to compare possible findings with the current state-of-the-art in a fair manner. To solve the first problem, proxy-based influence maximization strategies emerged. However, today's graphs became too large to be solved quickly for many well-established proxy strategies, since they do not scale to such large graphs. In this paper we propose 1) a novel update scheme for iterative influence maximization strategies named Update Approximation (UA) capable of large influence spreads within a few seconds on billion-scale graphs. Further, we present 2) a generic benchmark suite (Influence Maximization Benchmarker --- IMB) to implement and evaluate influence maximization strategies, alongside with implementations for several established strategies. IMB allows for easy to use benchmarks for further research by the community.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464510®Keywordsì∞graph algorithms∂influence maximization≥independent cascade¶Badges¿•Track∞research-article®Citation ®DownloadM©PaperRefsêã§Infoá§type≠inproceedings£key∫conf/sigmod/MakryniotiLV21•titleŸ5Machine learning in SQL by translation to TensorFlow.§year§2021£doiŸ'https://doi.org/10.1145/3462462.3468879©publisher´DEEM@SIGMODßauthorsì±Nantia Makrynioti¨Ruy Ley-Wild∞Vasilis Vassalos®Abstract⁄ÎWe present sql4ml, a framework for expressing machine learning (ML) algorithms in a relational database management system (RDBMS). The user writes the objective function of an ML model as a SQL query, then sql4ml translates the query into an equivalent TensorFlow (TF) graph, which can be automatically differentiated and optimized to learn the model weights. Sql4ml makes the database a unified programming environment for feature engineering, learning/inference, and evaluating models. The proposed approach is more expressive than using ready-made ML algorithms, but abstracts away the details of the training process. We present the architecture of sql4ml and describe the method for translating an objective function in SQL to a TensorFlow representation. We show how recent ideas from Factorized ML [7] can be leveraged to efficiently move data between a database and an ML framework. Finally, we present experimental results regarding both the proposed translation and the optimization techniques for data transfer. Our results show that translation time is negligible compared to time for data processing, and that the optimization techniques achieve up to 50% improvement in the export runtime and up to 85% decrease in the size of the exported data.©VideoSizeß24.7 MB©VideoLinkŸehttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3462462.3468879&amp;file=a2-makrynioti.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3462462.3468879®Keywordsî™TensorFlowŸ"mathematical optimization problems•RDBMS£SQL¶Badges¿•Track∞research-article®Citation ®DownloadK©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/PicadoTFPID21•titleŸEScalable and Usable Relational Learning With Automatic Language Bias.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457275©publisher±SIGMOD Conferenceßauthorsñ´Jose PicadoØArash Termehchy©Alan Fern∞Sudhanshu PathakÆPraveen Ilango™John Davis®Abstract⁄3A large body of machine learning and AI is focused on learning models composed of (probabilistic) logical rules, i.e., relational models, over relational databases and knowledge bases. To learn effective relational models over the huge space of possible ones efficiently, users of the current learning systems must restrict the structure of the candidate models using language bias. ML experts have to spend a long time inspecting the data and performing many rounds of trial and error to develop an effective language bias. We propose AutoBias, a system that leverages information in the underlying data to generate the language bias. As its induced language bias may not restrict the set of candidate models as tightly as the manually-written ones, learning may not scale to large datasets. Thus, we design novel and efficient methods to sample and learn effective relational models over large data. Our extensive empirical study shows that AutoBias delivers the same accuracy as using manually-written language bias by imposing only a slight overhead on the learning time.©VideoSize®166.4 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457275&amp;file=3448016.3457275.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457275®Keywordsì®sampling≠language biasºscalable relational learning¶Badges¿•Track∞research-article®Citation ®DownloadJ©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Nagrecha21•titleŸ9Model-Parallel Model Selection for Deep Learning Systems.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450571©publisher±SIGMOD ConferenceßauthorsëÆKabir Nagrecha®Abstract⁄ÈAs deep learning becomes more expensive, both in terms of time and compute, inefficiencies in machine learning training prevent practical usage of state-of-the-art models for most users. The newest model architectures are simply too large to be fit onto a single processor. To address the issue, many ML practitioners have turned to model parallelism as a method of distributing the computational requirements across several devices. Unfortunately, the sequential nature of neural networks causes very low efficiency and device utilization in model parallel training jobs. We propose a new form of "shard parallelism" combining task parallelism and model parallelism, and package it into a framework we name Hydra. Hydra recasts the problem of model parallelism in the multi-model context to produce a fine-grained parallel workload of independent model shards, rather than independent models. This new parallel design promises dramatic speedups relative to the traditional model parallelism paradigm.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450571®Keywordsú¶memory´parallelism≠deep learning∏machine learning systemsßsystems£GPU∞machine learning±model parallelism™efficiencyÆmodel training∞database systems™scheduling¶Badges¿•Track®abstract®Citation ®DownloadJ©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/AnzumS21•titleŸAR2GSync and edge views: practical RDBMS to GDBMS synchronization.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464515©publisher±GRADES-NDA@SIGMODßauthorsí¨Nafisa AnzumØSemih Salihoglu®Abstract⁄¿Graph databases that are used in enterprises are primarily extracted from a main transactional store that is often an RDBMS. This data infrastructure set up raises the challenge of keeping the extracted graph in a graph database management system (GDBMS) in sync with the source RDBMS. When the extracted graphs contain edge types that are results of join queries, this synchronization requires incrementally maintaining these join queries. In this paper, we investigate an alternative design where we can map the individual relations in these joins to virtual nodes and edges to keep the synchronization very efficient and instead support view-based querying in the GDBMS. We present a system called R2GSync, that synchronizes an RDBMS with a GDBMS and our accompanying edge view design for a GDBMS. We describe our implementation of edge views in GraphflowDB and query optimization techniques for improving the performance of queries that involve edge views.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464515®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadI©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/LiYCS021•titleŸEIndoorViz: A Demonstration System for Indoor Spatial Data Management.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452761©publisher±SIGMOD Conferenceßauthorsï¶Yue Li™Shiyu YangµMuhammad Aamir Cheema©Zhou ShaoØXuemin Lin 0001®Abstract⁄Due to the growing popularity of indoor location-based services, indoor data management has received significant research attention in the past few years. However, we observe that the existing indexing and query processing techniques for the indoor space do not fully exploit the properties of the indoor space. Consequently, they provide below par performance which makes them unsuitable for large indoor venues with high query workloads. In this demonstration, we present IndoorViz, a new indoor spatial data management system that integrates three novel index structures proposed in [4] and [6] with well designed query processing algorithms and 3D visualization functions. The IndoorViz is able to support indoor spatial object indexing, efficient query processing and interactive 3D display.©VideoSize¶234 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452761&amp;file=3448016.3452761.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452761®KeywordsìΩindoor location-based service∑indoor query processing¨indoor index¶Badges¿•Track´short-paper®Citation ®DownloadI©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/DorreKHJ21•titleŸ(A GraphBLAS implementation in pure Java.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464627©publisher±GRADES-NDA@SIGMODßauthorsî∞Florentin D√∂rre∞Alexander Krause´Dirk Habich∞Martin Junghanns®Abstract⁄øAnalyzing connected data in forms of graphs is more relevant than ever. To allow users to write their own custom graph algorithms, graph computation models such as GraphBLAS have been developed. Unfortunately, the popular Java programming language was mostly neglected by existing GraphBLAS implementations so far. To overcome that issue, we present our implementation of essential GraphBLAS concepts in the Java programming language in this paper. For our purpose, we extended the linear algebra library Efficient Java Matrix Library (EJML). To show the benefits of our implementation, we compare us against existing graph algorithm libraries in Java using real world graphs and three graph algorithms.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464627®Keywordsì≠Java languageØgraph analytics©GraphBLAS¶Badges¿•Track∞research-article®Citation ®DownloadI©PaperRefsêã§Infoá§type≠inproceedings£keyªconf/sigmod/GeorgiouPOPSH21•titleŸ[Attaining Workload Scalability and Strong Consistency for Replicated Databases with Hihooi.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452746©publisher±SIGMOD Conferenceßauthorsñ≥Michael A. Georgiou≤Michael Panayiotou∞Lambros Odysseos¥Aristodemos Paphitis≤Michael Sirivianos≥Herodotos Herodotou®Abstract⁄≠Database replication can be employed for scaling transactional workloads while maintaining strong consistency semantics. However, past approaches suffer from various issues such as limited scalability, performance versus consistency tradeoffs, and requirements for database or application modifications. Hihooi is a new replication-based master-slave middleware system that is able to overcome the aforementioned limitations. The novelty of Hihooi lies in its modern architecture as well as its replication and transaction routing algorithms. In particular, Hihooi replicates all write statements asynchronously and applies them in parallel at the replica nodes, while ensuring replica consistency. At the same time, a fine-grained transaction routing algorithm ensures that all read transactions are load balanced to the replicas consistently. This demonstration will showcase the key functionalities of Hihooi, including (i) practical management of system components and databases (e.g., add a new replica node), (ii) increased scalability compared to state-of-the-art approaches, and (iii) support for elasticity by suspending and resuming database replicas online without service interruption.©VideoSizeß34.7 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452746&amp;file=3448016.3452746.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452746®Keywordsî¥workload scalability≥concurrency control∂transaction processing¥database replication¶Badges¿•Track´short-paper®Citation ®DownloadH©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/0002KEF21•titleŸWUnderstanding and optimizing packed neural network training for hyper-parameter tuning.§year§2021£doiŸ'https://doi.org/10.1145/3462462.3468880©publisher´DEEM@SIGMODßauthorsî¨Rui Liu 0002ØSanjay KrishnanØAaron J. Elmore≥Michael J. Franklin®Abstract⁄…As neural networks are increasingly employed in machine learning practice, how to efficiently share limited training resources among a diverse set of model training tasks becomes a crucial issue. To achieve better utilization of the shared resources, we explore the idea of jointly training multiple neural network models on a single GPU in this paper. We realize this idea by proposing a primitive, called pack. We further present a comprehensive empirical study of pack and end-to-end experiments that suggest significant improvements for hyperparameter tuning. The results suggest: (1) packing two models can bring up to 40% performance improvement over unpacked setups for a single training step and the improvement increases when packing more models; (2) the benefit of the pack primitive largely depends on a number of factors including memory capacity, chip architecture, neural network structure, and batch size; (3) there exists a trade-off between packing and unpacking when training multiple neural network models on limited resources; (4) a pack-aware Hyperband is up to 2.7X faster than the original Hyperband, with this improvement growing as memory size increases and subsequently the density of models packed.©VideoSizeß17.4 MB©VideoLinkŸ^https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3462462.3468880&amp;file=a3-liu.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3462462.3468880®Keywordsê¶Badges¿•Track∞research-article®Citation ®DownloadG©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/Gemawat21•titleŸEGraphGem: Optimized Scalable System for Graph Convolutional Networks.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450573©publisher±SIGMOD ConferenceßauthorsëØAdvitya Gemawat®Abstract⁄ùDeep Learning (DL), especially Graph Convolutional Networks (GCNs) have revolutionized several domains and applications dealing with unstructured data with non-euclidean and graphical relationships. Constructing large-scale Deep GCNs, however, are bottlenecked by glaring systems issues due to memory blow-ups, runtime slowdowns with random access, and I/O costs. This research abstract identifies various systems and scalability issues and proposes a novel system called GraphGem to handle GCN-centric DL tasks end-to-end. GraphGem tackles the bottlenecks by elevating entire GCN workloads for convenient input declarations by the user, and is inspired by lessons from the databases and machine learning systems worlds. This abstract also highlights the bigger picture of the potential research impact alongside tacking systems constraints and what it may mean for data science and deep learning practitioners going forward.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450573®Keywordsô£I/Oßsystems≠random accessºgraph convolutional networks≤feature extractionßstorage≠deep learning™embeddingsØneural networks¶Badges¿•Track®abstract®Citation ®DownloadA©PaperRefsêã§Infoá§type≠inproceedings£key±conf/sigmod/Zhu21•titleŸ.Data Summarization with Hierarchical Taxonomy.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450578©publisher±SIGMOD Conferenceßauthorsë´Xuliang Zhu®Abstract⁄Data summarization has wide applications in real world, e.g. attributes filter, image set labeling and personalized recommendation. In this work, we study a new problem HSD to summarize a dataset using k concepts in a hierarchical taxonomy. Different from the existed works of whole hierarchy summarization, we focus on the accurate coverage of the given query set Q. The objective is to cover more items in Q and less items not in Q. To tackle it, we first propose a dynamic programming based algorithm on the tree hierarchy, which is a simple instance of HSD problem. Furthermore, we propose a heuristic method to assign the vertex to one of its in-neighbors for HDAGs and apply the tree algorithm on it. The experimental results confirm the quality of our methods on both tree and HDAG datasets.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450578®Keywordsí©hierarchy≠summarization¶Badges¿•Track®abstract®Citation ®DownloadA©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Knopf21•titleŸWFramework for Differentially Private Data Analysis with Multiple Accuracy Requirements.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450587©publisher±SIGMOD Conferenceßauthorsë™Karl Knopf®Abstract⁄$Organizations who collect sensitive data, such as hospitals or governments, may want to share the data with others. There could be multiple applications or analysts that want to use this data. Directly releasing the data could violate the privacy of individual data contributors. To address this privacy concern, differential privacy [1,2] has arisen as a popular technique for allow for sensitive data analysis. It frequently works through the addition of randomized noise to the output of the analysis, which is controlled through the privacy parameter or budget Œµ. This noise affects the utility of the analyses, where a smaller budget allocation results in larger noise values, and some applications may set accuracy requirements on the output to restrict the amount of noise added [3,9,10]. The total privacy loss of a sequence of differentially private mechanisms can be composed by summing up the privacy budgets they use, under the property of sequential composition [2]. Hence, if we intend to run multiple applications or analyses on the same dataset, given a total privacy budget, we can support each application by splitting the privacy budget evenly among them. However, if there are many applications, the privacy budget received per application could be very small, resulting in poor overall utility.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450587®Keywordsì®accuracy∞matrix mechanism¥differential privacy¶Badges¿•Track®abstract®Citation ®Download>©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/Amer-YahiaR21•titleŸAData Management to Social Science and Back in the Future of Work.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3457536©publisher±SIGMOD Conferenceßauthorsí∞Sihem Amer-Yahia∞Senjuti Basu Roy®Abstract⁄˜How will we work, live, and thrive in the post-pandemic future? The rapid mushrooming of online job markets has been transforming the definition of work and workplaces. After the pandemic, as we "cope with the new normal", the future world of work may change forever and become predominantly virtual. This makes an unprecedented pool of talent available at our beck and calls to work on "gigs" that disband when the job is over; this also is the time of destabilization and changing nature of job security. As scientists, we have a big responsibility and a tremendous opportunity in shaping the Future of Work (FoW) post pandemic, by designing effective platforms that support productive employment, mitigate social costs, and provide an effective and safe learning environment. A research agenda for FoW must mobilize the participation of various scientific, regulatory and miscellaneous stakeholders [10]. We will ask the questions: what is the role of Data Management (DM) in shaping research on FoW? Is now a ripe time to get Economics, Labor Theory, Psychology of Work and AI to help put DM research and technology at the center of research on FoW? Are we at all interested? The panelists will debate two complementary views: A pessimistic view on whether FoW will tend to see humans as machines, robots, or low-level agents and use them in the service of broader AI goals vs. a more optimistic view, where AI and Social Science will help DM to develop technologies that empower humans for future workforce and workplaces.©VideoSizeß22.1 MB©VideoLinkŸnhttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3457536&amp;file=SIGMODPanelTeaserVideo.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3457536®KeywordsïÆfuture of work®gig work≠peer learning≠human factors±working from home¶Badges¿•Track•panel®Citation ®Download=©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/Singla21•titleŸ;Raptor: Large Scale Processing of Big Raster + Vector Data.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450585©publisher±SIGMOD Conferenceßauthorsë∞Samriddhi Singla®Abstract⁄There has been an increase in the amount of spatial data in the recent years due to the advancements in remote sensing technology and the widespread use of smart phones and GPS technology. This has resulted in petabytes of satellite imagery as well as highly accurate geographical features such as city boundaries, roads, and others being made publicly available. Spatial data can generally be modeled in two representations: raster and vector. Satellite imagery is an example of raster data and is usually represented in form of multi-dimensional arrays. Vector data is represented as a set of points, lines, and polygons, and is used to represent geographical features such as regional boundaries. The growth of geospatial data has helped in new scientific discoveries in a wide range of applications that require combining raster and vector data. However, traditional systems implement algorithms that work with either raster or vector data. This paper proposes a novel approach for the concurrent processing of raster and vector data.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450585®Keywordsî±satellite imagery¶vector∞big spatial data¶raster¶Badges¿•Track®abstract®Citation ®Download<©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/IssaBT21•titleŸ6INCA: Inconsistency-Aware Data Profiling and Querying.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3452760©publisher±SIGMOD Conferenceßauthorsì¨Ousmane IssaØAngela BonifatiÆFarouk Toumani®Abstract⁄‹When exploring and querying inconsistent data, inconsistency measures referring to constraint violations can help the user to quantify the quality of the underlying data and query results. We showcase INCA, a system that allows the user to execute data profiling and query answering tasks in an inconsistency-aware fashion. By using data instances annotated with novel inconsistency measures based on why-provenance and polynomial provenance, it becomes possible to visualize the share of the data which is consistent or inconsistent with respect to one or multiple denial constraints. Furthermore, data exploration by constraint or by subset of constraints allows to inspect the tuple violations according to multifaceted criteria. Finally, query profiling allows to enable inconsistency-aware query results accounting for most (in-)consistent top-k and threshold query results. To the best of our knowledge, INCA is the first system to allow such an inconsistency-driven analysis of both data and query results. Such an analysis is especially fruitful for enabling selective constraint-based data cleaning and inconsistency-aware ranking of query results in data science pipelines, thus leading to more explainable outputs of those processes.©VideoSizeß19.5 MB©VideoLinkŸghttps://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3448016.3452760&amp;file=3448016.3452760.mp4ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3452760®Keywordsï∂top-k query processing≤data inconsistency≥semiring provenanceÆwhy provenance¨data quality¶Badges¿•Track´short-paper®Citation ®Download9©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/ZhangSL21•titleŸFLeveraging Approximate Constraints for Localized Data Error Detection.§year§2021£doiŸ'https://doi.org/10.1145/3464509.3464888©publisher´aiDM@SIGMODßauthorsì´Mohan ZhangÆOliver Schulte™Yudong Luo®Abstract⁄∂Error detection is key for data quality management. AI techniques can leverage user domain knowledge to identifying sets of erroneous records that conflict with domain knowledge. To represent a wide range of user domain knowledge, several recent papers have developed and utilized soft approximate constraints (ACs) that a data relation is expected to satisfy only to a certain degree, rather than completely. We introduce error localization, a new AI-based technique for enhancing error detection with ACs.  Our starting observation is that approximate constraints are context-sensitive: the degree to which they are satisfied depends on the sub-population being considered. An error region is a subset of the data that violates an AC to a higher degree than the data as a whole, and is therefore more likely to contain erroneous records. For example, an error region may contain the set of records from before a certain year, or from a certain location. We describe an efficient optimization algorithm for error localization: identifying distinct error regions that violate a given AC the most, based on a recursive tree partitioning scheme. The tree representation describes different error regions in terms of data attributes that are easily interpreted by users (e.g., all records before 2003). This helps to explain to the user why some records were identified as likely errors.  After identifying error regions, we apply error detection methods to each error region separately, rather than to the dataset as a whole. Our empirical evaluation, based on four datasets containing both real world and synthetic errors, shows that error localization increases both accuracy and speed of error detection based on ACs.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3464509.3464888®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download8©PaperRefsêã§Infoá§type≠inproceedings£key≥conf/sigmod/Zheng21•titleŸ+Contextual Data Cleaning with Ontology FDs.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450583©publisher±SIGMOD Conferenceßauthorsë´Zheng Zheng®Abstract⁄œFunctional Dependencies (FDs) define attribute relationships based on syntactic equality, and, when used in data cleaning, they erroneously label syntactically different but semantically equivalent values as errors. We motivate the need to include context in data cleaning in order to account for the subjective nature of data quality. We enhance dependency-based data cleaning with Ontology Functional Dependencies (OFDs), which express semantic attribute relationships such as synonyms and is-a hierarchies defined by an ontology. We study the data and ontology repair problem for a set of OFDs, and propose an algorithm that finds the best ontological interpretation of the data that minimizes the number of repairs.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450583®Keywordsì≠data cleaningŸ ontology functional dependenciesπconstraint-based cleaning¶Badges¿•Track®abstract®Citation ®Download6©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/DengGB21•titleŸÜFogBus2: a lightweight and distributed container-based framework for integration of IoT-enabled systems with edge and cloud computing.§year§2021£doiŸ'https://doi.org/10.1145/3460866.3461768©publisher≠BiDEDE@SIGMODßauthorsì™Qifan Deng±Mohammad GoudarziÆRajkumar Buyya®Abstract⁄Edge/Fog computing is a novel computing paradigm that provides resource-limited Internet of Things (IoT) devices with scalable computing and storage resources. Compared to cloud computing, edge/fog servers have fewer resources, but they can be accessed with higher bandwidth and less communication latency. Thus, integrating edge/fog and cloud infrastructures can support the execution of diverse latency-sensitive and computation-intensive IoT applications. Although some frameworks attempt to provide such integration, there are still several challenges to be addressed, such as dynamic scheduling of different IoT applications, scalability mechanisms, multi-platform support, and supporting different interaction models. To overcome these challenges, we propose a lightweight and distributed container-based framework, called FogBus2. It provides a mechanism for scheduling heterogeneous IoT applications and implements several scheduling policies. Also, it proposes an optimized genetic algorithm to obtain fast convergence to well-suited solutions. Besides, it offers a scalability mechanism to ensure efficient responsiveness when either the number of IoT devices increases or the resources become overburdened. Also, the dynamic resource discovery mechanism of FogBus2 assists new entities to quickly join the system. We have also developed two IoT applications, called Conway's Game of Life and Video Optical Character Recognition to demonstrate the effectiveness of FogBus2 for handling real-time and non-real-time IoT applications. Experimental results show FogBus2's scheduling policy improves the response time of IoT applications by 53% compared to other policies. Also, the scalability mechanism can reduce up to 48% of the queuing waiting time compared to frameworks that do not support scalability.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3460866.3461768®Keywordsî≤internet of thingsµcontainers scheduling´scalability≤edge/fog computing¶Badges¿•Track∞research-article®Citation ®Download6©PaperRefsêã§Infoá§type≠inproceedings£key∑conf/sigmod/Jahangiri21•titleŸ7Wisconsin Benchmark Data Generator: To JSON and Beyond.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450577©publisher±SIGMOD ConferenceßauthorsëØShiva Jahangiri®Abstract⁄ÙBenchmarks have always been one of the greatest assets in evaluating a Data Management System (DBMS) performance and providing a standard way to compare different DBMSs from various angles. As DBMSs evolve over time, new benchmarks are created, and the older ones need to advance and adapt in order to continue to be a valid and capable comparison tool. Many of the standard benchmarks use synthetic data and as such may not be able to capture the complexity of real data and its relationships correctly. The Wisconsin Benchmark was one of the first and main benchmarking tools made four decades ago by Dewitt et al at the University of Wisconsin. One of the most powerful features of this benchmark is that its relations are designed so their structure and distribution of attributes is easy to understand and control. While the Wisconsin Benchmark was a very powerful and widely-used benchmark years ago, it is given less attention in current studies. We believe that the Wisconsin Benchmark and its carefully designed relations can be utilized and provide capabilities that are unique and useful.. In this paper, we present a flexible, easy-to-use and scalable JSON Data Generator implemented in java based on the Wisconsin Benchmark Data Generator description, with more advanced features to provide relations and attributes closer to real-world data. Attribute skewness and variable length records using different size distributions are some of these newly added features. It is a ready-to-use, parameterized, and scalable data generator tool that since its development has been used in several AsterixDB's [4] performance benchmarking and publications [13,14,17,18], and we believe that can be useful to many others. The source code and more information are provided at [1].©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450577®Keywordsî≥wisconsin benchmarkÆdata generator®skewness§json¶Badges¿•Track®abstract®Citation ®Download6©PaperRefsêã§Infoá§type≠inproceedings£key¥conf/sigmod/Dann0F21•titleŸPDemystifying memory access patterns of FPGA-based graph processing accelerators.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464512©publisher±GRADES-NDA@SIGMODßauthorsì™Jonas Dann≤Daniel Ritter 0001ØHolger Fr√∂ning®Abstract⁄µRecent advances in reprogrammable hardware (e. g., FPGAs) and memory technology (e. g., DDR4, HBM) promise to solve performance problems inherent to graph processing like irregular memory access patterns on traditional hardware (e. g., CPU). While several of these graph accelerators were proposed in recent years, it remains difficult to assess their performance and compare them on common graph workloads and accelerator platforms, due to few open source implementations and excessive implementation effort. In this work, we build on a simulation environment for graph processing accelerators, to make several existing accelerator approaches comparable. This allows us to study relevant performance dimensions such as partitioning schemes and memory technology, among others. The evaluation yields insights into the strengths and weaknesses of current graph processing accelerators along these dimensions, and features a novel in-depth comparison.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464512®Keywordsñ§DRAM§FPGA£HBM©benchmark∞graph processing™simulation¶Badges¿•Track∞research-article®Citation ®Download3©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/KundePMS21•titleŸ>Distributed training for accelerating metalearning algorithms.§year§2021£doiŸ'https://doi.org/10.1145/3460866.3461773©publisher≠BiDEDE@SIGMODßauthorsî¨Shruti Kunde´Amey Pandit≠Mayank Mishra≠Rekha Singhal®Abstract⁄™The lack of large amounts of training data diminishes the power of deep learning to train models with a high accuracy. Few shot learning (i.e. learning using few data samples) is implemented by Meta-learning, a learn to learn approach. Most gradient based metalearning approaches are hierarchical in nature and computationally expensive. Metalearning approaches generalize well across new tasks by training on very few tasks; but require multiple training iterations which lead to large training times. In this paper, we propose a generic approach to accelerate the training process of metalearning algorithms by leveraging a distributed training setup. Training is conducted on multiple worker nodes, with each node processing a subset of the tasks, in a distributed training paradigm. We propose QMAML (Quick MAML), which is a distributed variant of the MAML (Model Agnostic Metalearning) algorithm, to illustrate the efficacy of our approach. MAML is one of the most popular meta-learning algorithms that estimates initialization parameters for a meta model to be used by similar newer tasks for faster adaptation. However, MAML being hierarchical in nature is computationally expensive. The learning-tasks in QMAML are run on multiple workers in order to accelerate the training process. Similar to the distributed training paradigm, gradients for learning-tasks are consolidated to update the meta-model. We leverage a lightweight distributed training library, Horovod, to implement QMAML. Our experiments illustrate that QMAML reduces the training time of MAML by 50% over an open source library, learn2learn, for image recognition tasks, which are quasi-benchmark tasks in the field of metalearning.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3460866.3461773®Keywordsî¨acceleration¨metalearning≠deep learning¥distributed training¶Badges¿•Track∞research-article®Citation ®Download2©PaperRefsêã§Infoá§type≠inproceedings£keyºconf/sigmod/Shanghooshabad21•title®XLJoins.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450582©publisher±SIGMOD ConferenceßauthorsëºAli Mohammadi Shanghooshabad®Abstract⁄tIn many analytic settings join operations are fundamental as data is dispersed across different data sets (SQL or NoSQL tables, .csv files recording logs, click streams, KPIs from system/network monitoring, IoT telemetry, etc). However, in the era of big data the join operation can become exorbitantly expensive in terms of execution times and/or memory/space footprints.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450582®Keywordsî≠join sampling∞uniform sampling´model joins´join graphs¶Badges¿•Track®abstract®Citation ®Download/©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/AzimovEG21•titleŸLContext-free path querying with all-path semantics by matrix multiplication.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464513©publisher±GRADES-NDA@SIGMODßauthorsì≠Rustam Azimov≠Ilya Epelbaum≤Semyon V. Grigorev®Abstract⁄€Context-Free Path Querying (CFPQ) allows one to use context-free grammars as path constraints in navigational graph queries. Many algorithms for CFPQ were proposed but recently showed that the state-of-the-art CFPQ algorithms are still not performant enough for practical use. One promising way to achieve high-performance solutions for graph querying problems is to reduce them to linear algebra operations. Recently, there are two CFPQ solutions formulated in terms of linear algebra: the one based on the Boolean matrix multiplication operation proposed by Azimov et al. (2018) and the Kronecker product-based CFPQ algorithm proposed by Orachev et al. (2020). However, the algorithm based on matrix multiplication still does not support the most expressive all-path query semantics and cannot be truly compared with Kronecker product-based CFPQ algorithm. In this work, we introduce a new matrix-based CFPQ algorithm with all-path query semantics that allows us to extract all found paths for each pair of vertices. Also, we implement our algorithm by using appropriate high-performance libraries for linear algebra. Finally, we provide a comparison of the most performant linear algebra-based CFPQ algorithms for different query semantics.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464513®Keywordsó∫context-free path queryingØgraph databases¥context-free grammar≤transitive closure©GraphBLASÆlinear algebraµmatrix multiplication¶Badges¿•Track∞research-article®Citation ®Download/©PaperRefsêã§Infoá§type≠inproceedings£key∂conf/sigmod/Scarpino21•titleºNetworked data and COVID-19.§year§2021£doiŸ'https://doi.org/10.1145/3461837.3464688©publisher±GRADES-NDA@SIGMODßauthorsë≤Samuel V. Scarpino®Abstract⁄êThe COVID-19 pandemic has upended our societies and re-shaped the way we go about our day-to-day lives---from how we work and interact to the way we buy groceries and attend school. Leveraging global data sets that represent billions of people, I will present a series of studies exploring how our behavior [2, 10], mobility patterns [6, 7], and social networks [3, 9] have altered and been altered by COVID-19 and the non-pharmaceutical interventions implemented to control its spread. Next, I will examine how we can better incorporate stochasticity and social network heterogeneity [4] and link directionality [1] into forecasting pandemic risk. With these results, I will demonstrate how the complexity of COVID-19 creates epistemological challenges associated with model identifiability [5, 8, 11]. Finally, I will discuss work by Global.health, a new collaborative network of researchers, technologists, and public health experts that has developed and built an open access platform for collecting, storing, securing, and sharing anonymized, individual-level COVID-19 data. Currently, our data includes almost 30M individual-level cases from 160 countries, which are tagged with up to 40 fields of meta-data. Writing for The New York Times Magazine, Steven Johnson said the data captured by Global.health, "may well be the single most accurate portrait of the virus's spread through the human population in existence."©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3461837.3464688®Keywordsî¨epidemiology®datasetsØnetwork science®COVID-19¶Badges¿•Track¨invited-talk®Citation ®Download.©PaperRefsêã§Infoá§type≠inproceedings£key∞conf/sigmod/Li21•titleŸ5Grouped Learning: Group-By Model Selection Workloads.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450576©publisher±SIGMOD ConferenceßauthorsëßSide Li®Abstract⁄˘Machine Learning (ML) is gaining popularity in many applications. Increasingly, companies prefer more targeted models for different subgroups of the population like locations, which helps improve accuracy. This practice is comparable to Group-By aggregation in SQL; we call it learning over groups. A smaller group means the data distribution is more straightforward than the whole population. So, a group-level model may offer more accuracy in many cases. Non-technical business needs, such as privacy and regulatory compliance, may also necessitate group-level models. For instance, online advertising platforms would need to build disaggregated partner-specific ML models, where all partner groups' training data are aggregated together in one data pipeline.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450576®Keywordsê¶Badges¿•Track®abstract®Citation ®Download-©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/CerinAG21•titleŸ`Towards an emulation tool based on ontologies and data life cycles for studying smart buildings.§year§2021£doiŸ'https://doi.org/10.1145/3460866.3461772©publisher≠BiDEDE@SIGMODßauthorsì±Christophe C√©rin≤Fr√©d√©ric Andr√®s∫Danielle Geldwerth-Feniger®Abstract⁄•In this paper, we share our vision to study a complex Information Technology (IT) system handling a massive amount of data in the context of 'smart buildings.' One technique for analyzing complex IT systems relies on emulation, where the final software system is fully deployed on real architectures, and is evaluated in considering "small" instances of situations the system is supposed to solve. We propose a software architecture for studying the ecosystem of 'smart buildings'. This software architecture is built: 1) on top of ontologies for the description of smart buildings; 2) on a special tool for mastering the life cycle of data produced by sensors and actuators inside the buildings. We assume that it is equally important to model both the building's components and the flow of data produced inside the building. We use existing software components for both goals and to make real our concerns. According to a translational methodology, we also discuss use cases for illustrating the potential of our approach and the particular challenges associated with making the two main components of our emulation tool inter-operate. Therefore, our main contribution is to propose a comprehensive, ambitious and realistic research plan to guide communities. The paper illustrates how computer scientists and smart buildings domain scientists may communicate to address and solve specific research problems related to Big Data in emergent distributed environments. We are also guessing that experimental results that can demonstrate the practicality of the proposed combination of tools could be devised in the future, based on our broad vision. The paper is, first and foremost, a visionary paper.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3460866.3461772®KeywordsñÆbig data tools¥emulation principlesØdata life cycle®ontologyØsmart buildings≥systems and methods¶Badges¿•Track∞research-article®Citation ®Download*©PaperRefsêã§Infoá§type≠inproceedings£key≤conf/sigmod/LiPW21•titleŸ:Subteam Replacement: Problem Definition and Fast Solution.§year§2021£doiŸ'https://doi.org/10.1145/3448016.3450575©publisher±SIGMOD Conferenceßauthorsì´Zhaoheng Li®Xinyu Pi´Mingyuan Wu®Abstract⁄‡In settings such as corporate management where team structure is highly volatile and large-scale personnel changes are commonplace, the ability to simultaneously replace multiple team members in a team is highly appreciated. We define the problem of Subteam Replacement to address this observation: given a team of people embedded in a social network to complete a certain task, and a subset of members - subteam - in this team which has become unavailable, find another set of people which can perform the subteam's role in the larger team. We propose a holistic evaluation metric and scalable solution for Subteam Replacement with strong theoretical guarantees and perform quantitative evaluations on both generated and real datasets.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3448016.3450575®Keywordsî≠graph kernels∑approximation algorithm≥team recommendation¨graph mining¶Badges¿•Track®abstract®Citation ®Download)©PaperRefsêã§Infoá§type≠inproceedings£keyπconf/sigmod/BodzionyKPW21•titleŸPOn discovering semantics of user-defined functions in data processing workflows.§year§2021£doiŸ'https://doi.org/10.1145/3460866.3461771©publisher≠BiDEDE@SIGMODßauthorsîØMichal Bodziony≥Hubert Krzyzanowski¨Lukasz PietaÆRobert Wrembel®Abstract⁄ÔUser defined functions (UDFs) are frequent components of SQL queries and data processing workflows (DPWs). In both of these applications, UDFs are typically treated as black boxes, i.e., their semantics and performance characteristics are unknown, which prevents from optimizing execution plans of queries and DPWs. In this paper, we present an initial alternative solution for learning the semantics of simple UDFs, followed by its experimental evaluation. Our semantics discovery method is based on three steps: (1) discovering the type of an operation that a UDF implements, (2) discovering the types of restrictions on attributes and logical operators that combine the restrictions, and (3) representing the semantics of an original code of a UDF.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3460866.3461771®Keywordsê¶Badges¿•Track∞research-article®Citation ®Download"©PaperRefsêã§Infoá§type≠inproceedings£keyµconf/sigmod/BotherR21•titleŸ"Scale-down experiments on TPCx-HS.§year§2021£doiŸ'https://doi.org/10.1145/3460866.3461774©publisher≠BiDEDE@SIGMODßauthorsí≤Maximilian B√∂ther¨Tilmann Rabl®Abstract⁄&The Transaction Processing Performance Council's (TPC) benchmarks are the standard for evaluating data processing performance and are extensively used in academia and industry. Official TPC results are usually produced on high-end deployments, making transferability to commodity hardware difficult. Recent performance improvements on low-power ARM CPUs have made low-end computers, such as the Raspberry Pi, a candidate platform for distributed, low-scale data processing. In this paper, we conduct a feasibility study of executing scaled-down big data workloads on low-power ARM clusters. To this end, we run the TPCx-HS benchmark on two Raspberry Pi clusters. TPCx-HS is the ideal candidate for hardware comparisons and understanding hardware characteristics for data processing workloads because TPCx-HS results do not depend on specific software implementations and the benchmark has limited options for workload-specific tuning. Our evaluation shows that Pis exhibit similar behavior to large-scale big data systems in terms of price performance and relative throughput to performance results. Current generation Pi clusters are becoming a reasonable choice for GB-scale data processing due to the increasing amount of available memory, while older versions struggle with stable execution of high-load scenarios.©VideoSize¿©VideoLink¿ßPdfLinkŸ2https://dl.acm.org/doi/pdf/10.1145/3460866.3461774®Keywordsñ¶Hadoop£TPC©benchmarkßTPCx-HS£ARM¨Raspberry Pi¶Badges¿•Track∞research-article®Citation ®Download©PaperRefsêã§Infoá§type´proceedings£key∞conf/sigmod/2021•titleŸcSIGMOD '21: International Conference on Management of Data, Virtual Event, China, June 20-25, 2021.§year§2021£doiøhttps://doi.org/10.1145/3448016©publisher±SIGMOD Conferenceßauthorsî∞Guoliang Li 0001´Zhanhuai LiÆStratos Idreos±Divesh Srivastava®Abstract⁄vThis year, due to the global uncertainties, travel restrictions, and other potential
                        difficulties associated with Covid-19, SIGMOD is being held entirely online, instead
                        of at its originally planned location of Xi'an, Shaanxi, China. The online SIGMOD
                        conference is complemented with a local physical event at Xi'an, primarily targeting
                        the data management community in China. Despite the challenging times that we find
                        ourselves in, we have an exciting technical program with outstanding research, industrial
                        and demonstration track presentations, keynotes, tutorials, panels, and the awards
                        session. 
                     For the first time, SIGMOD is being held round the clock, with each technical talk
                        presented twice 12 hours apart, to better accommodate online participants from around
                        the world, Also, for the first time, presentations are being grouped into curated
                        sessions to give participants a cohesive, single track experience on a variety of
                        leading edge topics in data management. We are using the latest technologies to keep
                        SIGMOD vibrant, and we will be archiving most SIGMOD presentations, for those who
                        want to review them at a later date. 
                     This year, with the approval of the SIGMOD EC, we introduced two new categories of
                        papers in the Research Track, (a) Data Science &amp; Engineering and (b) Applications,
                        to complement the traditional Data Management category. Data Science &amp; Engineering
                        papers focused on dataintensive components of data science pipelines, solving problems
                        in areas of interest to the community inspired by real applications. Applications
                        papers presented novel applications of data management systems and technologies to
                        inspire future research in the community. 
                     In the Research Track this year, we received 450 research submissions (172 for Round
                        1 and 278 for Round 2), which were extensively reviewed by 175 program committee members,
                        23 associate editors, and several external reviewers. We accepted 188 submissions
                        (a 41.8% acceptance rate), most of them after a revision phase that gave authors 10+
                        weeks to revise and resubmit their papers in response to the reviewer comments. This
                        year we introduced a new set of detailed reviewing instructions focused on reviewing
                        constructively as well as redesigned the review forms to promote constructive reviewing.
                        In addition, we introduced a new step in the reviewing process, the Review Quality
                        week where the associate editors check reviews for certain quality criteria and probe
                        reviewers for constructive rewrites before reviews are released to the authors. In
                        addition, the authors were able to provide structured feedback directly to the associate
                        editors and the program chairs about review quality. Overall, more than 300 reviews
                        were updated for quality during this process leading to a higher number of revision
                        requests. 
                     In addition to the Research Track, the Industrial Track selected 21 papers from 54
                        submissions; the Demonstration Track selected 27 demonstrations from 75 submissions;
                        the Tutorial Track selected 8 tutorials from 20 submissions and the Student Research
                        Competition selected all 18 submissions for the second round of competition. 
                     This year, we will have two exciting keynote talks, reflecting emerging topics of
                        great interest to the data management community: "Utilizing (and Designing) Modern
                        Hardware for Data- Intensive Computations: The Role of Abstraction" by Kenneth A.
                        Ross (Columbia University) and "Deep Data Integration" by Wang-Chiew Tan (Facebook
                        AI). iv In addition, we will have two timely and interesting panels: "Data Management
                        to Social Science and Back in the Future of Work" organized by Sihem Amer-Yahia (CNRS)
                        and Senjuti Basu Roy (New Jersey Institute of Technology), and "Automation of Data
                        Prep, ML, and Data Science: New Cure or Snake Oil?" organized by Arun Kumar (University
                        of California, San Diego).©VideoSize¿©VideoLink¿ßPdfLink¿®Keywordsê¶Badges¿•Track¿®Citationˇ®Downloadˇ©PaperRefsê